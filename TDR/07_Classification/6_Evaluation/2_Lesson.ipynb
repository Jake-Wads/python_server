{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf81782",
   "metadata": {},
   "source": [
    "# Evaluation Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cdb93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd09c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with mock data\n",
    "glasses = pd.DataFrame(\n",
    "                       {'preds': ['no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes'],\n",
    "                        'actual': ['no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes']}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47740131",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba211107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preds actual\n",
       "0    no     no\n",
       "1    no    yes\n",
       "2   yes    yes\n",
       "3    no     no\n",
       "4    no     no\n",
       "5   yes     no\n",
       "6   yes    yes\n",
       "7    no     no\n",
       "8   yes     no\n",
       "9   yes    yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm dataframe meets expectations\n",
    "glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0476ffd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>no</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  no  yes\n",
       "preds          \n",
       "no       4    1\n",
       "yes      2    3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate confusion matrix using pd.crosstab\n",
    "pd.crosstab(glasses['preds'], glasses['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b629d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate confusion matrix using sklearn function\n",
    "confusion_matrix(glasses['preds'], glasses['actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052dbdf4",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b69cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     6\n",
       "yes    4\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show value counts of actual labels to determine baseline\n",
    "glasses['actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbcb61dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>actual</th>\n",
       "      <th>base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preds actual base\n",
       "0    no     no   no\n",
       "1    no    yes   no\n",
       "2   yes    yes   no\n",
       "3    no     no   no\n",
       "4    no     no   no\n",
       "5   yes     no   no\n",
       "6   yes    yes   no\n",
       "7    no     no   no\n",
       "8   yes     no   no\n",
       "9   yes    yes   no"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most prevalent label becomes baseline\n",
    "glasses['base'] = 'no'\n",
    "glasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df738a",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "How many correct guesses over total number of guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0abda2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2    False\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6    False\n",
       "7     True\n",
       "8     True\n",
       "9    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for matching labels between baseline and actual labels\n",
    "glasses['base'] == glasses['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b35673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute baseline accuracy\n",
    "baseline_acc = (glasses['base'] == glasses['actual']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f2f215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multiply by 100 to get percentage\n",
    "baseline_acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b750737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat with predictions column\n",
    "(glasses['preds'] == glasses['actual']).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb989b6",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Of all the times I guess the positive case, how many times am I correct?\n",
    "\n",
    "TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dfee0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3    False\n",
       "4    False\n",
       "5     True\n",
       "6     True\n",
       "7    False\n",
       "8     True\n",
       "9     True\n",
       "Name: preds, dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create mask for all the times the model predicted the positive case\n",
    "subset = glasses['preds'] == 'yes'\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed178b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create subset of dataframe from the mask\n",
    "prec_df = glasses[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "894bde05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     True\n",
       "5    False\n",
       "6     True\n",
       "8    False\n",
       "9     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare predictions to actual labels\n",
    "prec_df['preds'] == prec_df['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3443fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute precision\n",
    "(prec_df['preds'] == prec_df['actual']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d053c87",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Of all actual positive cases, how many did I correctly identify?\n",
    "\n",
    "TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c94c3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2     True\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6     True\n",
       "7    False\n",
       "8    False\n",
       "9     True\n",
       "Name: actual, dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create mask for all actual positive cases\n",
    "subset2 = glasses['actual'] == 'yes'\n",
    "subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0d6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mask to subset original dataframe\n",
    "recall_df = glasses[subset2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d0a85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    False\n",
       "2     True\n",
       "6     True\n",
       "9     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compare predictions to actual labels\n",
    "recall_df['preds'] == recall_df['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c25a4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate recall\n",
    "(recall_df['preds'] == recall_df['actual']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f43164",
   "metadata": {},
   "source": [
    "## Other metrics\n",
    "\n",
    "Misclassification rate\n",
    "\n",
    "1 - accuracy\n",
    "\n",
    "\n",
    "Sensitivity\n",
    "\n",
    "Detection of the positive class, aka recall\n",
    "\n",
    "\n",
    "Specifity\n",
    "\n",
    "Detection of the negative class, recall for the negative class\n",
    "\n",
    "\n",
    "False positive rate\n",
    "\n",
    "The rate at which a model produces false positives\n",
    "\n",
    "\n",
    "F1 score\n",
    "\n",
    "Harmonic mean of precision and recall\n",
    "\n",
    "\n",
    "AUC-ROC\n",
    "\n",
    "Area under ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7c790",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "Multiclass classification will repeat the calculations for precision and recall, treating each possible outcome as the positive case once. These results can be averaged (simple average, or weighted average factoring in support) to evaluate the performance of the model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba0937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.80      0.67      0.73         6\n",
      "         yes       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.71      0.70        10\n",
      "weighted avg       0.72      0.70      0.70        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report\n",
    "print(classification_report(glasses['actual'], glasses['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fa287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
