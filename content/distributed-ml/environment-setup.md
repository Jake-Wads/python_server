# Spark Environment Setup

## Install Dependencies

Run the following commands from your terminal:

```
brew tap adoptopenjdk/openjdk
brew install adoptopenjdk11
```

After running the commands above, you can run

```
java -version
```

And you should see output similar to:

    openjdk version "11.0.4" 2019-07-16
    OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.4+11)
    OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.4+11, mixed mode)

After java is installed, we can install `pyspark`, the library that provides the
python interface to spark:

```
python -m pip install pyspark
```

## Testing Your Installation

Once everything is installed, you can run the code below:

```python
import pyspark
spark = pyspark.sql.SparkSession.builder.getOrCreate()
spark.range(5).show()
```

After running the line that creates the `spark` variable, you may see lots of
output, including some warnings, as the spark process is started. This is
normal.

If you see the following as output, you are good to go!

```
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
+---+
```

## Running Spark

In order to interact with spark, we'll create a `spark` object that lets us
interface with the underlying spark instance and contains all of our environment
configuration. In the rest of the lessons, we'll see more interaction with the
`spark` object. This can be done in a python script, or a jupyter notebook.

Here is a simple example:

```python
import pyspark

spark = pyspark.sql.SparkSession.builder.getOrCreate()
```

And here is a more complex example with more configuration options:

```python
import multiprocessing
import pyspark

nprocs = multiprocessing.cpu_count()

spark = (pyspark.sql.SparkSession.builder
 .master('local')
 .config('spark.jars.packages', 'mysql:mysql-connector-java:8.0.16')
 .config('spark.driver.memory', '4G')
 .config('spark.driver.cores', nprocs)
 .config('spark.sql.shuffle.partitions', nprocs)
 .appName('MySparkApplication')
 .getOrCreate())
```

!!!note "Session Reuse"
    Using `.getOrCreate` will only create a new spark session once. Subsequent
    calls to that method will reuse the existing session.

## Other Imports

It's common to import some functions from `pyspark.sql.functions` as well. You
will see multiple functions from here as we progress through this module, and it
is also common to see this module imported as `F` like this:

```python
import pyspark.sql.functions as F
```

## Gitignore

As you continue work in this module, you'll find that spark might autogenerate
some files and folders as you are reading and writing data. Make sure you are
running `git status` frequently and don't commit any autogenerated files.
