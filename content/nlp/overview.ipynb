{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "## Summary\n",
    "\n",
    "- NLP or Natural Language Processing (not Neurolinguistic Processing).\n",
    "- Use programming & machine learning techniques to help understand and make use of large amounts of text data.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Voice of Customer Analytics:** Improve products and services through analysis of customer interactions, such as support emails, social media posts, online comments, telephone transcriptions, i.e., to discover what factors drive the most positive and negative experiences. An example would be to extract key phrases and topics by summarizing blocks of text from open-ended survey responses in order to extract the most important and central ideas that can lead to actionable insights.  \n",
    "\n",
    "- **Semantic Search:**  Provide a better search experience by enabling your search engine to index key phrases, entities, and sentiment. This enables you to focus the search on the intent and the context of the articles instead of basic keywords.\n",
    "\n",
    "- **Knowledge Management & Discovery:**  Organize and categorize your documents by topic for easier discovery. You might want to personalize content recommendations for readers by recommending other articles related to the same topic. Or you might want to ensure the security of documents by closely monitoring those documents containing sensitive materials  (Topic Modeling).\n",
    "\n",
    "## Methods\n",
    "\n",
    "- Text Classification: \n",
    "\n",
    "    - Assign tags or categories to text according to its content.\n",
    "    \n",
    "    - Applications: sentiment analysis, topic labeling, spam detection, and intent detection.\n",
    "    \n",
    "    - Similar to topic modeling, but is supervised learning, so the set of possible classes are known/defined in advance.\n",
    "\n",
    "\n",
    "- Topic Modeling: \n",
    "\n",
    "    - Discover the abstract “topics” that occur in a collection of documents.\n",
    "    \n",
    "    - Latent Dirichlet Allocation (LDA) is a commonly used algorithm.\n",
    "    \n",
    "    - Similar to text classification but is unsupervised, like clustering, so the set of possible topics are unknown prior. The topics are defined as part of generating the topic models.\n",
    "\n",
    "\n",
    "## General Process\n",
    "\n",
    "![NLP Process Workflow](https://cdn-images-1.medium.com/max/2000/1*BiVCmiQtCBIdBNcaOKjurg.png)\n",
    "\n",
    "1. Processing & Understanding Text\n",
    "2. Feature Engineering & Text Representation\n",
    "3. Supervised Learning Models for Text Data\n",
    "4. Unsupervised Learning Models for Text Data\n",
    "5. Advanced Topics\n",
    "\n",
    "### Wrangle\n",
    "- Acquire your corpus (your sample, your dataset) of documents (your observations).\n",
    "\n",
    "- Convert to sentences (if applicable).\n",
    "\n",
    "- Normalize text (as applicable to your use case).\n",
    "\n",
    "    - Make all text lowercase.\n",
    "    \n",
    "    - Remove accents, special characters, numbers, punctuation.\n",
    "    \n",
    "    - Stem or lemmatize words.\n",
    "\n",
    "### Tokenize and Prep\n",
    "\n",
    "- Tokenize: Break text up into tokens, linguistic units such as words. Units could be:\n",
    "\n",
    "    - Individual words\n",
    "    \n",
    "    - n-grams: Set of co-occuring words (n-words) within a given window and when computing the n-grams you typically move one word forward, e.g. 'data science' is a bi-gram and 'codeup data science' is a tri-gram. \n",
    "\n",
    "- Remove Stopwords\n",
    "\n",
    "### POS Tagging & Chunking (not always used).\n",
    "\n",
    "- POS (Part-of-Speech) Tagging:\n",
    "\n",
    "    - Explains how a word is used in a sentence. \n",
    "    \n",
    "    - 8 main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions, and interjections.\n",
    "\n",
    "\n",
    "- Chunking:\n",
    "\n",
    "    - Extracting phrases from unstructured text. \n",
    "    \n",
    "    - Instead of just simple tokens which may not represent the actual meaning of the text, its advisable to use phrases such as “South Africa” as a single word instead of ‘South’ and ‘Africa’ separate words, this is an example of an n-gram or bi-gram in this case.\n",
    "    \n",
    "    - Uses pos-tags as input and provides chunks as output.\n",
    "    \n",
    "    - Similar to POS tags, there are a standard set of chunk tags like Noun Phrase(NP), Verb Phrase (VP), etc.\n",
    "    \n",
    "    - Use Cases: Extracting Locations, Person Names, Named Entity Extraction.\n",
    "\n",
    "\n",
    "### Vectorization\n",
    "    \n",
    "- Treat each sentence as a separate document.\n",
    "    \n",
    "- Make a list of all words from all documents.\n",
    "    \n",
    "- Create vectors. Vectors convert text that can be used by the machine learning algorithm.\n",
    "    \n",
    "- Input:\n",
    "    - `“It was the best of times”`\n",
    "    - `“It was the worst of times”`\n",
    "    - `“It was the age of wisdom”`\n",
    "    - `“It was the age of foolishness”`\n",
    "    \n",
    "- Output: \n",
    "    - Dictionary = `[‘It’, ‘was’, ‘the’, ‘best’, ‘of’, ‘times’, ‘worst’, ‘age’, ‘wisdom’, ‘foolishness’]`\n",
    "    - `“It was the best of times” = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]`\n",
    "    - `“It was the worst of times” = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]`\n",
    "    - `“It was the age of wisdom” = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]`\n",
    "    - `“It was the age of foolishness” = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]`\n",
    "\n",
    "### Features\n",
    "\n",
    "- Bag of Words\n",
    "    \n",
    "    - It is a way of extracting features from the text for use in machine learning algorithms.\n",
    "    \n",
    "    - Use the word vectors, calculate the frequency that each word appears in a document out of all the words in the document.\n",
    "    \n",
    "    - Counting the occurrences of tokens and building a sparse matrix of documents x-tokens.\n",
    "    \n",
    "    - `CountVectorizer()` \n",
    "\n",
    "\n",
    "- Normalized Count Occurrence\n",
    "    \n",
    "    - Term Frequency: (count of keyword in given document)/(count of all words in given doc)\n",
    "    \n",
    "    - TF-IDF without the IDF\n",
    "    \n",
    "    - `TfidfVectorizer(use_idf=False, norm='l2')`\n",
    "\n",
    "\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "    \n",
    "    - Based on the approach that high frequency may not able to provide much information gain. i.e. Rare words contribute more weights to the model.\n",
    "    \n",
    "    - Word importance will be increased if the number of occurrences is high within the same document. On the other hand, it will be decreased if it occurs often in the entire corpus.\n",
    "\n",
    "    - TF: (count of keyword in given document)/(count of all words in given doc)\n",
    "    \n",
    "    - IDF: (number of documents)/(number of documents containing the keyword)\n",
    "\n",
    "    - TF/IDF\n",
    "\n",
    "    - `TfidfVectorizer(use_idf=True, norm='l2')`\n",
    "\n",
    "\n",
    "- Word Embedding\n",
    "\n",
    "    - Representation of document vocabulary.\n",
    "    \n",
    "    - It is capable of capturing context of a word in a document. Context such as semantic and syntactic similarity, relation with other words, etc.\n",
    "    \n",
    "    - Word embeddings are vector representations of a particular word. \n",
    "    \n",
    "    - **Word2Vec** is one of the most popular techniques to learn word embeddings using a shallow neural network. It can be obtained using two methods (both involving neural networks) (1) Skip Gram and (2) Common Bag Of Words (CBOW).\n",
    "\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "- Entities: Identify the type of entity extracted, such as a person, place or organization using Named Entity.\n",
    "- Stemming: Reduce words to their root, or stem. For example, 'running','runs', and 'runned' become 'run'.\n",
    "- Lemmatization: Return the base or dictionary form of a word, which is the lemma. For example 'better' becomes 'good' and 'walking' becomes 'walk'. Lemmatization trys to use context to choose the lemma (truncated form), where stemming just chops down to the root form of the word.\n",
    "- Tokenization: Breaking text up into linguistic units such as words or n-grams.\n",
    "- Corpus: Set of documents, dataset, sample, etc.\n",
    "- Document: A single observation, like the body of an email.\n",
    "- Sentence: What it sounds like.  Sometimes, you want to treat each sentence as a separate document and sometimes you want to merge all the sentences together to be a single document.\n",
    "\n",
    "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "![Analysis of a sentence](https://cdn.glitch.com/c1e65908-81db-4c5b-8274-40cc385dfa54%2Fparts_of_speach_diagramming.jpg?v=1565032453346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
