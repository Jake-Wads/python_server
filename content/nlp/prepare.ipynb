{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Text\n",
    "\n",
    "In this lesson we'll take our acquired data and *parse* it, that is, we'll better understand the text data by breaking it down into smaller components.\n",
    "\n",
    "We'll be using the `nltk` package, which requires a little bit of up-front setup:\n",
    "\n",
    "```bash\n",
    "# We don't need to install nltk, it should come with anaconda, but nltk\n",
    "# does need to download some data.\n",
    "python -c \"import nltk; nltk.download('stopwords')\"\n",
    "```\n",
    "\n",
    "Here's our plan for parsing the text data:\n",
    "\n",
    "1. Convert text to all lower case for normalcy.\n",
    "1. Remove any accented characters, non-ASCII characters.\n",
    "1. Remove special characters.\n",
    "1. Stem or lemmatize the words.\n",
    "1. Remove stopwords.\n",
    "1. Store the clean text and the original text for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Coming into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, exactly? Just what exactly are the data science math and stats principles you need to know?', 'What are the main math principles you need to know to get into Codeup’s Data Science program?'\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"Coming into our Data Science program, you will need to know some math and \\\n",
    "stats. However, many of our applicants actually learn in the application process – you \\\n",
    "don’t need to be an expert before applying! Data science is a very accessible field to \\\n",
    "anyone dedicated to learning new skills, and we can work with any applicant to help them \\\n",
    "learn what they need to know. But what “skills” do we mean, exactly? Just what exactly \\\n",
    "are the data science math and stats principles you need to know?', 'What are the main \\\n",
    "math principles you need to know to get into Codeup’s Data Science program?'\"\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to all lower case for normalcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coming into our data science program, you will need to know some math and stats. however, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. but what “skills” do we mean, exactly? just what exactly are the data science math and stats principles you need to know?', 'what are the main math principles you need to know to get into codeup’s data science program?'\n"
     ]
    }
   ],
   "source": [
    "article = article.lower()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing accented characters\n",
    "\n",
    "Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example is converting é to e.\n",
    "\n",
    "We'll go about this in three steps:\n",
    "\n",
    "1. `unicodedata.normalize` removes any inconsistencies in unicode character encoding.\n",
    "1. `.encode` to convert the resulting string to the ASCII character set. We'll `ignore` any errors in conversion, meaning we'll drop anything that isn't an ASCII character.\n",
    "1. `.decode` to turn the resulting `bytes` object back into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the math and stats principles you need for data science?\n",
      "oct 21, 2020 | data science\n",
      "\n",
      "\n",
      "coming into our data science program, you will need to know some math and stats. however, many of our applicants actually learn in the application process  you dont need to be an expert before applying! data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. but what skills do we mean, exactly?\n"
     ]
    }
   ],
   "source": [
    "article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "print(article[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Special Characters\n",
    "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the math and stats principles you need for data science\n",
      "oct 21 2020  data science\n",
      "\n",
      "\n",
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process  you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean exactly\n"
     ]
    }
   ],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "After removing non-ASCII characters and special characters, it's common to **tokenize** the strings, to break words and any punctuation left over into discrete units. Tokenization is the process of breaking something down into discrete units. In the context of NLP, this means breaking text down into discrete words, punctuation, etc.\n",
    "\n",
    "We will use `nltk` to do tokenization for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the math and stats principles you need for data science\n",
      "oct 21 2020 data science\n",
      "\n",
      "\n",
      "coming into our data science program you will need to know some math and stats however many of our applicants actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skills and we can work with any applicant to help them learn what they need to know but what skills do we mean exactly\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(article, return_str=True)[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Usually you will want to use lemmatization.  We will demonstrate why that is the case by looking at both here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Word **stems** are the base form of a word.\n",
    "\n",
    "We create new words by attaching *affixes* in a process known as *inflection*. For example, \"calls\", \"called\", and \"calling\" all share the base stem \"call\".\n",
    "\n",
    "The Porter stemmer is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules.\n",
    "\n",
    "Note that usually stemming has a fixed set of rules, hence, the root stems may not be lexicographically correct. This means that the stemmed words may not be semantically correct, and might have a chance of not being present in the dictionary (as we'll see in the output of stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "ps.stem('call'), ps.stem('called'), ps.stem('calling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this stemming transformation to all the words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the math and stat principl you need for data scienc oct 21 2020 data scienc come into our data scienc program you will need to know some math and stat howev mani of our applic actual learn in the applic process you dont need to be an expert befor appli data scienc is a veri access field to anyon dedic to learn new skill and we can work with ani applic to help them learn what they need to know but what skill do we mean exactli\n"
     ]
    }
   ],
   "source": [
    "stems = [ps.stem(word) for word in article.split()]\n",
    "article_stemmed = ' '.join(stems)\n",
    "print(article_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to        6\n",
       "need      4\n",
       "data      4\n",
       "scienc    4\n",
       "what      3\n",
       "learn     3\n",
       "applic    3\n",
       "and       3\n",
       "you       3\n",
       "skill     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(stems).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is very similar to stemming, however, the base form in this case is known as the root word, but not the root stem. The difference is that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary.\n",
    "\n",
    "Note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary.\n",
    "\n",
    "Let's take a look at a simple example of the difference between stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: he -- lemma: He\n",
      "stem: wa -- lemma: wa\n",
      "stem: run -- lemma: running\n",
      "stem: and -- lemma: and\n",
      "stem: eat -- lemma: eating\n",
      "stem: at -- lemma: at\n",
      "stem: same -- lemma: same\n",
      "stem: time. -- lemma: time.\n",
      "stem: he -- lemma: He\n",
      "stem: ha -- lemma: ha\n",
      "stem: bad -- lemma: bad\n",
      "stem: habit -- lemma: habit\n",
      "stem: of -- lemma: of\n",
      "stem: swim -- lemma: swimming\n",
      "stem: after -- lemma: after\n",
      "stem: play -- lemma: playing\n",
      "stem: long -- lemma: long\n",
      "stem: hour -- lemma: hour\n",
      "stem: in -- lemma: in\n",
      "stem: the -- lemma: the\n",
      "stem: sun. -- lemma: Sun.\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "for word in sentence.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem: studi -- lemma: studying\n",
      "stem: what -- lemma: what\n",
      "stem: they -- lemma: they\n",
      "stem: need -- lemma: needed\n",
      "stem: to -- lemma: to\n",
      "stem: study, -- lemma: study,\n",
      "stem: the -- lemma: the\n",
      "stem: student -- lemma: student\n",
      "stem: studi -- lemma: studied\n",
      "stem: studious -- lemma: studiously\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in 'studying what they needed to study, the students studied studiously'.split():\n",
    "    print('stem:', ps.stem(word), '-- lemma:', wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can apply lemmatization to our entire document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the math and stats principle you need for data science oct 21 2020 data science coming into our data science program you will need to know some math and stats however many of our applicant actually learn in the application process you dont need to be an expert before applying data science is a very accessible field to anyone dedicated to learning new skill and we can work with any applicant to help them learn what they need to know but what skill do we mean exactly\n"
     ]
    }
   ],
   "source": [
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "\n",
    "print(article_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of the lemmas, we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to           6\n",
       "need         4\n",
       "data         4\n",
       "science      4\n",
       "what         3\n",
       "and          3\n",
       "you          3\n",
       "skill        2\n",
       "know         2\n",
       "applicant    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lemmas).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "Words which have little or no significance, especially when constructing meaningful features from text, are known as **stop words** (or **stopwords**). These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords: a, an, the, and like.\n",
    "\n",
    "While there is no universal stopword list, we will use a standard English language stopwords list from `nltk`. You can also add your own domain-specific stopwords as needed.\n",
    "\n",
    "Before removing stopwords, we want to segment text into linguistic units such as words or numbers. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 41 stopwords\n",
      "---\n",
      "math stats principles need data science oct 21 2020 data science coming data science program need know math stats however many applicants actually learn application process dont need expert applying data science accessible field anyone dedicated learning new skills work applicant help learn need know skills mean exactly\n"
     ]
    }
   ],
   "source": [
    "words = article.split()\n",
    "filtered_words = [w for w in words if w not in stopword_list]\n",
    "\n",
    "print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "print('---')\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "print(article_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end result of this exercise should be a file named `prepare.py` that defines\n",
    "the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data.\n",
    "These functions should apply equally well to both the codeup blog articles and\n",
    "the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named `basic_clean`. It should take in a string and apply\n",
    "   some basic text cleaning to it:\n",
    "\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "1. Define a function named `tokenize`. It should take in a string and tokenize\n",
    "   all the words in the string.\n",
    "\n",
    "1. Define a function named `stem`. It should accept some text and return the\n",
    "   text after applying stemming to all the words.\n",
    "\n",
    "1. Define a function named `lemmatize`. It should accept some text and return\n",
    "   the text after applying lemmatization to each word.\n",
    "\n",
    "1. Define a function named `remove_stopwords`. It should accept some text and\n",
    "   return the text after removing all the stopwords.\n",
    "\n",
    "    This function should define two optional parameters, `extra_words` and\n",
    "    `exclude_words`. These parameters should define any additional stop words to\n",
    "    include, and any words that we *don't* want to remove.\n",
    "\n",
    "1. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`.\n",
    "\n",
    "1. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`.\n",
    "\n",
    "1. For each dataframe, produce the following columns:\n",
    "    - `title` to hold the title\n",
    "    - `original` to hold the original article/post content\n",
    "    - `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "    - `stemmed` to hold the stemmed version of the cleaned data.\n",
    "    - `lemmatized` to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "1. Ask yourself: \n",
    "    - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
