{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Data through Web Scraping\n",
    "\n",
    "When the data you need is not accessible through CSVs, APIs, SQL, or other types, there is an option. This option is known as web scraping.\n",
    "\n",
    "!!!caution \"Web Scraping Ethics\"\n",
    "    Make sure the website's terms of use allow for web scraping. You can generally find a terms of service page, or take a look at `example.com/robots.txt` to find the policy for computers looking at the web site.\n",
    "\n",
    "At a high level, we'll go about web scraping through this process:\n",
    "\n",
    "1. Manually explore the site in a web browser, and identify the relevant HTML elements.\n",
    "1. Use the `requests` module to obtain the HTML from the page.\n",
    "1. Use `BeautifulSoup` to parse the HTML and obtain the text/data that we want.\n",
    "1. (Maybe) Script the process of requesting another page and parsing the data from it as well.\n",
    "1. Take this data further down the data science pipeline.\n",
    "\n",
    "### Steps \n",
    "\n",
    "1. Import the get() function from the requests module, BeautifulSoup from bs4, and pandas.\n",
    "2. Assign the address of the web page to a variable named url.\n",
    "3. Request the server the content of the web page by using get(), and store the server’s response in the variable response.\n",
    "4. Print the response text to ensure you have an html page.\n",
    "5. Take a look at the actual web page contents and inspect the source to understand the structure a bit.\n",
    "6. Use BeautifulSoup to parse the HTML into a variable ('soup').\n",
    "7. Identify the key tags you need to extract the data you are looking for.\n",
    "8. Create a dataframe of the data desired.\n",
    "9. Run some summary stats and inspect the data to ensure you have what you wanted.\n",
    "10. Edit the data structure as needed, especially so that one column has all the text you want included in this analysis.\n",
    "11. Create a corpus of the column with the text you want to analyze.\n",
    "12. Store that corpus for use in a future notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we'll take a look at an article from Codeup's blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/data-science/math-in-data-science/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the request, we'll perform a quick sanity check to make sure what we are looking at is indeed HTML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\" />\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "\t<link rel=\"pingback\" href=\"https://codeup.com/xmlrpc.php\" />\n",
      "\n",
      "\t<script type=\"text/javascript\">\n",
      "\t\tdocument.documentElement.className = 'js';\n",
      "\t</script>\n",
      "\t\n",
      "\t<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin /><script id=\"diviarea-loader\">window.DiviPopupData=wi\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look at the actual web page contents and inspect the source to understand the structure a bit.\n",
    "\n",
    "As we see from the first line of the response, the server sent us an HTML document. This document describes the overall structure of that web page, along with its specific content (which is what makes that particular page unique).\n",
    "\n",
    "For the most part, all of the pages from a single website will have the same (or very similar) overall structure. To write our script, we will need to understand the HTML structure of one page, and we will use the browser’s Developer Tools to do that. \n",
    "\n",
    "- `command + option + u` will let you view the source of a page in chrome.\n",
    "- `command + option + i` will open up the chrome dev tools page inspector.\n",
    "- Right clicking on specific text in the page and selecting 'inspect' will take you right to the html of that text\n",
    "\n",
    "In general, we'll be looking for HTML tags, and using a couple properties of those tags to identify the content that we want. Two element properties are important to us:\n",
    "\n",
    "- `class`: This is a list of the class(es) that are applied to an element, these can be used to target certain elements, but are not guaranteed to be unique.\n",
    "- `id`: This is a unique identifier for an element on a page.\n",
    "\n",
    "We'll use the beautiful soup library to work with HTML data in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a soup variable holding the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup Methods and Properties\n",
    "- `soup.title.string` gets the page's title (the same text in the browser tab for a page, this is the `<title>` element\n",
    "- `soup.prettify()` is useful to print in case you want to see the HTML\n",
    "- `soup.find_all(\"a\")` find all the anchor tags, or whatever argument is specified.\n",
    "- `soup.find(\"h1\")` finds the first matching element\n",
    "- `soup.get_text()` gets the text from within a matching piece of soup/HTML\n",
    "- The `soup.select()` method takes in a CSS selector as a string and returns all matching elements. **super useful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\nWhat are the Math and Stats Principles You Need for Data Science?Oct 21, 2020 | Data Science\\n\\n\\nComing into our Data Science program, you will need to know some math and stats. However, many of our applicants actually learn in the application process – you don’t need to be an expert before applying! Data science is a very accessible field to anyone dedicated to learning new skills, and we can work with any applicant to help them learn what they need to know. But what “skills” do we mean, exactly? Just what exactly are the data science math and stats principles you need to know?\\nWhat are the main math principles you need to know to get into Codeup’s Data Science program?\\n\\n\\nAlgebra\\nDo you know PEMDAS and can you solve for x? You will need to be or become comfortable with the following:\\xa0\\n\\nVariables (x, y, n, etc.)\\nFormulas, functions, and variable manipulations (e.g. x^2 = x + 6, solve for x).\\nOrder of evaluation: PEMDAS: parentheses, exponents, then multiplication, division, addition, and subtraction\\nCommutativity where a + b = b + a\\nAssociativity where a + (b + c) = (a + b) + c\\nAdding and subtracting matrices\\nA conceptual understanding of exponential growth/decay- things can increase at an increasing rate\\n\\nDescriptive Statistics\\nKnow what a min, max, mode, median, and average are. Have a conceptual understanding that stats/probability is about trying to quantify uncertainty.\\nData Visualization\\nKnow what a scatterplot is and how to read a barplot.\\nHow to Learn and Expand on These Concepts\\nThere are a number of great resources out there to teach you these and similar concepts. Khan Academy is a great starting place for data science math! If you want to know what exactly we assign our applicants, you’ll just have to apply!\\n\\xa0\\nWhat about once you’re in Codeup?\\n\\n\\nWhat You Won’t Do\\nDo we do any mathematical proofs for concepts or perform derivations? No.\\xa0\\nDo we do any calculus and probability calculating by hand? No.\\nAre we transforming equations, where we cancel out units or terms and do lots of algebraic gymnastics? No\\nWhat You Will Do\\nWill we have Python solve our linear algebra problems for us? Yes\\nWill we have Python calculate probabilities, the area under a curve, and the slope of a line for us? Yes\\nWill we have Python do all of the calculus for us? Yes\\n\\xa0\\nSee, the data science math and stats slice of the pie is certainly doable. If you like problem-solving and are ready to challenge yourself, you’ll love data science! If you are interested in learning about data science, just apply! Our Admissions Manager can work with you to get you where you need to be starting from where you are now. Let us help you get there so you can launch a great new career.\\n\\n\\n\\n\\n\\nOur ProgramsFull Stack Web Development\\nData Science\\nCyber Cloud\\nSystems Engineering\\n\\n\\n\\n\\n\\nLatest Blog Articles\\nBoris – Behind the Billboards\\nIs Codeup the Best Bootcamp in San Antonio…or the World?\\nCodeup Launches First Podcast: Hire Tech\\nWhy Should I Become a System Administrator?\\nAnnouncing our Candidacy for Accreditation!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see also `soup.find_all`\n",
    "#\n",
    "# beautiful soup uses `class_` as the keyword argument for searching\n",
    "# for a class because `class` is a reserved word in python\n",
    "# we'll use the class name that we identified from looking in the inspector in chrome\n",
    "article = soup.find('div', id='main-content')\n",
    "article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some text to process, we can store it for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article.txt', 'w') as f:\n",
    "    f.write(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now package all of our code up in a nice function that we can use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text():\n",
    "    # if we already have the data, read it locally\n",
    "    if path.exists('article.txt'):\n",
    "        with open('article.txt') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    # otherwise go fetch the data\n",
    "    url = 'https://codeup.com/data-science/math-in-data-science/'\n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    response = get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    article = soup.find('div', id='main-content')\n",
    "    \n",
    "    # save it for next time\n",
    "    with open('article.txt', 'w') as f:\n",
    "        f.write(article.text)\n",
    "    \n",
    "    return article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and CSS Crash Course\n",
    "\n",
    "HTML is the language for content and structure on the web. This means that HTML specifies what content is what: tex, images, links, tables, containers, etc...\n",
    "\n",
    "CSS is the language for styling and presentation. This means CSS specifies color, background, texture, position, etc...\n",
    "\n",
    "### HTML Basics\n",
    "\n",
    "HTML consists of elements denoted by tags. These tags are contained in angle brackets like `<main>`. Notice how there are opening and closing tags that contain other elements.\n",
    "\n",
    "HTML tags nest inside of other HTML tags, just like directories and files are nested in other directories.\n",
    "\n",
    "[Further reading on HTML Elements](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "\n",
    "````html\n",
    "<html>\n",
    "    <head>\n",
    "        <title>This is the title of the page</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <heading>\n",
    "            <h1>Welcome to the blog!</h1>\n",
    "            <p>Blog is short for \"back-log\"</p>\n",
    "        </heading>\n",
    "        <main>\n",
    "            <h2>Read your way to insight!</h2>\n",
    "            <section id=\"posts\">\n",
    "                <article class=\"blog_post\">\n",
    "                    <h3>Hello World</h3>\n",
    "                    <p>This is the first post!</p>\n",
    "                </article>\n",
    "                <article class=\"blog_post\">\n",
    "                    <h3>HTML Is Awesome</h3>\n",
    "                    <p>It's the language and structure for the web!</p>\n",
    "                </article>\n",
    "                <article class=\"blog_post\">\n",
    "                    <h3>CSS Is Totally Rad</h3>\n",
    "                    <p>CSS Selectors are super powerful</p>\n",
    "                </article>\n",
    "            </section>\n",
    "        </main>\n",
    "        <footer>\n",
    "            <p>All rights reserved.</p>\n",
    "        </footer>\n",
    "    </body>\n",
    "</html>\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "### CSS Selectors\n",
    "\n",
    "- The name of the element itself is a selector. For example `soup.select(\"p\")` will select every paragraph tag and `soup.select(\"footer\")` selects the footer element (and everything inside it)\n",
    "- The id selector is denoted with a `#`. For example `soup.select(\"#posts\")` will return the html element noted with the `id=posts` attribute\n",
    "- The class selector is denoted with a `.` symbol before the class name. For example, `soup.select(\".blog_post\")` returns all of the elements that have that class name.\n",
    "\n",
    "[Further reading on CSS Selectors](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) \n",
    "- [Web Scraping with Beautiful Soup Tutorial](https://www.dataquest.io/blog/web-scraping-beautifulsoup/)\n",
    "- [Practitioner's Guide to Understanding Text](https://www.kdnuggets.com/2018/07/practitioners-guide-processing-understanding-text-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "By the end of this exercise, you should have a file named `acquire.py` that\n",
    "contains the specified functions. If you wish, you may break your work into\n",
    "separate files for each website (e.g. `acquire_codeup_blog.py` and\n",
    "`acquire_news_articles.py`), but the end function should be present in\n",
    "`acquire.py` (that is, `acquire.py` should  import `get_blog_articles` from\n",
    "the `acquire_codeup_blog` module.)\n",
    "\n",
    "1. Codeup Blog Articles\n",
    "\n",
    "    Visit [Codeup's Blog](https://codeup.com/blog/) and record the urls for at\n",
    "    least 5 distinct blog posts. For each post, you should scrape at least the\n",
    "    post's title and content.\n",
    "\n",
    "    Encapsulate your work in a function named `get_blog_articles` that will return a\n",
    "    list of dictionaries, with each dictionary representing one article. The shape\n",
    "    of each dictionary should look like this:\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "        'title': 'the title of the article',\n",
    "        'content': 'the full text content of the article'\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Plus any additional properties you think might be helpful.\n",
    "\n",
    "    **Bonus:** Scrape the text of **_all_** the articles linked on [codeup's blog page](https://codeup.com/blog/).\n",
    "\n",
    "1. News Articles\n",
    "\n",
    "    We will now be scraping text data from [inshorts](https://inshorts.com/), a\n",
    "    website that provides a brief overview of many different topics.\n",
    "\n",
    "    Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "    - Business\n",
    "    - Sports\n",
    "    - Technology\n",
    "    - Entertainment\n",
    "\n",
    "    The end product of this should be a function named `get_news_articles` that\n",
    "    returns a list of dictionaries, where each dictionary has this shape:\n",
    "\n",
    "    ```python\n",
    "    {\n",
    "        'title': 'The article title',\n",
    "        'content': 'The article content',\n",
    "        'category': 'business' # for example\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Hints:\n",
    "\n",
    "    1. Start by inspecting the website in your browser. Figure out which\n",
    "       elements will be useful.\n",
    "    1. Start by creating a function that handles a single article and produces a\n",
    "       dictionary like the one above.\n",
    "    1. Next create a function that will find all the articles on a single page\n",
    "       and call the function you created in the last step for every article on\n",
    "       the page.\n",
    "    1. Now create a function that will use the previous two functions to scrape\n",
    "       the articles from all the pages that you need, and do any additional\n",
    "       processing that needs to be done.\n",
    "\n",
    "1. Bonus: cache the data\n",
    "\n",
    "    Write your code such that the acquired data is saved locally in some form or\n",
    "    fashion. Your functions that retrieve the data should prefer to read the\n",
    "    local data instead of having to make all the requests everytime the function\n",
    "    is called. Include a boolean flag in the functions to allow the data to be\n",
    "    acquired \"fresh\" from the actual sources (re-writing your local cache)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
