{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "**What is Logistic Regression?**\n",
    "\n",
    "- Technically a regression algorithm (goal is to find the values for the coefficients that weight each input variable)\n",
    "\n",
    "- Used for predicting discrete outcomes (binomial and multinomial)\n",
    "\n",
    "- Because the prediction for the output is transformed using the logistic function, a non-linear function, it is a classification algorithm.\n",
    "\n",
    "- The output is a value between 0 and 1 that represents the probability of one class over the other. \n",
    "\n",
    "- Like linear regression, logistic regression works better when you remove attributes that are either unrelated to the output variable or correlated to other attributes.\n",
    "\n",
    "\n",
    "Linear vs Logistic Regression       |     Logistic Regression\n",
    ":----------------------------------:|:----------------------------------:\n",
    "![linvlog_reg.png](linvlog_reg.png) | ![logreg.png.png](logreg.png)\n",
    "\n",
    "\n",
    "**Pros**\n",
    "\n",
    "1. High interpretabability. It's explainable to others, i.e. it's useful for understanding the influence of several independent variables on a single outcome variable. \n",
    "\n",
    "2. We can choose to ‘snap’ predictions to 0 and 1 via a rule (such as if < .5, 0 else 1) OR we can choose to use the output as is, which is a probability of being class 1.  \n",
    "\n",
    "3. It’s a fast model and is a good place to start with a benchmark for comparing with other classification algorithms.\n",
    "\n",
    "4. Very efficient and does not require many computational resources. Runs fast.\n",
    "\n",
    "5. Outputs clear predicted probabilities.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "1. Assumes all predictors are independent of each other.\n",
    "\n",
    "2. Missing values must be dealt with prior to fitting the model.\n",
    "\n",
    "3. We can’t solve non-linear problems with logistic regression since it’s decision surface is linear. \n",
    "\n",
    "4. Not always as accurate as other classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  versicolor  virginica\n",
       "1           5.1          3.5           1.4          0.2           0          0\n",
       "2           4.9          3.0           1.4          0.2           0          0\n",
       "3           4.7          3.2           1.3          0.2           0          0\n",
       "4           4.6          3.1           1.5          0.2           0          0\n",
       "5           5.0          3.6           1.4          0.2           0          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "# read Iris data from pydatset\n",
    "df = data('iris')\n",
    "\n",
    "# convert column names to lowercase, replace '.' in column names with '_'\n",
    "df.columns = [col.lower().replace('.', '_') for col in df]\n",
    "\n",
    "# we will have 2 different target variables \n",
    "dummies = pd.get_dummies(df['species'], drop_first=True)\n",
    "\n",
    "df = pd.concat([df, dummies], axis=1).drop(columns=['species'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validate Test\n",
    "\n",
    "Now we'll do our train/validate/test split:\n",
    "\n",
    "- We will walk through the lesson aiming to predict versicolor. \n",
    "\n",
    "- We'll do exploration and train our model on the `train` data\n",
    "\n",
    "- We tune our model on `validate`, since it will be out-of-sample until we use it.\n",
    "\n",
    "- And keep the `test` nice and safe and separate, for our final out-of-sample dataset, to see how well our tuned model performs on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_validate_test_split(df, target, seed=123):\n",
    "    '''\n",
    "    This function takes in a dataframe, the name of the target variable\n",
    "    (for stratification purposes), and an integer for a setting a seed\n",
    "    and splits the data into train, validate and test. \n",
    "    Test is 20% of the original dataset, validate is .30*.80= 24% of the \n",
    "    original dataset, and train is .70*.80= 56% of the original dataset. \n",
    "    The function returns, in this order, train, validate and test dataframes. \n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=0.2, \n",
    "                                            random_state=seed, \n",
    "                                            stratify=df[target])\n",
    "    train, validate = train_test_split(train_validate, test_size=0.3, \n",
    "                                       random_state=seed,\n",
    "                                       stratify=train_validate[target])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, validate, test\n",
    "train, validate, test = train_validate_test_split(df, target='versicolor', seed=123)\n",
    "\n",
    "# create X & y version of train, where y is a series with just the target variable and X are all the features. \n",
    "\n",
    "X_train = train.drop(columns=['versicolor','virginica'])\n",
    "y_train = train.versicolor\n",
    "\n",
    "X_validate = validate.drop(columns=['versicolor','virginica'])\n",
    "y_validate = validate.versicolor\n",
    "\n",
    "X_test = test.drop(columns=['versicolor','virginica'])\n",
    "y_test = test.versicolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class from the `sklearn.linear_model` module to implement our model.\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "**penalty:** str, 'l1' or 'l2', default: 'l2', Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. We will discuss l1 & l2 penalties & regularization\n",
    " \n",
    "**C:** float, default: 1.0, Inverse of regularization strength; must be a positive float. \n",
    "\n",
    "**class_weight:** dict or 'balanced', default: None, Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  \n",
    "\n",
    "**random_state:** set the seed for reproducibility. \n",
    "\n",
    "**intercept_scaling:** float, default 1. Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True and you have not already scaled your data. \n",
    "\n",
    "**solver:** {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default: 'liblinear', but will change to 'lbfgs' in v 0.22. The solver is the algorithm to use in the optimization problem. For small datasets, 'liblinear' is a good choice, whereas 'sag' and  'saga' are faster for large ones. For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes. 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas 'liblinear' and 'saga' handle L1 penalty.\n",
    "*If using sag and saga solvers, make sure the features are on a similar scale.*\n",
    "\n",
    "**max_iter:** int, default: 100, Useful only for the newton-cg, sag and lbfgs solvers, Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "**multi_class:** I recommend using other algorithms for multiclass or one-vs-rest if you want to use logistic regression. options: {'ovr', 'multinomial', 'auto'}, default: 'ovr' (one-versus-rest). If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'.  \n",
    "\n",
    " \n",
    "## Model 1\n",
    "\n",
    "For the first model, we will set the solver to be lbfgs. \n",
    "\n",
    "### Make the Model\n",
    "\n",
    "**Create the object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, class_weight={0:1, 1:99}, random_state=123, intercept_scaling=1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**\n",
    "\n",
    "Fit the logistic regression algorithm to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight={0: 1, 1: 99}, random_state=123)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "Evaluate importance, or weight, of each feature, using the coefficients.\n",
    "\n",
    "Evaluate the intercept of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-0.45745489 -4.33000304  2.00440881 -2.79033335]]\n",
      "Intercept: \n",
      " [14.54733857]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit.coef_)\n",
    "print('Intercept: \\n', logit.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**\n",
    "\n",
    "Estimate whether or not the species is versicolor for each observation, using the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = logit.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate Probability**\n",
    "\n",
    "Estimate the probability of species being versicolor for each observation, using the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_proba = logit.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "**Compute the Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.55\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 38]\n",
      " [ 0 28]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a classificaiton report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.32      0.49        56\n",
      "           1       0.42      1.00      0.60        28\n",
      "\n",
      "    accuracy                           0.55        84\n",
      "   macro avg       0.71      0.66      0.54        84\n",
      "weighted avg       0.81      0.55      0.52        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "We can create new models by changing features we feed the algorithm, hyperparameters, and/or the alogrithm itself. For this second model, we will adjust our hyperparameter, C. \n",
    "\n",
    "### Make the Model\n",
    "\n",
    "**Create the object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2 = LogisticRegression(C=.1, class_weight={0:1, 1:99}, random_state=123, intercept_scaling=1, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**\n",
    "\n",
    "Fit the logistic regreession algorithm to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight={0: 1, 1: 99}, random_state=123)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**\n",
    "\n",
    "Evaluate importance, or weight, of each feature, using the coefficients.\n",
    "\n",
    "Evaluate the intercept of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[-0.20880009 -1.67727952  1.00954681 -0.25663236]]\n",
      "Intercept: \n",
      " [6.02992374]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit2.coef_)\n",
    "print('Intercept: \\n', logit2.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**\n",
    "\n",
    "Estimate whether or not the species is versicolor for each observation, using the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = logit2.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate Probability**\n",
    "\n",
    "Estimate the probability of species being versicolor for each observation, using the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba2 = logit2.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "**Compute the Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.40\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit2.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6 50]\n",
      " [ 0 28]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a classificaiton report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.19        56\n",
      "           1       0.36      1.00      0.53        28\n",
      "\n",
      "    accuracy                           0.40        84\n",
      "   macro avg       0.68      0.55      0.36        84\n",
      "weighted avg       0.79      0.40      0.31        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Precision:** $\\frac{TP}{(TP + FP)}$\n",
    "\n",
    "**Recall:** $\\frac{TP}{(TP + FN)}$\n",
    "\n",
    "**F1-Score:** A measure of accuracy. The harmonic mean of precision & recall. The harmonic mean is the reciprocal of the arithmetic mean of the reciprocals.  \n",
    "\n",
    "F1 $\\in [0, 1]$\n",
    "\n",
    "F1-score = harmonic mean = $\\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$\n",
    "\n",
    "**Support:** number of occurrences of each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Models\n",
    "\n",
    "**Evaluate on Out-of-Sample data**\n",
    "\n",
    "Are either overfitting? Let's validate on unseen data, X_validate. \n",
    "This means we use logit & logit2 to predict on X_validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.53\n",
      "[[ 7 17]\n",
      " [ 0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.29      0.45        24\n",
      "           1       0.41      1.00      0.59        12\n",
      "\n",
      "    accuracy                           0.53        36\n",
      "   macro avg       0.71      0.65      0.52        36\n",
      "weighted avg       0.80      0.53      0.50        36\n",
      "\n",
      "Model 2: solver = lbfgs, c = .1\n",
      "Accuracy: 0.33\n",
      "[[ 0 24]\n",
      " [ 0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.33      1.00      0.50        12\n",
      "\n",
      "    accuracy                           0.33        36\n",
      "   macro avg       0.17      0.50      0.25        36\n",
      "weighted avg       0.11      0.33      0.17        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "y_pred1 = logit.predict(X_validate)\n",
    "y_pred2 = logit2.predict(X_validate)\n",
    "\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "# accuracy of model 1\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_validate, y_validate)))\n",
    "\n",
    "# confusion matrix of model 1\n",
    "print(confusion_matrix(y_validate, y_pred1))\n",
    "\n",
    "# classification report of model 1\n",
    "print(classification_report(y_validate, y_pred1))\n",
    "\n",
    "print(\"Model 2: solver = lbfgs, c = .1\")\n",
    "\n",
    "# accuracy of model 2\n",
    "print('Accuracy: {:.2f}'.format(logit2.score(X_validate, y_validate)))\n",
    "\n",
    "# confusion matrix of model 2\n",
    "print(confusion_matrix(y_validate, y_pred2))\n",
    "\n",
    "# classification report of model 2\n",
    "print(classification_report(y_validate, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "Using the best model, compute the accuracy of the model when run on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.47\n",
      "[[ 4 16]\n",
      " [ 0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33        20\n",
      "           1       0.38      1.00      0.56        10\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.69      0.60      0.44        30\n",
      "weighted avg       0.79      0.47      0.41        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "y_pred_proba = logit.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of probabilities of being versicolor (versicolor == 1)\n",
    "\n",
    "y_pred_proba = np.array([i[1] for i in y_pred_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa04b64ab00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# scatter plot where x is the probabilities and y is the class (0, 1)\n",
    "ax.scatter(y_pred_proba, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Example Logistic Regressions\n",
    "\n",
    "[Binary Logistic Regression](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) predicting likelihood of bank customers to sign up for a new product.\n",
    "\n",
    "[Multinomial Logistic Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html) predicting species from the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In these exercises, we'll continue working with the titanic dataset and\n",
    "building logistic regression models. Throughout this exercise, be sure you\n",
    "are training, evaluation, and comparing models on the train and validate\n",
    "datasets. The test dataset should only be used for your final model.\n",
    "\n",
    "For all of the models you create, choose a threshold that optimizes for\n",
    "accuracy.\n",
    "\n",
    "Create a new notebook, `logistic_regression`, use it to answer the following questions:\n",
    "\n",
    "1. Create a model that includes only age, fare, and pclass.\n",
    "   Does this model perform better than your baseline?\n",
    "\n",
    "2. Include sex in your model as well. Note that you'll need to encode or create a dummy variable of this\n",
    "   feature before including it in a model.\n",
    "\n",
    "3. Try out other combinations of features and models.\n",
    "\n",
    "4. Use you best 3 models to predict and evaluate on your validate sample.\n",
    "\n",
    "5. Choose you best model from the validation performation, and evaluate it on the test dataset. How do the performance metrics compare to validate? to train? \n",
    "\n",
    "**Bonus1** How do different strategies for handling the missing values in\n",
    "   the age column affect model performance?\n",
    "\n",
    "**Bonus2**: How do different strategies for encoding sex affect model\n",
    "   performance?\n",
    "\n",
    "**Bonus3**: `scikit-learn`'s `LogisticRegression` classifier is actually\n",
    "   applying [a regularization penalty to the coefficients][1] by default.\n",
    "   This penalty causes the magnitude of the coefficients in the resulting\n",
    "   model to be smaller than they otherwise would be. This value can be\n",
    "   modified with the `C` hyper parameter. Small values of `C` correspond to\n",
    "   a larger penalty, and large values of `C` correspond to a smaller penalty.  \n",
    "   Try out the following values for `C` and note how the coefficients and\n",
    "   the model's performance on both the dataset it was trained on and on the\n",
    "   validate split are affected.\n",
    "   $$ C = .01, .1, 1, 10, 100, 1000$$\n",
    "    \n",
    "**Bonus Bonus**: how does scaling the data interact with your choice of `C`?\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Regularized_least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
