{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation\n",
    "\n",
    "In this lesson, we will discuss common ways of evaluating a classification model's performance.\n",
    "\n",
    "**Note** To simplify things, this lesson uses a single dataframe and does not perform data splitting. The methods used in this section along with the `classification_report` function, which you'll see in the next lesson, are used to evaluate model performance on `train` to see in-sample performance, on `validate` to see out-of-sample performance and allow us to tune hyper-parameters, and ultimately on the `test` dataset.\n",
    "\n",
    "## The Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a cross-tabulation of our model's predictions against the actual values. We will discuss the different evaluation metrics in this lesson.\n",
    "\n",
    "![annotated confusion matrix](./annotated_confusion_matrix.png)\n",
    "\n",
    "\n",
    "As a simple example, imagine we are predicting whether or not someone likes coffee. Our data and predictions might look like this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coffee</td>\n",
       "      <td>no coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual prediction\n",
       "0     coffee  no coffee\n",
       "1  no coffee  no coffee\n",
       "2  no coffee     coffee\n",
       "3     coffee     coffee\n",
       "4     coffee     coffee\n",
       "5     coffee     coffee\n",
       "6  no coffee  no coffee\n",
       "7     coffee  no coffee"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crosstab matrix of 'actual' and 'prediction' columns would look like this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>coffee</th>\n",
       "      <th>no coffee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no coffee</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction  coffee  no coffee\n",
       "actual                       \n",
       "coffee           3          2\n",
       "no coffee        1          2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.actual, df.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn confusion matrix would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.actual, df.prediction,\n",
    "                 labels = ('no coffee', 'coffee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix here represent the 4 possible outcomes of our classification task:\n",
    "\n",
    "- c[0,0]: There are 2 **True Negatives**, where we predicted the people don't like cofee and they really don't\n",
    "\n",
    "- c[0:1]: There is 1 **False Positive**, where we predicted the person likes coffee but they really don't\n",
    "- c[1,0]: There are 2 **False Negatives**, where we predicted those people don't like coffee, but they really do\n",
    "- c[1,1]: There are 3 **True Positives**, that is for 4 people they really do like coffee and we predicted they do\n",
    "\n",
    "\n",
    "!!!note \"Positives and Negatives\"\n",
    "    Here we are treating liking coffee as the positive case and not liking coffee as the negative case. This choice is arbitrary and we could have chosen not liking coffee as the positive case and liking cofee as the negative case.\n",
    "    \n",
    "    Either way, when discussing classification model performance, you'll see one outcome classified as positive and the other as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "For a classification problem, a common choice for the baseline model is a model that simply predicts the most common class every single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coffee       5\n",
       "no coffee    3\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, there are 5 coffee drinkers and 3 non-coffee drinkers, so our baseline model would be to predict that someone likes coffee every single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline_prediction'] = 'coffee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Evaluation Metrics\n",
    "\n",
    "Now that we have introduced the idea of a confusion matrix, we can discuss some metrics that are derived from it.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is the number of times we predicted correctly divided by the total number of observations. Put another way:\n",
    "\n",
    "$$ \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}} $$\n",
    "\n",
    "In our example above, this would be\n",
    "\n",
    "$$ \\frac{3 + 2}{3 + 2 + 1 + 2} = \\frac{5}{8} = 0.625 $$\n",
    "\n",
    "So our model's overall accuracy is 62.5%.\n",
    "\n",
    "Accuracy is a good, easy to understand metric, but can fail to capture the whole picture when the classes in the original dataset are not evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "!!!note \"Positives and Negatives\"\n",
    "    While the overall accuracy will remain the same no matter which outcome we designate as the positive and the negative, because of their definition, precision and recall **are** affected by these choices.\n",
    "    \n",
    "Precision is the percentage of positive predictions that we made that are correct. Precision tells us how good our model's positive predictions are, and does not take into account false negatives or true negatives. More formally:\n",
    "\n",
    "$$ \\frac{\\text{TP}}{\\text{TP + FP}} $$\n",
    "\n",
    "In our example:\n",
    "\n",
    "$$ \\frac{3}{3 + 1} = 0.75 $$\n",
    "\n",
    "That is, 75% of the time that we predicted someone likes coffee, we were right.\n",
    "\n",
    "We might choose to optimize for precision when the cost of acting on a positive prediction is high. With precision as a metric, false negatives are \"free\", but false positives are costly. For example we might optimize for precision when predicting whether or not an email message is spam, as it is better to send a spam message to a user's inbox than it is to send a real message to the spam folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall is the percentage of positive cases that we accurately predicted. Recall tells us how well our model does at \"capturing\" the actually positve cases. Recall does not take into account false positives or True negatives.\n",
    "\n",
    "$$ \\frac{TP}{TP + FN} $$\n",
    "\n",
    "In our example:\n",
    "\n",
    "$$ \\frac{3}{3 + 2} = 0.6 $$\n",
    "\n",
    "We predicted 60% of the people that like coffee correctly.\n",
    "\n",
    "We might choose to optimize for recall when the cost of missing out on a positive case is high, or when it is better to act on a predicted positive than not to. With recall as a metric, false positives are \"free\", but false negatives are costly. For example, we might optimize for recall when trying to flag fradulent bank transactions, as it is better to flag is non-fraudulent transaction for review than it is to miss out on an actually fraudulent transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics\n",
    "\n",
    "While the metrics above are some of the most common, they are not by far an exhaustive list. Here is an overview of several other common metrics:\n",
    "\n",
    "- **Misclassification Rate**: 1 - accuracy; how often does the model get it wrong?\n",
    "- **Sensitivity**: aka *True Positive Rate*; how good is our model when the actual value is positive? recall for the positive class\n",
    "- **Specificity**: How good is our model when the actual value is negative? Recall for the negative class\n",
    "- **False positive rate**: How likely is it we get a false positive when the actual value is negative?\n",
    "- **F1 Score**: the harmonic mean of precision and recall\n",
    "- **Area Under ROC Curve**: A way to measure overall model performance for models that predict not just a class, but a probability as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now we will put the metrics we've discussed into practice with python code.\n",
    "\n",
    "First we can calculate accuracy.\n",
    "Accuracy is simply the number of times where we got the prediction right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model accuracy: 62.50%\n",
      "baseline accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "baseline_accuracy = (df.baseline_prediction == df.actual).mean()\n",
    "\n",
    "print(f'   model accuracy: {model_accuracy:.2%}')\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is how well we do on actually positive cases. Here we'll define positive as preferring coffe.\n",
    "\n",
    "First we'll subset the dataframe so that we are only looking at the rows where we have the positive case.\n",
    "Then we'll evaluate how well our model's predictions do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model recall: 60.00%\n",
      "baseline recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "subset = df[df.actual == 'coffee']\n",
    "\n",
    "model_recall = (subset.prediction == subset.actual).mean()\n",
    "baseline_recall = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "print(f'   model recall: {model_recall:.2%}')\n",
    "print(f'baseline recall: {baseline_recall:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that our baseline model has 100% recall.\n",
    "This is because the baseline is to always predict the person prefers coffee, so we'll never miss an actually positive case.\n",
    "\n",
    "Next we'll calculate precision.\n",
    "Precision is based on just the times that the model predicts the positive class.\n",
    "Because the predictions for our model and the baseline differ, we'll need to create 2 seperate subsets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 75.00%\n",
      "baseline precision: 62.50%\n"
     ]
    }
   ],
   "source": [
    "subset = df[df.prediction == 'coffee']\n",
    "model_precision = (subset.prediction == subset.actual).mean()\n",
    "\n",
    "subset = df[df.baseline_prediction == 'coffee']\n",
    "baseline_precision = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the baseline model's precision is the same as it's accuracy. This is because the baseline model always predicts the positive case, so the subset of the data used for the precision calculation is the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "All of the above metrics can be applied to a multi-class classfication problems as well.\n",
    "Overall, we treat the multiclass classification performance evaluation as a sequence of binary classification performance evaluations, one for each class.\n",
    "This approach is sometimes referred to as **one-vs-rest**.\n",
    "\n",
    "The steps for doing so are:\n",
    "\n",
    "1. Look at one class individually. Treat correctly identifying the class as the positive case.\n",
    "1. Compute performance metrics for this class.\n",
    "1. Repeat for every other classes.\n",
    "1. Average the performance metrics together.\n",
    "\n",
    "The average calculation can be performed by simply averaging all the metrics together and dividing by the number of data points (a **macro average**), or by a weighted average.\n",
    "For the weighted average, we weight each metric by the number of data points in the class and divide by the total number of data points.\n",
    "This process is referred to as a **micro average**.\n",
    "\n",
    "One way to think about this is that the macro average weighs each class equally, while the micro average weighs each observation equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Wikipedia: Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "- [Wikipedia: Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "- [A helpful notebook on classification model evaluation](https://www.ritchieng.com/machine-learning-evaluate-classification-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Create a new file named `model_evaluation.py` or `model_evaluation.ipynb` for these exercises.\n",
    "\n",
    "1. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "        |               | pred dog   | pred cat   |\n",
    "        |:------------  |-----------:|-----------:|\n",
    "        | actual dog    |         46 |         7  |\n",
    "        | actual cat    |         13 |         34 |\n",
    "        \n",
    "\n",
    "    - In the context of this problem, what is a false positive?\n",
    "    - In the context of this problem, what is a false negative?\n",
    "    - How would you describe this model?\n",
    "\n",
    "1. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant. \n",
    "\n",
    "    Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions [can be found here](https://ds.codeup.com/data/c3.csv).\n",
    "    \n",
    "    Use the predictions dataset and pandas to help answer the following questions:\n",
    "    \n",
    "    - An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "    - Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "    \n",
    "1. You are working as a data scientist for Gives You Paws &#8482;, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "    At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "    Several models have already been developed with the data, and [you can find their results here](https://ds.codeup.com/data/gives_you_paws.csv).\n",
    "    \n",
    "    Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "    1. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?\n",
    "    1. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?\n",
    "    1. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?\n",
    "\n",
    "1. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "    - [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "    - [`sklearn.metrics.precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "    - [`sklearn.metrics.recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "    - [`sklearn.metrics.classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
