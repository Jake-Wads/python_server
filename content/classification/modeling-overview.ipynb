{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4b810a",
   "metadata": {},
   "source": [
    "# Modeling Overview\n",
    "\n",
    "Planning - Acquisition - Preparation - Exploratory Analysis - **Modeling** - Product Delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615d42",
   "metadata": {},
   "source": [
    "### Decision Tree (CART: Classification And Regression Trees)\n",
    "\n",
    "[sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "- A sequence of rules used to classify 2 or more classes.\n",
    "- Each node represents a single input variable (x) and a split point or class of that variable.\n",
    "- The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.  \n",
    "- Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Simple to understand, visualize & explain. | Risk of Overfitting: Can create complex trees that do not generalize well.  \n",
    "Requires little data preparation. | Can be unstable because small variations in the data might lead to overfitting.  \n",
    "Can handle both numerical and categorical data. | \n",
    "Performs well for a broad range of problems. |  \n",
    "\n",
    "Example below:  If an observation has a length of 45, blue eyes, and 2 legs, it's going to be classified as red.\n",
    "\n",
    "![decision_tree.png](decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a296d3d",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "Random forest is an implementation of bootstrap aggregation, aka bagging, which is an ensemble algorithm. \n",
    "\n",
    "Bootstrapping is a statistical method for estimating a quantity from a data sample, e.g. mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value. In bootstrap aggregation, or bagging, the same approach is used for estimating entire statistical models, such as decision trees. Multiple samples of your training data are taken and models are constructed for each sample set.\n",
    "\n",
    "When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimation of the true output value.\n",
    "\n",
    "Random forest is a tweak on this approach of bootstrapping, where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.  The models created for each sample of the data are therefore more different than they otherwise would be, in normal bootstrapping, but still accurate in their unique and different ways. This combines their prediction results in a better estimate of the true underlying output value.\n",
    "\n",
    "If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm, e.g. using a random forest.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Less risk of overfitting than with a decision tree. | High demand on computational resources.\n",
    "More accurate than decision trees in most cases. | Difficult to implement.\n",
    "| Somewhat of a blackbox model, difficult to explain.\n",
    "\n",
    "![random_forest.png](random_forest.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e30d9",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "- Technically a regression algorithm (Goal is to find the values for the coefficients that weight each input variable).\n",
    "- Used to predict binary outcomes.\n",
    "- The output is a value between 0 and 1 that represents the probability of one class over the other. \n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Interpretable: Good for understanding the influence of _several_ independent variables on a _single_ outcome variable. | Need to remove attributes which are either unrelated to the output variable or correlated to other attributes.  \n",
    "Flexible: We can choose to ‘snap’ predictions to 0 and 1 via a rule (such as if < .5, 0 else 1) OR we can choose to use the output as is, which is a probability of being class 1.  | Not one of the top performing classification algorithms. \n",
    "Easy to implement, meaning it is good to use for creating a benchmark. |\n",
    "Very efficient and does not require many computational resources.|\n",
    "\n",
    "Linear vs Logistic Regression       |     Logistic Regression\n",
    ":----------------------------------:|:----------------------------------:\n",
    "![linvlog_reg.png](linvlog_reg.png) | ![logreg.png.png](logreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bb244",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor\n",
    "\n",
    "### Description\n",
    "\n",
    "[sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "K-Nearest Neighbor (KNN) makes predictions based on how close a new data point is to known data points.\n",
    "\n",
    "It is considered \"lazy\" as it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the K nearest neighbors of each point.\n",
    "\n",
    "Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances.  For regression problems, this might be the mean output variable. For classification problems, this might be the mode (or most common) class value.\n",
    "\n",
    "It is important to define a metric to measure the similarity between data instances. Euclidean distance can be used if attributes are all on the same scale (or you convert them to the same scale).\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Simple to implement. | Need to determine the value of K.\n",
    "Robust to noise. | High Computational Cost: It has to compute the distance of each instance to all the training samples...you have to hang on to your entire training dataset.  \n",
    "Performs calculations \"just in time\", i.e. when a prediction is needed (as opposed to ahead of time) | \"Curse of dimensionality\": Distance can break down in very high dimensions, negatively affecting the performance.  \n",
    "Training instances can be updated and curated over time to keep predictions accurate. |\n",
    "\n",
    "![KNN.png](KNN.png)\n",
    "\n",
    "- [Tutorial on an implementation of KNN in python](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983032a",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "A technique that uses higher dimensions to best separate data points into two classes.\n",
    "\n",
    "Support Vector Machines select hyperplanes (a line that splits the input variable space) to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line. \n",
    "\n",
    "An optimization algorithm is used to find the values for the coefficients that maximizes the margin. The distance between the hyperplane and the closest data points is referred to as the **margin**. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points, called the **support vectors**, are relevant in defining the hyperplane and in the construction of the classifier.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Effective in high dimensional spaces | Does not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.  \n",
    "Memory efficient: Uses a subset of training points in the decision function | \n",
    "Highly successful classifier | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e784d",
   "metadata": {},
   "source": [
    "### Naïve Bayes\n",
    "\n",
    "Naive Bayes is based on Bayes’ theorem that assumes independence between every pair of features.\n",
    "\n",
    "It is comprised of two types of probabilities that can be calculated directly from your training data:\n",
    "\n",
    "- The probability of each class\n",
    "- The conditional probability for each class given each x value\n",
    "\n",
    "Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem.  When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities. (so normalize your data!)\n",
    " \n",
    "It assumes that each input variable is independent (which is often not the case), thus it is called \"naive\". This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems, including document classification and spam filtering.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Works with a smaller sample size of training data than other classifiers | Can be a bad estimator if used in less than ideal problems.  \n",
    "Extremely fast compared to more sophisticated methods. | \n",
    "Simple & Powerful | \n",
    "\n",
    "Use cases: \n",
    "\n",
    "- Based on their purchase and browsing history, what promos should I offer to my customers?\n",
    "\n",
    "- Learn from IB to develop methods for prospecting new customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
