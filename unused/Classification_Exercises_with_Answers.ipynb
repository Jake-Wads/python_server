{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Data Acquisition & Summary\n",
    "\n",
    "\n",
    "1. Use seaborn to load the iris data set into a dataframe, `df_iris`\n",
    "\n",
    "    - print the first 3 rows \n",
    "    \n",
    "    - print the number of rows and columns (shape)\n",
    "    \n",
    "    - print the column names\n",
    "    \n",
    "    - print the data type of each column\n",
    "    \n",
    "    - print the summary statistics for each of the numeric variables.  Would you recommend rescaling the data based on these statistics?\n",
    "    \n",
    "    \n",
    "2. Read the data tab from the stats module dataset, Excel_Stats.xlsx, into a dataframe, `df_excel`\n",
    "    \n",
    "    - assign the first 100 rows to a new dataframe, `df_excel_sample`\n",
    "    \n",
    "    - print the number of rows of your original dataframe\n",
    "    \n",
    "    - print the first 5 column names\n",
    "    \n",
    "    - print the column names that have a data type of `object`\n",
    "    \n",
    "    - compute the range for each of the numeric variables.\n",
    "    \n",
    "    \n",
    "3. Read train.csv from google drive (shared through classroom in topic 'Classification') into a dataframe labeled `df_google`\n",
    "    \n",
    "    - print the first 3 rows \n",
    "    \n",
    "    - print the number of rows and columns\n",
    "    \n",
    "    - print the column names\n",
    "    \n",
    "    - print the data type of each column\n",
    "    \n",
    "    - print the summary statistics for each of the numeric variables\n",
    "    \n",
    "    - print the unique values for each of your categorical variables\n",
    "\n",
    "\n",
    "4. In mysql workbench or a terminal, write a query to select all the columns of titanic_db.passengers. Export that table to a csv you store locally.  Read that csv into a dataframe `df_csv`. \n",
    "\n",
    "    - print the head and tail of your new dataframe\n",
    "    \n",
    "    - print the number of rows and columns \n",
    "    \n",
    "    - print the column names\n",
    "    \n",
    "    - print the data type of each column\n",
    "    \n",
    "    - print the summary statistics for each numeric variable\n",
    "    \n",
    "    - print the unique values for each categorical variables.  If there are more than 5 distince values, print the top 5 in terms of prevelence or frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using df_iris\n",
    "\n",
    "Scenario: A local flower shop is trying to identify the species of the iris that are distributed to their shop to sell.  They tags identifying the species are missing.  They want to both *understand* the differences in these species (exploratory analysis stage) and have a model that will label the species for these plants that arrived not labeled.  \n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Compute 2 new variables, add to your existing columns, and assign to a new dataframe `df_with_area`: sepal_area, petal_area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "\n",
    "1. Split data into train (70%) & test (30%) samples.  You should end with 2 data frames: `train_df` and `test_df`\n",
    "\n",
    "> `train_df, test_df = train_test_split(df_with_area, test_size = .30,  \n",
    "                                random_state = 123,    \n",
    "                                stratify = df[['species']])`   \n",
    "                               \n",
    "                               \n",
    "2. Create a swarmplot where the x-axis is each of the independent variable names (petal_length, petal_width, etc).  The y-axis is the value of the variable.  Use color to represent species as another dimension.  Hint: You will to 'melt' the dataframe into a 'long' dataframe in order to accomplish this.  What are your takeaways from this visualization?  \n",
    "\n",
    "> `sns.set(style=\"whitegrid\", palette=\"muted\")   \n",
    "    train_melt = pd.melt(train_df, \"species\", var_name=\"measurement\")   \n",
    "    plt.figure(figsize=(8,6))  \n",
    "    sns.swarmplot(x=\"measurement\", y=\"value\", hue=\"species\",   \n",
    "        palette=[\"r\", \"c\", \"y\"], data=train_melt)`   \n",
    "              \n",
    "\n",
    "> We can see a clear separation within both petal length and petal width for the 3 species. Sepal length shows a separation of setosa from the other two.  But virginica and versicolor are pretty mixed in there.  If we calculate sepal area and petal area, maybe we can find more space between the 3 species.\n",
    "\n",
    "\n",
    "2. Create 4 subplots (2 rows x 2 columns) of scatterplots \n",
    "    - sepal_length x sepal_width \n",
    "    - petal_length x petal_width\n",
    "    - sepal_area x petal_area\n",
    "    - sepal_length x petal_length\n",
    "    - Make your figure size 14 x 8.  What are your takeaways?\n",
    "\n",
    "\n",
    "> `plt.figure(figsize=(14,8))  \n",
    "    plt.subplot(2,2,1)  \n",
    "    plt.scatter(x='sepal_length', y='sepal_width', hue='species',   \n",
    "        palette=[\"r\", \"c\", \"y\"], data=train_df)     \n",
    "    plt.title('Sepal Length & Width => Species?')`  \n",
    "\n",
    "> `plt.subplot(2,2,2)   \n",
    "    plt.scatter(x='petal_length', y='petal_width', hue='species',  \n",
    "       palette=[\"r\", \"c\", \"y\"], data=train_df)  \n",
    "    plt.title('Petal Length & Width => Species?')`   \n",
    "\n",
    "> `plt.subplot(2,2,3)  \n",
    "    plt.scatter(x='sepal_area', y='petal_area', hue='species',    \n",
    "        palette=[\"r\", \"c\", \"y\"], data=train_df)  \n",
    "    plt.title('Area of Sepals & Petals  => Species?')`   \n",
    "\n",
    "> `plt.subplot(2,2,4)   \n",
    "    plt.scatter(x='sepal_length', y='petal_length', hue='species',   \n",
    "        palette=[\"r\", \"c\", \"y\"], data=train_df)   \n",
    "    plt.title('Length of Sepals & Petals => Species?')`   \n",
    "\n",
    "\n",
    "3. Create a heatmap of each variable layering correlation coefficient on top.  \n",
    "\n",
    "> `plt.figure(figsize=(8,6))   \n",
    "    sns.heatmap(train.corr(), cmap='Blues', annot=True)`  \n",
    "\n",
    "\n",
    "4. Create a scatter matrix visualizing the interaction of each variable \n",
    "\n",
    "> `from pandas.tools.plotting import scatter_matrix   \n",
    "    from matplotlib import cm    \n",
    "    cmap = cm.get_cmap('gnuplot')   \n",
    "    scatter = pd.scatter_matrix(train, marker = 'o', s=40, hist_kwds={'bins':15},  \n",
    "        figsize=(9,9), cmap = cmap)`\n",
    "\n",
    "\n",
    "5. Is the sepal length significantly different in virginica than versicolor? Run an experiment to test this. \n",
    "\n",
    "    - must include null hypothesis, alternative hyp, t-test, results, summary \n",
    "    \n",
    "    - H0: the difference in sepal length between virginica and versicolor is insignificant.\n",
    "    \n",
    "    - Ha: the difference in sepal length between virginica and versicolor is substantial.\n",
    "    \n",
    "    - We will test if the sepal length of virginica is significantly different than that of the versicolor.\n",
    "    \n",
    "    - If there is difference, then variable sepal_length is a good choice to keep as a feature.\n",
    "    \n",
    "    - We can use a t-test here, as sepal_length is somwhat normally distributed.    \n",
    "\n",
    "\n",
    "> `import scipy as sp    \n",
    "    import numpy as np   \n",
    "    sp.stats.ttest_ind(train.dropna()[train['species']=='virginica']['sepal_length'],    \n",
    "        train.dropna()[train['species']=='versicolor']['sepal_width'])`  \n",
    "                   \n",
    "> We 'fail to confirm the null hypothesis' that there is no difference in `sepal_length` between versica and verginica species.  Therefore, it is a good idea to keep `sepal_length` as a feature. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Modeling\n",
    "\n",
    "##### Logistic Regression\n",
    "\n",
    "2. Fit the logistic regression classifier to your training sample and transform, i.e. make predictions on the training sample\n",
    "3. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n",
    "4. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support. \n",
    "5. Look in the scikit-learn documentation to research the `solver` parameter.  What is your best option(s) for the particular problem you are trying to solve and the data to be used? \n",
    "6. Run through steps 2-4 using another `solver` (from question 5) \n",
    "7. Which performs better on your in-sample data?\n",
    "8. Save the best model in `logit_fit`\n",
    "\n",
    "##### Decision Tree\n",
    "\n",
    "2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample) \n",
    "3. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n",
    "4. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support. \n",
    "5. Run through steps 2-4 using entropy as your measure of impurity. \n",
    "7. Which performs better on your in-sample data?\n",
    "8. Save the best model in `tree_fit`\n",
    "\n",
    "##### KNN\n",
    "\n",
    "2. Fit the K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample) \n",
    "3. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "4. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support. \n",
    "5. Run through steps 2-4 setting k to 10\n",
    "6. Run through setps 2-4 setting k to 20\n",
    "7. What are the differences in the evaluation metrics?  Which performs better on your in-sample data? Why?\n",
    "8. Save the best model in `knn_fit`\n",
    "\n",
    "##### Random Forest\n",
    "\n",
    "2. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 20.  \n",
    "3. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "4. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support. \n",
    "6. Run through steps increasing your min_samples_leaf to 5 and decreasing your max_depth to 3.  \n",
    "7. What are the differences in the evaluation metrics?  Which performs better on your in-sample data?  Why?  \n",
    "8. Save the best model in `forest_fit`\n",
    "\n",
    "##### Test\n",
    "\n",
    "Once you have determined which algorithm (with metaparameters) performs the best, try reducing the number of features to the top 4 features in terms of information gained for each feature individually. That is, how close do we get to predicting accurately the species with each feature?  \n",
    "\n",
    "1. Compute the information gained. \n",
    "2. Create a new dataframe with top 4 features (`train_df_reduced`).  \n",
    "3. Use the top performing algorithm with the metaparameters used in that model. Create the object, fit, transform on in-sample data, and evaluate the results.  Compare your evaluation metrics with those from the original model (with all the features).  Select the best model. \n",
    "4. Run your final model on your out-of-sample dataframe (`test_df`). Evaluatethe results.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
