{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA Pneumonia Detection Challenge\n",
    "## Can you build an algorithm that automatically detects potential pneumonia cases?\n",
    "\n",
    "https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\n",
    "\n",
    "### Overview\n",
    "\n",
    "n this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.\n",
    "\n",
    "Here’s the backstory and why solving this problem matters.\n",
    "\n",
    "Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.\n",
    "\n",
    "While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.\n",
    "\n",
    "CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.\n",
    "\n",
    "To improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA®) has reached out to Kaggle’s machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.\n",
    "\n",
    "RSNA Banner\n",
    "\n",
    "The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.\n",
    "\n",
    "Challenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.\n",
    "\n",
    "Acknowledgements\n",
    "Thank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].\n",
    "\n",
    "NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\n",
    "\n",
    "Original source files and documents\n",
    "\n",
    "Also, a big thank you to the competition organizers!\n",
    "\n",
    "References\n",
    "Rui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables. Table 27. Available from: www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf\n",
    "\n",
    "Deaths: Final Data for 2015. Supplemental Tables. Tables I-21, I-22. Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf\n",
    "\n",
    "Franquet T. Imaging of community-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print). PMID 30036297\n",
    "\n",
    "Kelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148\n",
    "\n",
    "Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:\n",
    "IoU(A,B)=A∩BA∪B.\n",
    "The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.4 to 0.75 with a step size of 0.05: (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n",
    "\n",
    "At each threshold value t, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n",
    "TP(t)TP(t)+FP(t)+FN(t).\n",
    "A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object.\n",
    "\n",
    "Important note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.\n",
    "\n",
    "The average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:\n",
    "1|thresholds|∑tTP(t)TP(t)+FP(t)+FN(t).\n",
    "In your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.\n",
    "\n",
    "NOTE: In nearly all cases confidence will have no impact on scoring. It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases. None of these edge cases are known to exist in the data set. If you do not wish to use or calculate confidence you can use a placeholder value - like 1.0 - to indicate that no particular order applies to the evaluation of your submission boxes.\n",
    "\n",
    "Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n",
    "\n",
    "Intersection over Union (IoU)\n",
    "Intersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects). It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.\n",
    "\n",
    "It can be visualized as the following:\n",
    "\n",
    "Image of Intersection over Union\n",
    "\n",
    "The two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together. IoU would be low - and would likely not count as a \"hit\" at higher IoU thresholds.\n",
    "\n",
    "Submission File\n",
    "The submission format requires a space delimited set of bounding boxes. For example:\n",
    "\n",
    "0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\n",
    "\n",
    "indicates that image 0004cfab-14fd-4e49-80ba-63a80b6bddd6 has a bounding box with a confidence of 0.5, at x == 0 and y == 0, with a width and height of 100.\n",
    "\n",
    "The file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.\n",
    "\n",
    "patientId,PredictionString\n",
    "0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\n",
    "00313ee0-9eaa-42f4-b0ab-c148ed3241cd,\n",
    "00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5\n",
    "etc...\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "Kaggle and RSNA would like to recognize the radiologists from the Society of Thoracic Radiology and RSNA who contributed the considerable effort required to annotate the datasets in preparation for the challenge:\n",
    "\n",
    "Society of Thoracic Radiology\n",
    "Judith Amorosa, MD- Rutgers Robert Wood Johnson Medical School\n",
    "Veronica Arteaga, MD - University of Arizona *\n",
    "Maya Galperin-Aizenberg, MD - University of Pennsylvania\n",
    "Ritu Gill, MD - Beth Israel Deaconess Medical Center *\n",
    "Myrna Godoy, MD, PhD - MD Anderson Cancer Center *\n",
    "Stephen Hobbs, MD - University of Kentucky *\n",
    "Jean Jeudy, MD - University of Maryland *\n",
    "Archana Laroia, MD - University of Iowa *\n",
    "Palmi Shah, MD - Rush University *\n",
    "Dharshan Vummidi, MD - University of Michigan *\n",
    "Carol Wu, MD - MD Anderson Cancer Center *\n",
    "Kavitha Yaddanapudi, MD - Stony Brook School of Medicine *\n",
    "Radiological Society of North America\n",
    "Tessa Cook, MD, PhD - University of Pennsylvania *\n",
    "Safwan Halabi, MD - Stanford University *\n",
    "Marc Kohli, MD - University of California - San Francisco *\n",
    "Luciano M Prevedello, MD, MPH - The Ohio State University *\n",
    "Arjun Sharma, MD - Amita Health *\n",
    "George Shih, MD - Weill Cornell Medicine *\n",
    "* 1,000 cases or more\n",
    "\n",
    "Special thanks to Anouk Stein, MD of MD.ai, who contributed the annotation tools used in creating the challenge datasets, helped to organize the team of annotators and provided extensive consulting on the annotation process.\n",
    "\n",
    "Special thanks to Jayashree Kalpathy-Cramer, PhD, Massachusetts General Hospital; Peter Chang, MD and John Mongan, MD, PhD, University of California - San Francisco; and George Shih, MD, Weill Cornell Medicine for their work in validating the evaluation metric used in the challenge.\n",
    "\n",
    "Finally, thanks to the members of the RSNA Machine Learning Steering and Machine Learning Data Standards subcommittees, who were responsible for designing, organizing and conducting the challenge in collaboration with Kaggle:\n",
    "\n",
    "Katherine P. Andriole, PhD - MGH & BWH Center for Clinical Data Science\n",
    "Falgun H. Chokshi, MD - Emory University\n",
    "Brad Erickson, MD - Mayo Clinic\n",
    "Adam Flanders, MD - Thomas Jefferson University\n",
    "Safwan Halabi, MD - Stanford University\n",
    "Jayashree Kalpathy-Cramer, PhD - Massachusetts General Hospital\n",
    "Marc Kohli, University of California, MD - San Francisco\n",
    "Luciano Prevedello, MD, MPH - The Ohio State University\n",
    "George Shih, MD - Weill Cornell Medicine\n",
    "Carol Wu, MD - MD Anderson Cancer Center\n",
    "\n",
    "### Data\n",
    "\n",
    "STAGE 2 UPDATE\n",
    "Note that new files are available to download! The training set now contains both the train and test set from stage 1. The test set is comprised of new, unseen images. The metric and file formats remain the same, but you'll now be making predictions using the updated train and test sets. We have an FAQ about two-stage competitions that provides some context for how this all works. Please give it a read.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "What files do I need?\n",
    "This is a two-stage challenge. You will need the images for the current stage - provided as stage_2_train_images.zip and stage_2_test_images.zip. You will also need the training data - stage_2_train_labels.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. The file stage_2_detailed_class_info.csv contains detailed information about the positive and negative classes in the training set, and may be used to build more nuanced models.\n",
    "\n",
    "What should I expect the data format to be?\n",
    "The training data is provided as a set of patientIds and bounding boxes. Bounding boxes are defined as follows: x-min y-min width height\n",
    "\n",
    "There is also a binary target column, Target, indicating pneumonia or non-pneumonia.\n",
    "\n",
    "There may be multiple rows per patientId.\n",
    "\n",
    "DICOM Images\n",
    "All provided images are in DICOM format.\n",
    "\n",
    "What am I predicting?\n",
    "In this challenge competitors are predicting whether pneumonia exists in a given image. They do so by predicting bounding boxes around areas of the lung. Samples without bounding boxes are negative and contain no definitive evidence of pneumonia. Samples with bounding boxes indicate evidence of pneumonia.\n",
    "\n",
    "When making predictions, competitors should predict as many bounding boxes as they feel are necessary, in the format: confidence x-min y-min width height\n",
    "\n",
    "There should be only ONE predicted row per image. This row may include multiple bounding boxes.\n",
    "\n",
    "A properly formatted row may look like any of the following.\n",
    "\n",
    "For patientIds with no predicted pneumonia / bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,\n",
    "\n",
    "For patientIds with a single predicted bounding box: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\n",
    "\n",
    "For patientIds with multiple predicted bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 0.5 0 0 100 100, etc.\n",
    "\n",
    "File descriptions\n",
    "stage_2_train.csv - the training set. Contains patientIds and bounding box / target information.\n",
    "stage_2_sample_submission.csv - a sample submission file in the correct format. Contains patientIds for the test set. Note that the sample submission contains one box per image, but there is no limit to the number of bounding boxes that can be assigned to a given image.\n",
    "stage_2_detailed_class_info.csv - provides detailed information about the type of positive or negative class for each image.\n",
    "Data fields\n",
    "patientId _- A patientId. Each patientId corresponds to a unique image.\n",
    "x_ - the upper-left x coordinate of the bounding box.\n",
    "y_ - the upper-left y coordinate of the bounding box.\n",
    "width_ - the width of the bounding box.\n",
    "height_ - the height of the bounding box.\n",
    "Target_ - the binary Target, indicating whether this sample has evidence of pneumonia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
