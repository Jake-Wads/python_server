{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "**Description:** \n",
    "Based on Bayes’ theorem with the assumption of independence between every pair of features. Naive Bayes classifiers work well in many real-world situations such as document classification and spam filtering.  It is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem.  When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities. (so normalize your data!)\n",
    "It  assumes that each input variable is independent, thus ‘naive’.  This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.\n",
    "\n",
    "**Output Type:** Multi-class \n",
    "\n",
    "**Pros:**\n",
    "- Requires a smaller amount of training data than other algorithms to estimate the necessary parameters.  \n",
    "- Easily calculated.  \n",
    "- Operate well under strongly independent conditions.  \n",
    "- Extremely fast compared to more sophisticated methods.  \n",
    "- Simple\n",
    "- Powerful\n",
    "\n",
    "**Cons:**\n",
    "- Can be a bad estimator if used in less than ideal problems.\n",
    "\n",
    "**Example Use Case:**\n",
    "- Detecting Spam\n",
    "- Detecting Fraud\n",
    "__________________________________________________\n",
    "\n",
    "from *Thoughtful Machine Learning* by Matthew Kirk, O'Reilly Media, Inc., 2017\n",
    "\n",
    "**Using Bayes’ Theorem to Find Fraudulent Orders**\n",
    "\n",
    "\"Imagine you’re running an online store and lately you’ve been overrun with fraudulent orders. You estimate that about 10% of all orders coming in are fraudulent. In other words, in 10% of orders, people are stealing from you. Now of course you want to mitigate this by reducing the fraudulent orders, but you are facing a conundrum.\n",
    "\n",
    "Every month you receive at least 1,000 orders, and if you were to check every single one, you’d spend more money fighting fraud than the fraud was costing you in the first place. Assuming that it takes up to 60 seconds per order to determine whether it’s fraudulent or not, and a customer service representative costs around $15 per hour to hire, that totals 200 hours and $3,000 per year.\n",
    "\n",
    "Another way of approaching this problem would be to construct a probability that an order is over 50% fraudulent. In this case, we’d expect the number of orders we’d have to look at to be much lower. But this is where things become difficult, because the only thing we can determine is the probability that it’s fraudulent, which is 10%. Given that piece of information, we’d be back at square one looking at all orders because it’s more probable that an order is not fraudulent!\n",
    "\n",
    "Let’s say that we notice that fraudulent orders often use gift cards and multiple promotional codes. Using this knowledge, how would we determine what is fraudulent or not—namely, how would we calculate the probability of fraud given that the purchaser used a gift card?\"\n",
    "\n",
    "this is a conditional probability:  \n",
    "$P(A|B) = \\frac{P(A \\bigcap B)}{P(B)}$  \n",
    "$P( fraud | giftcard ) = \\frac{P( fraud \\bigcap giftcard )}{P( giftcard )}$\n",
    "\n",
    "This is saying that the probability of a transaction being fraudulent given it's a gift card is the probability of fraud and gift card occurring in the same transaction divided by the probability of a transaction using a gift card.\n",
    "\n",
    "$A \\bigcap B$ \n",
    "- Intersection of A and B\n",
    "- What exists in both A **AND** B\n",
    "- if A = [1,2,3] and B = [1,4,5] then set(A) & set(B) => 1, or $A \\bigcap B$ == 1\n",
    "\n",
    "$P(A)$\n",
    "- Probability of A\n",
    "\n",
    "$A \\bigcup B$\n",
    "- Union of A and B\n",
    "- What exists in either A **OR** B\n",
    "- if A = [1,2,3] and B = [1,4,5] then set(A) | set(B) => [1,2,3,4,5], or $A \\bigcup B$ == [1,2,3,4,5]\n",
    "\n",
    "Probability of A given B: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "a = set([1,2,3])\n",
    "b = set([1,4,5])\n",
    "\n",
    "total = 5.0\n",
    "\n",
    "p_a_and_b = len(a & b) / total\n",
    "p_b = len(b) / total\n",
    "\n",
    "p_a_given_b = p_a_and_b / p_b\n",
    "\n",
    "print(p_a_given_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is possible if you know the actual probability of fruad and giftcard, or $P(fraud \\bigcap giftcard)$  \n",
    "In practice, we rarely know this probability, and that is where Bayes' Theorem comes into play.  \n",
    "\n",
    "**Bayes’ Theorem, aka Inverse Conditional Probability**\n",
    "\n",
    "\"In the 1700s, Reverend Thomas Bayes came up with the original research that would become Bayes’ theorem. Pierre-Simon Laplace extended Bayes’ research to produce the beautiful result we know today.\"\n",
    "\n",
    "Bayes’ theorem is as follows:\n",
    "$P(B|A) = \\frac{P(A|B)\\centerdot P(B)}{P(A)}$\n",
    "\n",
    "From what we have just learned about conditional probability:\n",
    "$P(A|B) = \\frac{P(A \\bigcap B)}{P(B)}$   \n",
    "We can expand Bayes theorem to the following:\n",
    "$P(B|A) = \\frac{\\frac{P(A \\bigcap B)\\centerdot P(B)}{P(B)}}{P(A)} = \\frac{P(A \\bigcap B)}{P(A)}$\n",
    "\n",
    "$P(Fraud | Giftcard) = \\frac{P(Giftcard | Fraud) \\centerdot P(Fraud)}{P(Giftcard)}$\n",
    "\n",
    "$P(Fraud | Giftcard) = \\frac{60\\% \\centerdot 10\\%}{10\\%} = 60\\%$\n",
    "\n",
    "**Why Naive?**   \n",
    "\n",
    "TL;DR:  $P(A \\bigcap B) = P(B | A) \\centerdot P(A)$\n",
    "This is called a joint probability, i.e. the probability that all events will happen, and we use the **chain rule** to calculate that probability.  \n",
    "The general case of the rule is   \n",
    "$P(A_{1}, A_{2},..., A_{3}) = P(A_{1}) \\centerdot P(A_{2} | A_{1}) \\centerdot P(A_{3} | A_{1}, A_{2}) \\centerdot P(A_{n} | A_{1}, A_{2}, ... , A_{n-1})$ . \n",
    "Calculation of the joint probabiility assumes all events are not mutually exclusive.  \n",
    "You can see how this can grow exponentially, as well as the difficulty in knowing all the probabilities of every possible interaction.  \n",
    "For example, if we were to introduce multiple promos into our fraud example, then we’d have to know the interactions of not just fraud with promos and fraud with giftcards, but also promos with giftcards.   \n",
    "The assumption we then make (thus *naive*), is that there is the promos and giftcards are independent of each other, i.e. there is no interaction.  In doing that, we only have to consider the interactions that include fraud.  \n",
    "With that assumption, we end up with a formula that is much simpler:  \n",
    "\n",
    "$P(Fraud | Giftcard, Promo) = P(Giftcard | Fraud) \\centerdot P(Promo | Fraud)$  \n",
    "\n",
    "This would be proportional to our numerator. And, to simplify things even more, we can assert that we’ll normalize later with some magical Z, which is the sum of all the probabilities of classes. So now our model becomes:\n",
    "\n",
    "$P(Fraud | Giftcard, Promo) = \\frac{1}{Z} \\centerdot P(Giftcard | Fraud) \\centerdot P(Promo | Fraud)$  \n",
    "\n",
    "\n",
    "To turn this into a classification problem, we simply determine which input—fraud or not fraud—yields the highest probability. \n",
    "\n",
    "**Gotchas: Zeroing-out effect**\n",
    "\n",
    "\n",
    "Pseudocount:  if history shows a probability of 0 for an interaction, like we have never seen fraud with a certain promo, then that 0 will cause the final probability to be 0, known as the 'zeroing-out effect'.  The fact we have never seen it in that promo could be due to that promo being new. To avoid this error of dismissing all other interactions that could come into play, like that transaction was also using a giftcard, we add 1 to the count of the word. So, everything will end up being transaction count + 1. This helps mitigate the zeroing-out effect for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.063436</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.384267</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.051237</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447097</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.359135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.220910</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Embarked      Fare  Parch  Pclass  Sex  SibSp  Title\n",
       "0  0.421965  0.666667  0.063436    0.2     0.5  0.0  0.125   0.75\n",
       "1  0.384267  0.666667  0.051237    0.2     0.5  0.0  0.125   0.75\n",
       "2  0.447097  0.666667  0.051310    0.0     0.0  1.0  0.000   0.50\n",
       "3  0.359135  0.000000  0.015412    0.0     1.0  1.0  0.000   0.50\n",
       "4  0.220910  0.666667  0.022447    0.0     0.5  1.0  0.000   0.50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r X_train_scaled\n",
    "%store -r X_test_scaled\n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "#### Create the Gaussian Naive Bayes object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate whether or not a passenger would survive, using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.80\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the probability of a passenger surviving, using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = gnb.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "#### Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  71]\n",
      " [ 51 193]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Precision, Recall, F1-score, and Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.81      0.83       379\n",
      "        1.0       0.73      0.79      0.76       244\n",
      "\n",
      "avg / total       0.81      0.80      0.81       623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "#### Compute the accuracy of the model when run on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on test set: 0.79\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'gnb' (GaussianNB)\n"
     ]
    }
   ],
   "source": [
    "%store gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Read in your train and test dataframes\n",
    "2. Walk through the steps of training the logistic regression classifier\n",
    "3. Evaluate your results using the model score, confusion matrix, and classification report.\n",
    "4. Print and clearly label the following:  Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support. \n",
    "5. Look in the scikit-learn documentation to research the *x parameter*.  What is your best option(s) for the particular problem you are trying to solve and the data to be used? \n",
    "6. Run through steps 2-4 using another *x parameter* (from question 5) \n",
    "7. Which appears to perform better?\n",
    "8. Test the best model on your testing data. \n",
    "9. Store your final model into logit for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
