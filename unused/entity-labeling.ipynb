{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "## Top Down NLP\n",
    "\n",
    "\n",
    "## Use cases:\n",
    "- Autocomplete\n",
    "- Privacy:  remove names, e.g.\n",
    "- Sentiment\n",
    "- Chat bots\n",
    "- tagging\n",
    "\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "1. Sentence Segmentation\n",
    "2. Word Tokenizing\n",
    "3. Preict Parts of Speech\n",
    "4. Text Lemmatization\n",
    "5. Handling Stopwords\n",
    "6. Dependency Parsing\n",
    "7. Finding Noun Phrases\n",
    "8. Named Entity Recognition (NER)\n",
    "9. Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n",
    "\n",
    "To run an NLP pipeline on a piece of text\n",
    "You’ll get a list of named entities and entity types detected in our document\n",
    "For Entity types:\n",
    "https://spacy.io/usage/linguistic-features#entity-types\n",
    "\n",
    "https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London (GPE)\n",
      "England (GPE)\n",
      "the United Kingdom (GPE)\n",
      "Great Britain (GPE)\n",
      "London (GPE)\n",
      "two millennia (DATE)\n",
      "Romans (NORP)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv('data/news.csv')\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)\n",
    "\n",
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes all the names it detects, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-28bf12893256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-28bf12893256>\u001b[0m in \u001b[0;36mscrub\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mretokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_name_with_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m s = \"\"\"\n",
      "\u001b[0;32m<ipython-input-5-28bf12893256>\u001b[0m in \u001b[0;36mreplace_name_with_placeholder\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"[REDACTED] \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loop through all the entities in a document and check if they are names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "# Replace a token with \"REDACTED\" if it is a name\n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    else:\n",
    "        return token.string\n",
    "\n",
    "# Loop through all the entities in a document and check if they are names\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\"\n",
    "\n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Facts\n",
    "you can use the parsed output from spaCy as the input to more complex data extraction algorithms. There’s a python library called textacy that implements several common data extraction algorithms on top of spaCy.\n",
    "\n",
    "One of the algorithms it implements is called Semi-structured Statement Extraction. We can use it to search the parse tree for simple statements where the subject is “London” and the verb is a form of “be”. That should help us find facts about London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# import textacy.extract\n",
    "\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the document with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract semi-structured statements\n",
    "# statements = textacy.extract.semistructured_statements(doc, \"London\")\n",
    "\n",
    "# Print the results\n",
    "# print(\"Here are the things I know about London:\")\n",
    "\n",
    "# for statement in statements:\n",
    "#     subject, verb, fact = statement\n",
    "#     print(f\" - {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try installing the neuralcoref library and adding Coreference Resolution to your pipeline. That will get you a few more facts since it will catch sentences that talk about “it” instead of mentioning “London” directly.\n",
    "\n",
    "Autocomplete example:\n",
    "We need a list of possible completions to suggest to the user. We can use NLP to quickly generate this data.\n",
    "Here’s one way to extract frequently-mentioned noun chunks from a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract noun chunks that appear\n",
    "# noun_chunks = textacy.extract.noun_chunks(doc, min_freq=3)\n",
    "\n",
    "# Convert noun chunks to lowercase strings\n",
    "# noun_chunks = map(str, noun_chunks)\n",
    "# noun_chunks = map(str.lower, noun_chunks)\n",
    "\n",
    "# # Print out any nouns that are at least 2 words long\n",
    "# for noun_chunk in set(noun_chunks):\n",
    "#     if len(noun_chunk.split(\" \")) > 1:\n",
    "#         print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*other source...need to edit, customize and merge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition  \n",
    "\n",
    "In any text document, there are particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities , which more specifically refer to terms that represent real-world objects like people, places, organizations, and so on, which are often denoted by proper names. A naive approach could be to find these by looking at the noun phrases in text documents. Named entity recognition (NER) , also known as entity chunking/extraction , is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.\n",
    "\n",
    "SpaCy has some excellent capabilities for named entity recognition. Let’s try and use it on one of our sample news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sentence = str(news_df.iloc[1].full_text)\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "# print named entities in article\n",
    "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])\n",
    "\n",
    "# visualize named entities\n",
    "displacy.render(sentence_nlp, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the major named entities have been identified by spacy. To understand more in detail about what each named entity means, you can refer to the documentation or check out the following table for convenience.\n",
    "\n",
    "Let’s now find out the most frequent named entities in our news corpus! For this, we will build out a data frame of all the named entities and their types using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []\n",
    "for sentence in news_df.full_text:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        term = word.text \n",
    "        tag = word.ent_type_\n",
    "        if tag:\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None\n",
    "\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform and aggregate this data frame to find the top occuring entities and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top named entities\n",
    "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything interesting? (Hint: Maybe the supposed summit between Trump and Kim Jong!). We also see that it has correctly identified ‘Messenger’ as a product (from Facebook).\n",
    "\n",
    "We can also group by the entity types to get a sense of what types of entites occur most in our news corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top named entity types\n",
    "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that people, places and organizations are the most mentioned entities though interestingly we also have many other entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
