{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies in Time Series Data\n",
    "\n",
    "**Lesson Goals**\n",
    "\n",
    "- Use entropy as a quick way to identify fields that may have anomalies. \n",
    "\n",
    "- Use statistical properties to flag the data points that deviate from the expected. \n",
    "\n",
    "**The Data**\n",
    "\n",
    "- Logs of API requests to our data containing sales information about our stores and items. \n",
    "\n",
    "- `https://python.zach.lol/access.log`\n",
    "\n",
    "- Type of target variable: **Continuous** or Discrete\n",
    "\n",
    "- Type of observations: **Time Series** or Point in Time   \n",
    "\n",
    "\n",
    "**The Questions**\n",
    "\n",
    "- Are there unusual IP addresses accessing our data via the API? \n",
    "\n",
    "- Have we seen any spikes or unusual patterns in the size of requests? \n",
    "\n",
    "- In general: Does this new value deviate from what we would expect based on historical data? If so, is it something to be concerned about? Remember, we aren't detecting anomalies for the sake of detecting anomalies. \n",
    "\n",
    "\n",
    "_____________________________\n",
    "\n",
    "\n",
    "## Wrangle Data\n",
    "\n",
    "**Prepare Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates #to format dates on our plots\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# This is to make sure matplotlib doesn't throw the following error:\n",
    "# The next line fixes \"TypeError: float() argument must be a string or a number, not 'Timestamp' matplotlib\"\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Acquire**\n",
    "\n",
    "After doing some research, some experimentation of performing actions and watching the logs, we discovered what each of the fields represent. We then parse and name the fields accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['ip', 'timestamp', 'request_method', 'status', 'size',\n",
    "          'destination', 'request_agent']\n",
    "df = pd.read_csv('https://python.zach.lol/access.log',          \n",
    "                 engine='python',\n",
    "                 header=None,\n",
    "                 index_col=False,\n",
    "                 names=colnames,\n",
    "                 sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "                 na_values='\"-\"',\n",
    "                 usecols=[0, 3, 4, 5, 6, 7, 8]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this research, we are only interested in the IP address, timestamp and size of the requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['ip', 'timestamp', 'size']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore IP Address**\n",
    "\n",
    "In this sample data, it's pretty easy to take a look at value counts to see those IP's that are rare. However, usually the data is much, much larger and looking at simple value counts is not going to be enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of the frequency of ip addresses. Are the events made up of thousands of IP addresses that occur only a handful of time?  Are the events made up of only a few IP addresses that occur most of the time, but then a few that are seen only a handful of times? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe out of value counts\n",
    "ip_counts = pd.DataFrame(df.ip.value_counts()).reset_index()\n",
    "\n",
    "# rename columns\n",
    "ip_counts.columns=['ip', 'event_count']\n",
    "\n",
    "# get the number of ip addresses seen 1, or 2, or 11,998 times. \n",
    "ip_counts.groupby(['event_count']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the IP addresses that only occur once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter where event_count == 1\n",
    "ip_counts[ip_counts['event_count'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a glimpse into the questions above by computing entropy, using `scipy.stats.entropy`\n",
    "\n",
    "\"In probability theory, the entropy of a random variable measures the uncertainty about the value that might be assumed by the variable.\"\n",
    "http://www.scholarpedia.org/article/Entropy\n",
    "\n",
    "An entropy of 0 indicates there is no uncertainty. In other words, the value will always be one value. An entropy of 1 indicates every value is different. When using entropy in anomaly detection, fields with a low, but not 0 entropy is likely to contain an anomalous value. This is clearly not going to catch all anomalies, but it is another tool to put in your toolbox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# pass the entropy function an array-like object of counts. \n",
    "entropy(ip_counts.event_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entropy of .56 does not tell there is a clear anomaly, but it also does not tell me there is NOT one. \n",
    "This is useful when you have a bunch of variables and don't know where to start or want to be sure you arent missing something obvious. You could loop through your variables computing entropy for each one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "\n",
    "\n",
    "**Prepare Data to Explore Size**\n",
    "\n",
    "First, we will resample the existing data to daily increments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to put the timestamp in a more standard format. \n",
    "We will use regular expressions to remove the brackets. \n",
    "Then we will replace the first colon ':' with a space. \n",
    "At that point, the pandas function to_datetime should be able to read the timestamp and convert it correctly to datetime format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove brackets and timezone (+0000) by replacing them with empty string ''\n",
    "df.timestamp = df.timestamp.str.replace(r'(\\[|\\]|\\+0{4})', '', regex=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the first colon, the one that separates date from time,\n",
    "# with a space\n",
    "df.timestamp = pd.to_datetime(df.timestamp.str.replace(':', ' ', 1))\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index to timestamp\n",
    "df = df.set_index('timestamp')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample by day, summing the size\n",
    "df = df[['size']].resample('1d').sum()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with 0\n",
    "df = df.fillna(value=0)\n",
    "\n",
    "# summary stats of the size\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside: Simulate some new data to manufacture some anomalies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dataframe starting 4/18/2018 through the next year, setting values for size to 0. \n",
    "\n",
    "2. Use the mean and standard deviation of our original data in df to set boundaries for generating random values for our new dataset that fit into the original distribution. \n",
    "\n",
    "3. Randomly select a few datapoints to replace the values with anomalous values, and fill with random values that are above expected values. \n",
    "\n",
    "4. combine the newly generated dataset with the original. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*create the dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame({'timestamp': pd.date_range('20190418', periods=365),\n",
    "                    'size': 0}).set_index('timestamp')\n",
    "\n",
    "new.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*generate new values between* `[0, mean+2*standard deviation]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and standard deviation for randomly generating some data. \n",
    "mean = df['size'].mean()\n",
    "std = df['size'].std()\n",
    "\n",
    "# for all values of 0, fill with random value between 0 and mean + 2*stdev\n",
    "new['size'] = new['size'].apply(lambda x: np.random.randint(0, mean+2*std) if x==0 else x)\n",
    "\n",
    "new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill with some anomalies by replacing 5 random datapoints with a random number between* `[(mean+2*std), (mean+5*std)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of 5 datapoints\n",
    "sample = new.sample(n = 5) \n",
    "\n",
    "# for each sample, replace the size with a random number between our range. \n",
    "for i in sample.index.values:\n",
    "    new.loc[i] = np.random.randint(mean+3*std, mean+5*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Concatenate our new data with our original data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, new])\n",
    "\n",
    "print(df.head(2))\n",
    "print(df.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's represent size in GB for ease of conceptual understanding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_gb'] = [n/1024/1024/1024 for n in df['size']]\n",
    "df = df[['size_gb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into Train/Test**\n",
    "\n",
    "- train: 2019-04-16 => 2019-10-15\n",
    "\n",
    "- validate: 2019-10-16 => 2020-01-15\n",
    "\n",
    "- test: 2020-01-16 => 2020-04-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:'2019-10-15']\n",
    "validate = df['2019-10-16':'2020-01-15']\n",
    "test = df['2020-01-16':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train)\n",
    "plt.plot(validate)\n",
    "plt.plot(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Moving Average\n",
    "\n",
    "First, let's compute the simple moving average in order to compare to the exponential moving average. We will compute a few different windows, 7 days, 14 days, and 30 days. \n",
    "\n",
    "Notice that with the simple moving average, using `df.rolling(window=7).mean()`, when there are fewer than 7 values of history to compute, like on 2019-04-16 (the first day of data that we have), the result will be NaN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 week\n",
    "sma_short = train.rolling(window=7).mean()\n",
    "sma_short[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 day\n",
    "sma_mid = train.rolling(window=14).mean()\n",
    "sma_mid[11:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 day\n",
    "sma_long = train.rolling(window=30).mean()\n",
    "sma_long[27:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the SMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.plot(train.index, train, label='Size (MB)', alpha=.5)\n",
    "\n",
    "ax.plot(train.index, sma_short, label = '7-day SMA')\n",
    "\n",
    "ax.plot(train.index, sma_mid, label = '14-day SMA')\n",
    "\n",
    "ax.plot(train.index, sma_long, label = '30-day SMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "# ax.xaxis.(rotate=90)\n",
    "# ax.xaxis.set_major_formatter(my_datetime_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and try some other windows to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Moving Average\n",
    "\n",
    "SMA time series are much less noisy than the time series of the original data points. \n",
    "The challenge with SMA, however, is that the values of SMA lag the original values. This means that changes in the trend are only seen with a delay (lag) of L time units. \n",
    "\n",
    "Exponential Moving Average (EMA) helps reduce the lag induced by the use of the SMA. It does this by putting more weight on more recent observations, while the SMA weights all observations equally.\n",
    "\n",
    "The EMA function looks like this: \n",
    "\n",
    "$EMA_{t}= \\alpha * (t_{0} - EMA_{t-1}) + EMA_{t-1}$\n",
    "\n",
    "Where: \n",
    "\n",
    "- M = Number of time periods, span of the window\n",
    "\n",
    "- $t_{0}$ = Latest value\n",
    "\n",
    "- $t-1$ = Previous value\n",
    "\n",
    "- $EMA_{t-1}$ = Exponential moving average of previous day. \n",
    "\n",
    "- The multiplier: $\\alpha = \\frac{2}{M+1}$\n",
    "\n",
    "However, we will use the pandas ewm (Exponential Weighted functions) to compute our EMA. \n",
    "So we just need to define the following: \n",
    "\n",
    "- M = `span` argument = number of time periods. We will try 7 days, 14 days, and 30 days. \n",
    "\n",
    "- Notice how there are no missing values. ewm() will use as many values are available to compute the mean. So if the span is 7 days, but it is on the first day of data available, the EMA will equal the first value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 days EMA\n",
    "ema_short = train.ewm(span=7).mean()\n",
    "ema_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 days EMA\n",
    "ema_mid = train.ewm(span=14).mean()\n",
    "ema_mid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 days EMA\n",
    "ema_long = train.ewm(span=30).mean()\n",
    "ema_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of SMA and EMA**\n",
    "\n",
    "The exponential moving average puts more weight on the more recent values, while maintaining information about values before the window span through inclusion of the previous period's EMA in the formula. When comparing the plots below, take a look at how the spikes impact the SMA vs. the EMA. \n",
    "\n",
    "7-Day SMA vs. EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(train.index, train, label='Size (MB)', alpha=.5)\n",
    "\n",
    "ax.plot(train.index, sma_short, label = '7-day SMA')\n",
    "ax.plot(train.index, ema_short, label = '7-day EMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14-Day SMA vs. EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(train.index, train, label='Size (MB)', alpha=.5)\n",
    "\n",
    "ax.plot(train.index, sma_mid, label = '14-day SMA')\n",
    "ax.plot(train.index, ema_mid, label = '14-day EMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30-Day SMA vs. EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(train.index, train, label='Size (MB)', alpha=.5)\n",
    "\n",
    "ax.plot(train.index, sma_long, label = '30-day SMA')\n",
    "ax.plot(train.index, ema_long, label = '30-day EMA')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (MB)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ax.xaxis.(rotate=90)\n",
    "# ax.xaxis.set_major_formatter(my_datetime_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I know the spikes are completely unexpected and do not want those to impact the expected values, then the longer EMA is the way to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bollinger Bands and %b\n",
    "\n",
    "**Bollinger Bands**\n",
    "\n",
    "- a volatility indicator and commonly used in stock market trading. \n",
    "\n",
    "- Made up of 3 lines, the Upper Band (UB), the Lower Band (LB) and the Midband.  \n",
    "\n",
    "**Midband**\n",
    "\n",
    "- The Exponential Moving Average\n",
    "\n",
    "- `midband = train.ewm(span=30).mean()`\n",
    "\n",
    "**Upper & Lower Band**\n",
    "\n",
    "- UB/LB = Midband +/- stdev * K\n",
    "\n",
    "- `stdev = train.ewm(span=30).std()` \n",
    "\n",
    "- K = the number of standard deviations to go up and down from the EMA\n",
    "\n",
    "**%b, Percent Bandwidth**\n",
    "\n",
    "- Shows where the last value sits in relation to the bands\n",
    "\n",
    "- $\\%b = \\frac{last-LB}{UB-LB}$ \n",
    "\n",
    "- %b > 1 => point lies above UB\n",
    "\n",
    "- %b < 0 => point lies below LB\n",
    "\n",
    "- %b == .5 => point lies on the midband. \n",
    "\n",
    "**Bandwidth** \n",
    "\n",
    "- The width of the bands\n",
    "\n",
    "- $Bandwidth = \\frac{(UB-LB)}{Midband}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the window span\n",
    "span = 30\n",
    "\n",
    "# compute midband\n",
    "midband = train.ewm(span=span).mean()\n",
    "\n",
    "midband.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute exponential stdev\n",
    "stdev = train.ewm(span=span).std()\n",
    "\n",
    "stdev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute upper and lower bands\n",
    "ub = midband + stdev*2\n",
    "lb = midband - stdev*2\n",
    "\n",
    "# concatenate ub and lb together into one df, bb\n",
    "bb = pd.concat([ub, lb], axis=1)\n",
    "\n",
    "bb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, midband, bb], axis=1)\n",
    "train.columns = ['size_gb', 'midband', 'ub', 'lb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.plot(train.index, train.size_gb, label='Size (GB)')\n",
    "\n",
    "ax.plot(train.index, train.midband, label = '30-day EMA/midband')\n",
    "ax.plot(train.index, train.ub, label = 'Upper Band')\n",
    "ax.plot(train.index, train.lb, label = 'Lower Band')\n",
    "\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylabel('Size (GB)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do you think we will have a %b > 1? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute %b\n",
    "\n",
    "$\\%b = \\frac{last-LB}{UB-LB}$ \n",
    "\n",
    "Each row/time period, will have a %b value that answers the question, where does this point sit with respect to the expected value when considering the values before it. For example, we could take the last value in train and manually compute %b: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last measure of gb\n",
    "last_measure = train.iloc[-1].size_gb\n",
    "\n",
    "\n",
    "# get the lower band value on the last date\n",
    "last_lb = train.iloc[-1].lb\n",
    "\n",
    "# get the upper band value on the last date\n",
    "last_ub = train.iloc[-1].ub\n",
    "\n",
    "# compute %b\n",
    "last_pct_b = (last_measure - last_lb)/(last_ub - last_lb)\n",
    "\n",
    "print('%b for last datapoint: ', round(last_pct_b, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to compute for each point in time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pct_b'] = (train['size_gb'] - train['lb'])/(train['ub'] - train['lb'])\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, where do we see anomalies? We will search for %b values > 1. We don't need to search for values < 0 because with this example, a low extreme is not something we are concerned about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['pct_b']>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**file name:** time_series_anomaly_detection.py or time_series_anomaly_detection.ipynb\n",
    "\n",
    "The dataset for these exercises lives in the Codeup Data Science MySQL Server. The database name is `curriculum_logs`.\n",
    "\n",
    "Go through the lesson commenting code, adding docstrings, and adding markdown to support what is happening. \n",
    "\n",
    "\n",
    "Bonus:\n",
    "\n",
    "Discover users who are accessing our curriculum pages way beyond the end of their codeup time. What would the dataframe look like? Use time series method for detecting anomalies, like exponential moving average with %b.\n",
    "\n",
    "Can you label students who are viewing both the web dev and data science curriculum?\n",
    "Can you label students by the program they are in? \n",
    "Can you label users by student vs. staff?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
