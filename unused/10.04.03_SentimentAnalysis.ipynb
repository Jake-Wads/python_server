{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP:  Sentiment Analysis  \n",
    "\n",
    "```bash\n",
    "python -m pip install afinn\n",
    "```\n",
    "\n",
    "Sentiment analysis is one of the most popular applications of NLP.  It is about identifying the underlying sentiment or emotion tied to and expressed through a body of text.  Typically, we quantify this sentiment with a positive or negative value, called polarity. The overall sentiment is often inferred as positive, neutral or negative from the sign of the polarity score.\n",
    "\n",
    "Typically, sentiment analysis for text data can be computed on several levels, including on an individual sentence level, paragraph level, or the entire document as a whole. Often, sentiment is computed on the document as a whole or some aggregations are done after computing the sentiment for individual sentences. There are two major approaches to sentiment analysis.\n",
    "\n",
    "- Supervised machine learning or deep learning approaches\n",
    "- Unsupervised lexicon-based approaches\n",
    "\n",
    "For the first approach we typically need pre-labeled data. \n",
    "For the second we are using existing knowledge of words and their general meaning, with respect to sentiment. \n",
    "Lexicons are special dictionaries or vocabularies that have been created for analyzing sentiments. Most of these lexicons have a list of positive and negative polar words with some score associated with them, and using various techniques like the position of words, surrounding words, context, parts of speech, phrases, and so on, scores are assigned to the text documents for which we want to compute the sentiment. After aggregating these scores, we get the final sentiment.\n",
    "\n",
    "Various popular lexicons are used for sentiment analysis, including the following.\n",
    "\n",
    "- AFINN lexicon\n",
    "- Bing Liu’s lexicon\n",
    "- MPQA subjectivity lexicon\n",
    "- SentiWordNet\n",
    "- VADER lexicon\n",
    "- TextBlob lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Based Sentiment Analysis\n",
    "1. Data Acquisition: Acquire & Label data.  \n",
    "    - You need to gather a training dataset with examples of expressions with a positive, negative and neutral sentiment, label the sentiment of the statement.  There are many smart and reliable algorithms out there to tag statements with complex or simple sentiments. Some sentiment algorithm would also give you sentiments like anxiety, sadness etc while the most commonly used are positive, negative and neutral. Natural language processing is used to tag each word with a sentiment and the overall score or sentiment that the statement would get depends on the underlying word sentiments.\n",
    "\n",
    "2. Data prep: text cleaning & feature vectors:   \n",
    "    - Text Cleaning:  Stopwords removal, punctuation removal, stemming etc were few of the techniques used to clean the text. This is common to every text analysis problem. \n",
    "    - Feature Vectors:  Transform the text into feature vectors of words. A simple but effective method for doing this is bag-of-words, where each dimension represents the frequency of a given word in the document.\n",
    "\n",
    "3. Train: Use those vectors to train a machine learning algorithm (such as Logistic Regression, Naive Bayes, SVM, Decision Tree, Neural Networks) to make sentiment predictions in new unseen text by transforming them into vectors and feeding them to the classifier.  \n",
    "    - One of the simplest algorithms is logistic regression. In the simplest form, each feature will be associated with a weight. Let's say the word \"love\" has a weight equal to +4, \"hate\" is -10, \"the\" is 0 ... For a given example, the weights corresponding to the features will be summed, and it will be considered \"positive\" if the total is > 0, \"negative\" otherwise. Our model will then try to find the optimal set of weights to maximize the number of examples in our data that are predicted correctly.  If you have more than 2 output classes, for example if you want to classify between \"positive\", \"neutral\" and \"negative\", each feature will have as many weights as there are classes, and the class with the highest weighted feature sum wins.\n",
    "    - This model doesn't take into account context around words, which will lead to misclassification.  One way to overcome this problem is to generate more features, like the frequency of n-grams or the syntactic dependency between words. An excellent model by Socher et al. takes a completely different approach. It uses the syntactic structure of the document to build up a vector representation by combining recursively the vector representations of words.\n",
    "\n",
    "4. Test the model: After we have trained the parameters to fit the training data, we have to make sure our model generalizes to new data, because it's really easy to overfit. The general way of regularizing the model is to prevent parameters from having extreme values.\n",
    "\n",
    "Rule Based Sentiment Analysis\n",
    "\n",
    "For creating rule-based systems, you first need to define a set of polarized words and expressions that represent a particular sentiment. For example, you can specify a list of negative words such as ‘bad’, ’terrible’, and ’worst’, and a list of positive words such as ’good’, ’beautiful’, and ’nice’. Then, for a given text, you count how many positive and negative words appear in its content. Finally, the rule-based systems determine a text as positive if the appearances of positives words are greater than the number of negative words, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *for reference below...need to merge this with others, edit, delete, and change source data etc. to complete this section and rest of nlp*\n",
    "\n",
    "https://medium.com/@himanshu_23732/sentiment-analysis-with-afinn-lexicon-930533dfe75b\n",
    "\n",
    "#### Sentiment Analysis with AFINN Lexicon\n",
    "The AFINN lexicon is one of the simplest and most popular lexicons used extensively for sentiment analysis. Developed and curated by Finn Årup Nielsen, you can find more details on this lexicon in the paper, “A new ANEW: evaluation of a word list for sentiment analysis in microblogs”, proceedings of the ESWC 2011 Workshop. The current version of the lexicon is AFINN-en-165. txt and it contains over 3,300+ words with a polarity score associated with each word. You can find this lexicon at the author’s official GitHub repository along with previous versions of it, including AFINN-111. The author has also created a nice wrapper library on top of this in Python called afinn, which we will be using for our analysis.\n",
    "\n",
    "The following code computes sentiment for all our news articles and shows summary statistics of general sentiment per news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RBI to issue new ₹20 notes in greenish yellow ...\n",
       "1    Dhoni moves SC against Amrapali Group over own...\n",
       "2    Elon Musk reaches settlement with US regulator...\n",
       "3    137 Air India flights delayed today over Satur...\n",
       "4    Naspers to sell its entire MakeMyTrip stake to...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "\n",
    "af = Afinn()\n",
    "\n",
    "%store -r corp\n",
    "corpus = corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from acquire import get_article_text\n",
    "\n",
    "article = get_article_text()\n",
    "\n",
    "af.score(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4e1337543079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# sentiment statistics per news category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'news_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'news_category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores = [af.score(article) for article in corpus]\n",
    "sentiment_category = ['positive' if score > 0 \n",
    "                          else 'negative' if score < 0 \n",
    "                              else 'neutral' \n",
    "                                  for score in sentiment_scores]\n",
    "    \n",
    "    \n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores, sentiment_category]).T\n",
    "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['news_category']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a good idea of general sentiment statistics across different news categories. Looks like the average sentiment is very positive in sports and reasonably negative in technology! Let’s look at some visualizations now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "sp = sns.stripplot(x='news_category', y=\"sentiment_score\", \n",
    "                   hue='news_category', data=df, ax=ax1)\n",
    "bp = sns.boxplot(x='news_category', y=\"sentiment_score\", \n",
    "                 hue='news_category', data=df, palette=\"Set2\", ax=ax2)\n",
    "t = f.suptitle('Visualizing News Sentiment', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the spread of sentiment polarity is much higher in sports and world as compared to technology where a lot of the articles seem to be having a negative polarity. We can also visualize the frequency of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
    "                    data=df, kind=\"count\", \n",
    "                    palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises here that technology has the most number of negative articles and world the most number of positive articles. Sports might have more neutral articles due to the presence of articles which are more objective in nature (talking about sporting events without the presence of any emotion or feelings). Let’s dive deeper into the most positive and negative sentiment news articles for technology news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = df[(df.news_category=='technology') & (df.sentiment_score == 6)].index[0]\n",
    "neg_idx = df[(df.news_category=='technology') & (df.sentiment_score == -15)].index[0]\n",
    "\n",
    "print('Most Negative Tech News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
    "print()\n",
    "print('Most Positive Tech News Article:', news_df.iloc[pos_idx][['news_article']][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the most negative article is all about a recent smartphone scam in India and the most positive article is about a contest to get married in a self-driving shuttle. Interesting! Let’s do a similar analysis for world news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 16)].index[0]\n",
    "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -12)].index[0]\n",
    "\n",
    "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
    "print()\n",
    "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly Trump features in both the most positive and the most negative world news articles. Do read the articles to get some more perspective into why the model selected one of them as the most negative and the other one as the most positive (no surprises here!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob\n",
    "\n",
    "TextBlob is another excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. Typically, the scores have a normalized scale as compare to Afinn. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. Let’s use this now to get the sentiment polarity and labels for each news article and aggregate the summary statistics per news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in news_df['clean_text']]\n",
    "sentiment_category_tb = ['positive' if score > 0 \n",
    "                             else 'negative' if score < 0 \n",
    "                                 else 'neutral' \n",
    "                                     for score in sentiment_scores_tb]\n",
    "\n",
    "\n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores_tb, sentiment_category_tb]).T\n",
    "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['news_category']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the average sentiment is the most positive in world and least positive in technology! However, these metrics might be indicating that the model is predicting more articles as positive. Let’s look at the sentiment frequency distribution per news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
    "                    data=df, kind=\"count\", \n",
    "                    palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There definitely seems to be more positive articles across the news categories here as compared to our previous model. However, still looks like technology has the most negative articles and world, the most positive articles similar to our previous analysis. Let’s now do a comparative analysis and see if we still get similar articles in the most positive and negative categories for world news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 0.7)].index[0]\n",
    "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -0.296)].index[0]\n",
    "\n",
    "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
    "print()\n",
    "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, looks like the most negative world news article here is even more depressing than what we saw the last time! The most positive article is still the same as what we had obtained in our last model.\n",
    "\n",
    "Finally, we can even evaluate and compare between these two models as to how many predictions are matching and how many are not (by leveraging a confusion matrix which is often used in classification). We leverage our nifty model_evaluation_utils module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_evaluation_utils as meu\n",
    "meu.display_confusion_matrix_pretty(true_labels=sentiment_category, \n",
    "                                    predicted_labels=sentiment_category_tb, \n",
    "                                    classes=['negative', 'neutral', 'positive'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding table, the ‘Actual’ labels are predictions from the Afinn sentiment analyzer and the ‘Predicted’ labels are predictions from TextBlob. Looks like our previous assumption was correct. TextBlob definitely predicts several neutral and negative articles as positive. Overall most of the sentiment predictions seem to match, which is good!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter\n",
    "\n",
    "https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "Tweepy: tweepy is the python client for the official Twitter API.\n",
    "Install it using following pip command:\n",
    "`pip install tweepy`\n",
    "\n",
    "TextBlob: textblob is the python library for processing textual data.\n",
    "Install it using following pip command:\n",
    "`pip install textblob`\n",
    "\n",
    "Also, we need to install some NLTK corpora using following command:\n",
    "\n",
    "`python -m textblob.download_corpora`\n",
    "(Corpora is nothing but a large and structured set of texts.)\n",
    "\n",
    "Authentication:\n",
    "In order to fetch tweets through Twitter API, one needs to register an App through their twitter account. Follow these steps for the same:\n",
    "\n",
    "Open this link and click the button: ‘Create New App’\n",
    "Fill the application details. You can leave the callback url field empty.\n",
    "Once the app is created, you will be redirected to the app page.\n",
    "Open the ‘Keys and Access Tokens’ tab.\n",
    "Copy ‘Consumer Key’, ‘Consumer Secret’, ‘Access token’ and ‘Access Token Secret’.\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \n",
    "                                    |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "  \n",
    "    def get_tweets(self, query, count = 10): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "  \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "  \n",
    "def main(): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'Donald Trump', count = 200) \n",
    "  \n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    # percentage of neutral tweets \n",
    "    print(\"Neutral tweets percentage: {} % \\ \n",
    "        \".format(100*len(tweets - ntweets - ptweets)/len(tweets))) \n",
    "  \n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how a sample output looks like when above program is run:\n",
    "\n",
    "Positive tweets percentage: 22 %\n",
    "Negative tweets percentage: 15 %\n",
    "\n",
    "Positive tweets:\n",
    "RT @JohnGGalt: Amazing—after years of attacking Donald Trump the media managed\n",
    "to turn #InaugurationDay into all about themselves.\n",
    "#MakeAme…\n",
    "RT @vooda1: CNN Declines to Air White House Press Conference Live YES! \n",
    "THANK YOU @CNN FOR NOT LEGITIMI…\n",
    "RT @Muheeb_Shawwa: Donald J. Trump's speech sounded eerily familiar...\n",
    "POTUS plans new deal for UK as Theresa May to be first foreign leader to meet new \n",
    "president since inauguration \n",
    ".@realdonaldtrump #Syria #Mexico #Russia & now #Afghanistan. \n",
    "Another #DearDonaldTrump Letter worth a read @AJEnglish \n",
    "\n",
    "Negative tweets:\n",
    "RT @Slate: Donald Trump’s administration: “Government by the worst men.” \n",
    "RT @RVAwonk: Trump, Sean Spicer, et al. lie for a reason. \n",
    "Their lies are not just lies. Their lies are authoritarian propaganda.  \n",
    "RT @KomptonMusic: Me: I hate corn \n",
    "Donald Trump: I hate corn too\n",
    "Me: https://t.co/GPgy8R8HB5\n",
    "It's ridiculous that people are more annoyed at this than Donald Trump's sexism.\n",
    "RT @tony_broach: Chris Wallace on Fox news right now talking crap \n",
    "about Donald Trump news conference it seems he can't face the truth eithe…\n",
    "RT @fravel: With False Claims, Donald Trump Attacks Media on Crowd Turnout \n",
    "Aziz Ansari Just Hit Donald Trump Hard In An Epic Saturday NIght Live Monologue\n",
    "We follow these 3 major steps in our program:\n",
    "\n",
    "Authorize twitter API client.\n",
    "Make a GET request to Twitter API to fetch tweets for a particular query.\n",
    "Parse the tweets. Classify each tweet as positive, negative or neutral.\n",
    "Now, let us try to understand the above piece of code:\n",
    "\n",
    "First of all, we create a TwitterClient class. This class contains all the methods to interact with Twitter API and parsing tweets. We use __init__ function to handle the authentication of API client.\n",
    "In get_tweets function, we use:\n",
    "fetched_tweets = self.api.search(q = query, count = count)\n",
    "to call the Twitter API to fetch tweets.\n",
    "\n",
    "In get_tweet_sentiment we use textblob module.\n",
    "analysis = TextBlob(self.clean_tweet(tweet))\n",
    "TextBlob is actually a high level library built over top of NLTK library. First we call clean_tweet method to remove links, special characters, etc. from the tweet using some simple regex.\n",
    "Then, as we pass tweet to create a TextBlob object, following processing is done over text by textblob library:\n",
    "\n",
    "Tokenize the tweet ,i.e split words from body of text.\n",
    "Remove stopwords from the tokens.(stopwords are the commonly used words which are irrelevant in text analysis like I, am, you, are, etc.)\n",
    "Do POS( part of speech) tagging of the tokens and select only significant features/tokens like adjectives, adverbs, etc.\n",
    "Pass the tokens to a sentiment classifier which classifies the tweet sentiment as positive, negative or neutral by assigning it a polarity between -1.0 to 1.0 .\n",
    "Here is how sentiment classifier is created:\n",
    "\n",
    "TextBlob uses a Movies Reviews dataset in which reviews have already been labelled as positive or negative.\n",
    "Positive and negative features are extracted from each positive and negative review respectively.\n",
    "Training data now consists of labelled positive and negative features. This data is trained on a Naive Bayes Classifier.\n",
    "Then, we use sentiment.polarity method of TextBlob class to get the polarity of tweet between -1 to 1.\n",
    "Then, we classify polarity as:\n",
    "\n",
    "if analysis.sentiment.polarity > 0:\n",
    "       return 'positive'\n",
    "elif analysis.sentiment.polarity == 0:\n",
    "       return 'neutral'\n",
    "else:\n",
    "       return 'negative'\n",
    "Finally, parsed tweets are returned. Then, we can do various type of statistical analysis on the tweets. For example, in above program, we tried to find the percentage of positive, negative and neutral tweets about a query.\n",
    "References:\n",
    "\n",
    "http://www.ijcaonline.org/research/volume125/number3/dandrea-2015-ijca-905866.pdf\n",
    "https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis\n",
    "textblob.readthedocs.io/en/dev/_modules/textblob/en/sentiments.html\n",
    "This article is contributed by Nikhil Kumar. If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob\n",
    "\n",
    "TextBlob is another excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. Typically, the scores have a normalized scale as compare to Afinn. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. Let’s use this now to get the sentiment polarity and labels for each news article and aggregate the summary statistics per news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# compute sentiment scores (polarity) and labels\n",
    "sentiment_scores_tb = [round(TextBlob(article).sentiment.polarity, 3) for article in news_df['clean_text']]\n",
    "sentiment_category_tb = ['positive' if score > 0 \n",
    "                             else 'negative' if score < 0 \n",
    "                                 else 'neutral' \n",
    "                                     for score in sentiment_scores_tb]\n",
    "\n",
    "\n",
    "# sentiment statistics per news category\n",
    "df = pd.DataFrame([list(news_df['news_category']), sentiment_scores_tb, sentiment_category_tb]).T\n",
    "df.columns = ['news_category', 'sentiment_score', 'sentiment_category']\n",
    "df['sentiment_score'] = df.sentiment_score.astype('float')\n",
    "df.groupby(by=['news_category']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the average sentiment is the most positive in world and least positive in technology! However, these metrics might be indicating that the model is predicting more articles as positive. Let’s look at the sentiment frequency distribution per news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = sns.factorplot(x=\"news_category\", hue=\"sentiment_category\", \n",
    "                    data=df, kind=\"count\", \n",
    "                    palette={\"negative\": \"#FE2020\", \n",
    "                             \"positive\": \"#BADD07\", \n",
    "                             \"neutral\": \"#68BFF5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There definitely seems to be more positive articles across the news categories here as compared to our previous model. However, still looks like technology has the most negative articles and world, the most positive articles similar to our previous analysis. Let’s now do a comparative analysis and see if we still get similar articles in the most positive and negative categories for world news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = df[(df.news_category=='world') & (df.sentiment_score == 0.7)].index[0]\n",
    "neg_idx = df[(df.news_category=='world') & (df.sentiment_score == -0.296)].index[0]\n",
    "\n",
    "print('Most Negative World News Article:', news_df.iloc[neg_idx][['news_article']][0])\n",
    "print()\n",
    "print('Most Positive World News Article:', news_df.iloc[pos_idx][['news_article']][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, looks like the most negative world news article here is even more depressing than what we saw the last time! The most positive article is still the same as what we had obtained in our last model.\n",
    "\n",
    "Finally, we can even evaluate and compare between these two models as to how many predictions are matching and how many are not (by leveraging a confusion matrix which is often used in classification). We leverage our nifty model_evaluation_utils module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_evaluation_utils as meu\n",
    "meu.display_confusion_matrix_pretty(true_labels=sentiment_category, \n",
    "                                    predicted_labels=sentiment_category_tb, \n",
    "                                    classes=['negative', 'neutral', 'positive'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding table, the ‘Actual’ labels are predictions from the Afinn sentiment analyzer and the ‘Predicted’ labels are predictions from TextBlob. Looks like our previous assumption was correct. TextBlob definitely predicts several neutral and negative articles as positive. Overall most of the sentiment predictions seem to match, which is good!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
