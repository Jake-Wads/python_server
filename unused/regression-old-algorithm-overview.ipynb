{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Regression Algorithms\n",
    "\n",
    "1. Ordinary Least Squares\n",
    "2. Stepwise Regression\n",
    "\n",
    "### Ordinary Least Squares  \n",
    "\n",
    "OLS minimizes the sum of the squares error to find the best parameters (line intercept and coefficients) given a series of X's and Y's.  \n",
    "The algorithm chooses the parameters of a linear function by minimizing the sum of the squares of the differences between the observed dependent variable in the given dataset and the predicted value. \n",
    "This is known as the 'least squares' principle. \n",
    "\n",
    "#### Pros\n",
    "\n",
    "1. Fast to model \n",
    "2. Useful when the relationship to be modeled is not extremely complex\n",
    "3. Useful also when you don’t have a lot of data.\n",
    "4. Simple to understand and explain to stakeholders which can be very valuable for business decisions.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1. Must be univariate, that is, a single independent variables and single dependent variables. Generalized Linear Model is the substitute algorithm in those cases.  \n",
    "2. Very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.\n",
    "\n",
    "\n",
    "![ordinary_least_squares.jpeg](ordinary_least_squares.jpeg)\n",
    "\n",
    "\n",
    "### Stepwise Regression\n",
    "\n",
    "Stepwise regression is an appropriate analysis when you have many variables and you’re interested in identifying a useful subset of the predictors. It is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion\n",
    "\n",
    "#### Options\n",
    "\n",
    "1. **Forward selection**:  The algorithm begins with predictors in the model and adds the most significant variable for each step. It stops when all variables not in the model have p-values that are greater than the specified Alpha-to-Enter value.\n",
    "2. **Backward elimination**:  The algorithm begins with all predictors in the model and removes the least significant variable for each step. It stops when all variables in the model have p-values that are less than or equal to the specified Alpha-to-Remove value.\n",
    "\n",
    "#### Pros\n",
    "\n",
    "1. The stepwise approach is much faster , it's less prone to overfit the data, you often learn something by watching the order in which variables are removed or added, and it doesn't tend to drown you in details of rankings data that cause you to lose sight of the big picture.\n",
    "2. It can take into account all the predictors, as opposed to analyzing each predictor separately.   \n",
    "\n",
    "#### Cons\n",
    "\n",
    "1. If two independent variables are highly correlated, only one may end up in the model even though both may be important.\n",
    "2. Risk of overfitting\n",
    "3. No algorithm can take into account special knowledge the data scientist or analyst may have about the data. Therefore, the model selected may not be the most practical one.\n",
    "4. It is important to note that charting the individual predictors against the response is often misleading because these do not account for other predictors in the model.\n",
    "\n",
    "\n",
    "### Other Regression Algorithms\n",
    "\n",
    "1. Lasso\n",
    "2. Elastic Net\n",
    "3. Ridge Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
