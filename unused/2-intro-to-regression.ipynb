{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT REGRESSION\n",
    "\n",
    "## Linear Regression: How it works\n",
    "\n",
    "#### What it is\n",
    "\n",
    "Regression is a **supervised machine learning** technique used to model the relationships between one or more independent/feature variables and how they contribute to producing a particular outcome, represented by a **continuous dependent/target variable**.   \n",
    "\n",
    "\n",
    "#### Goal \n",
    "\n",
    "A function that 'mimics' or 'models' this relationship, so that when new observations are available, predictions of the output, or dependent variable can be made by computing the function with the new input variables.   \n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "The dependent variable is continuous and a linear function (at least to some approximation) of the independent variables. \n",
    "\n",
    "> $y = c_{0} + c_{1}x_{1} + c_{2}x_{2} + c_{3}x_{3} + … + c_{n}x_{n}$\n",
    "\n",
    "\n",
    "#### How\n",
    "\n",
    "The algorithm attempts to find the “best” choices of values for the parameters, which in a linear regression model are the coefficients, $c_{i}$, in order to make the formula as “accurate” as possible.  Once estimated, the parameters (intercept and coefficients) allow the value of the dependent variable to be obtained from the values of the independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression\n",
    "\n",
    "### Single Independent Variable\n",
    "\n",
    "- **Number of independent/feature variables:** 1\n",
    "\n",
    "- **Goal:** minimize the error between the actual values and the estimated values.  \n",
    "\n",
    "- **Parameters:**  slope, $\\alpha$, y-intercept, $\\beta$\n",
    "\n",
    "- **Function:** $y_{i} = \\beta x_{i} + \\alpha + \\epsilon_{i}$, where $\\epsilon$ is the error term\n",
    "\n",
    "![univariate_bestfitline.png](univariate_bestfitline.png)\n",
    "\n",
    "image source:  https://towardsdatascience.com/polynomial-regression-bbe8b9d97491"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Variate LInear Regression \n",
    "\n",
    "### Multiple Independent Variables\n",
    "\n",
    "Models the relationship between **multiple** independent input variables (feature variables) and an output dependent variable. The model remains linear in that the output is a linear combination of the input variables.  \n",
    "![multivariate.png](multivariate.png)\n",
    "\n",
    "image source:  http://nbviewer.ipython.org/urls/s3.amazonaws.com/datarobotblog/notebooks/multiple_regression_in_python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "Models the non-linear relationship of the independent variables and the dependent variable. For example, the relationship could follow a sine, cosine, exponential, logrithmic, or quadratic function, to name a few.  This is still considered to be linear model as the coefficients/weights associated with the features are still linear. While the curve we are fitting may be quadratic in nature. x² is only a feature. We must convert the original features into their higher order terms. (In Python, we can use the PolynomialFeatures class provided by scikit-learn, and then train the model using Linear Regression.)\n",
    "\n",
    "![polynomial_regression.png](polynomial_regression.png)\n",
    "\n",
    "image source:  https://towardsdatascience.com/polynomial-regression-bbe8b9d97491"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of regression models\n",
    "\n",
    "### Sum of Squared Error\n",
    "\n",
    "a.k.a. the residual sum of squares, the sum of squared residuals.  It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. \n",
    "\n",
    "### Coefficient of Determination\n",
    "\n",
    "aka R-Squared, measures the fraction of the total variation in the dependent variable that is captured by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "As the dimensionality of the feature space increases, the number of configurations can grow exponentially, and thus the number of configurations covered by an observation decreases.  \n",
    "This is visualized below, showing fewer observations per region as dimensionality increases.\n",
    "\n",
    "![curse_of_dimensionality.png](curse_of_dimensionality.png)\n",
    "\n",
    "image source:  https://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Regression Algorithms\n",
    "\n",
    "1. Ordinary Least Squares\n",
    "2. Stepwise Regression\n",
    "\n",
    "### Ordinary Least Squares  \n",
    "\n",
    "OLS minimizes the sum of the squares error to find the best parameters (line intercept and coefficients) given a series of X's and Y's.  \n",
    "The algorithm chooses the parameters of a linear function by minimizing the sum of the squares of the differences between the observed dependent variable in the given dataset and the predicted value. \n",
    "This is known as the 'least squares' principle. \n",
    "\n",
    "#### Pros\n",
    "\n",
    "1. Fast to model \n",
    "2. Useful when the relationship to be modeled is not extremely complex\n",
    "3. Useful also when you don’t have a lot of data.\n",
    "4. Simple to understand and explain to stakeholders which can be very valuable for business decisions.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1. Must be univariate, that is, a single independent variables and single dependent variables. Generalized Linear Model is the substitute algorithm in those cases.  \n",
    "2. Very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.\n",
    "\n",
    "\n",
    "![ordinary_least_squares.jpeg](ordinary_least_squares.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Regression\n",
    "\n",
    "Stepwise regression is an appropriate analysis when you have many variables and you’re interested in identifying a useful subset of the predictors. It is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion\n",
    "\n",
    "#### Options\n",
    "\n",
    "1. **Forward selection**:  The algorithm begins with predictors in the model and adds the most significant variable for each step. It stops when all variables not in the model have p-values that are greater than the specified Alpha-to-Enter value.\n",
    "2. **Backward elimination**:  The algorithm begins with all predictors in the model and removes the least significant variable for each step. It stops when all variables in the model have p-values that are less than or equal to the specified Alpha-to-Remove value.\n",
    "\n",
    "#### Pros\n",
    "\n",
    "1. The stepwise approach is much faster , it's less prone to overfit the data, you often learn something by watching the order in which variables are removed or added, and it doesn't tend to drown you in details of rankings data that cause you to lose sight of the big picture.\n",
    "2. It can take into account all the predictors, as opposed to analyzing each predictor separately.   \n",
    "\n",
    "#### Cons\n",
    "\n",
    "1. If two independent variables are highly correlated, only one may end up in the model even though both may be important.\n",
    "2. Risk of overfitting\n",
    "3. No algorithm can take into account special knowledge the data scientist or analyst may have about the data. Therefore, the model selected may not be the most practical one.\n",
    "4. It is important to note that charting the individual predictors against the response is often misleading because these do not account for other predictors in the model.\n",
    "\n",
    "\n",
    "### Other Regression Algorithms\n",
    "\n",
    "1. Lasso\n",
    "2. Elastic Net\n",
    "3. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
