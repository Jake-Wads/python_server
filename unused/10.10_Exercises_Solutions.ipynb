{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.01_Acquire\n",
    "1. Build a function which will use requests to access and get the HTML content from the landing pages of each of the three news categories, business, technology and sports. \n",
    "    - Request the server the content of the web page by using get()\n",
    "    - store the server’s response in the variable response\n",
    "    - Use BeautifulSoup to parse and extract the news headline and article textual content for all the news articles in each category. \n",
    "    - return a single dataframe, `df` that has all of the articles under the following 3 headers:  `'news_headline', 'news_article', 'news_category'`\n",
    "    - run the function and store the results in `'news_df'`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.02_Parse\n",
    "1. Write a function for removing the html tags, remove_html_tags(text).  It takes a single document as an input and returns the stripped text. Test the function on: `'<html><h1>Some important text</h1><h2>Less important text</h2></html>'`\n",
    "2. Write a function for removing accented characters, remove_accents(text).  It takes a single document as an input and returns the text but without the accents. Test the function on: `'Sómě Áccěntěd těxt'`\n",
    "3. Write a function for removing special characters, remove_special_chars(text, remove_digits=False, remove_apostrophe=False).  It takes as input a single document and a boolean argument for removing digits (remove_digits) and one for removing apostrophes (remove_apostrophe). It returns the text with the special characters removed.  Test the function on `'My birthday's April 1, 1999'` twice.  1) keeping the digits and the apostrophes 2) removing the digits and removing the apostrophes.\n",
    "4. Write a function for stemming each word in a single text document, simple_stemmer(text). It takes a single document as an input and returns the text with each word stemmed. Test the function on `'Cook also received perks worth $6,82,000 which include private air travel and security expenses.'`\n",
    "5. Write a function that uses spacy's lemmatization to remove word affixes to get to the base form of a word.  The function should be named lemmatize_text(text) takin a single document as input and returning the text in original form with the words' root forms.  Test the function on `'his frown goes down'`\n",
    "6. Write a function that uses nltk.corpus.stopwords to remove stopwords from a single document of text. The function should be called remove_stopwords(text) that takes as input the document.  Test the function on `'the, and, if are stopwords, computer is not'`. \n",
    "7. Build a Text Normalizer that brings all of the parsing functions together into a single function.  \n",
    "It takes as input a corpus and a boolean argument for each of the following parsing tasks as to whether it should be performed or not:  \n",
    "    1. html_stripping:  remove_html_tags(text)\n",
    "    2. to_lowercase: convert to lower case\n",
    "    3. accented_char_removal:  remove_accents(text)\n",
    "    4. special_char_removal: remove_special_chars(text, remove_digits=False, remove_apostrophe=False)\n",
    "    5. text_lemmatization: lemmatize_text(text)\n",
    "    6. stopword_removal: remove_stopwords(text)\n",
    "Add, as arguments, any additional arguments in the functions to be included (such as `remove_digits=False`)\n",
    "The function should return the normalized corpus in a list. \n",
    "`normalize_corpus(corpus, html_stripping=True, to_lowercase=True, accented_char_removal = True, special_char_removal = True, text_lemmatization = True, stopword_removal = True, remove_digits = True, remove_apostrophe = False)`\n",
    "\n",
    "\n",
    "\n",
    "8. Run your function on the entire news_df.full_text and write your new dataframe to a csv, news.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.03_PartsOfSpeech\n",
    "1. Write a function, tag_pos() takes a column of a dataframe (df.Full_Text) as input and walks through the steps to compute the parts of speech for each word in each document and returns a data frame with the following columns:  \n",
    "    - doc_id\n",
    "    - word\n",
    "    - POS_tag\n",
    "    - tag_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.01_Acquire\n",
    "1. Build a function which will use requests to access and get the HTML content from the landing pages of each of the three news categories, business, technology and sports. \n",
    "    - Request the server the content of the web page by using get()\n",
    "    - store the server’s response in the variable response\n",
    "    - Use BeautifulSoup to parse and extract the news headline and article textual content for all the news articles in each category. \n",
    "    - return a single dataframe, `df` that has all of the articles under the following 3 headers:  `'news_headline', 'news_article', 'news_category'`\n",
    "    - run the function and store the results in `'news_df'`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "seed_urls = ['https://inshorts.com/en/read/technology',\n",
    "             'https://inshorts.com/en/read/sports',\n",
    "             'https://inshorts.com/en/read/world']\n",
    "\n",
    "def build_dataset(seed_urls):\n",
    "    news_data = []\n",
    "    for url in seed_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = requests.get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        \n",
    "        news_articles = [{'news_headline': headline.find('span', \n",
    "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
    "                          'news_article': article.find('div', \n",
    "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                          'news_category': news_category}\n",
    "                         \n",
    "                            for headline, article in \n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        \n",
    "    df =  pd.DataFrame(news_data)\n",
    "    df = df[['news_headline', 'news_article', 'news_category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.02_Parse\n",
    "1. Write a function for removing the html tags, remove_html_tags(text).  It takes a single document as an input and returns the stripped text. Test the function on: `'<html><h1>Some important text</h1><h2>Less important text</h2></html>'`\n",
    "3. Wirte a function for removing accented characters, remove_accents(text).  It takes a single document as an input and returns the text but without the accents. Test the function on: `'Sómě Áccěntěd těxt'`\n",
    "3. Write a function for removing special characters, remove_special_chars(text, remove_digits=False).  It takes as input a single document and a boolean argument for removing digits (remove_digits) It returns the text with the special characters removed.  Test the function on `'My birthday is April 1, 1999'` twice.  1) keeping the digits 2) removing the digits.\n",
    "4. Write a function for stemming each word in a single text document, simple_stemmer(text). It takes a single document as an input and returns the text with each word stemmed. Test the function on `'Cook also received perks worth $6,82,000 which include private air travel and security expenses.'`\n",
    "5. Write a function that uses spacy's lemmatization to remove word affixes to get to the base form of a word.  The function should be named lemmatize_text(text) takin a single document as input and returning the text in original form with the words' root forms.  Test the function on `'his frown goes down'`\n",
    "6. Write a function that uses nltk.corpus.stopwords to remove stopwords from a single document of text. The function should be called remove_stopwords(text, is_lower_case=False) that takes as input: the document and a boolean value for whether the text is already converted to lower case or not.  In the function, provide 2 cases...one for filtering the tokens when the text is already in lower case and the other for when the text is not lower case so that it can be coverted to lower case.  Test the function on `'The, and, if are stopwords, computer is not'` and on `'the, and, if are stopwords, computer is not'`, setting the `is_lower_case` argument accordingly. \n",
    "7. Build a Text Normalizer that brings all of the parsing functions together into a single function.  \n",
    "It takes as input a corpus and a boolean argument for each of the parsing functions with a default value of `True`, and returns the normalized corpus in a list. `normalize_corpus(corpus, html_stripping=True, contraction_expansion = True, accented_char_removal = True, text_lower_case = True, text_lemmatization = True, special_char_removal = True, stopword_removal = True, remove_digits = True)`\n",
    "8. Run your function on the entire news_df.full_text and write your new dataframe to a csv, news.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some important textLess important text'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Write a function for removing the html tags, remove_html_tags(text).  \n",
    "# It takes a single document as an input and returns the stripped text.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "remove_html_tags('<html><h1>Some important text</h1><h2>Less important text</h2></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Wirte a function for removing accented characters, remove_accents(text).  \n",
    "# It takes a single document as an input and returns the text but without the accents. \n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "text = 'Sómě Áccěntěd těxt'\n",
    "remove_accents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[^a-zA-Z'\\s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My birthday's April 1 1999\n",
      "My birthdays April  \n"
     ]
    }
   ],
   "source": [
    "# 3. Write a function for removing special characters, remove_special_chars(text, remove_digits=False)\n",
    "# It takes as input a single document and a boolean argument for removing digits (remove_digits)\n",
    "# It returns the text with the special characters removed.\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False, remove_apostrophe=False):\n",
    "    if remove_digits==False and remove_apostrophe==False:\n",
    "        pattern = r\"[^a-zA-Z0-9'\\s]\"\n",
    "    elif remove_digits==False and remove_apostrophe==True:\n",
    "        pattern = r\"[^a-zA-Z0-9\\s]\"\n",
    "    elif remove_digits==True and remove_apostrophe==False:\n",
    "        pattern = r\"[^a-zA-Z'\\s]\"\n",
    "    else:\n",
    "        pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "text = \"My birthday's April 1, 1999\"\n",
    "print(remove_special_characters(text, remove_digits=False, remove_apostrophe=False))\n",
    "print(remove_special_characters(text, remove_digits=True, remove_apostrophe=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook also receiv perk worth $6,82,000 which includ privat air travel and secur expenses.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Write a function for stemming each word in a single text document, simple_stemmer(text).\n",
    "# It takes a single document as an input and returns the text with each word stemmed. \n",
    "import nltk\n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "text = 'Cook also received perks worth $6,82,000 which include private air travel and security expenses.'\n",
    "simple_stemmer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his frown go down'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Write a function that uses spacy's lemmatization to remove word affixes to get to the base form of a word\n",
    "# The function should be named lemmatize_text(text) takin a single document as input and returning the text\n",
    "# in original form with the words' root forms.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en', parse = True, tag=True, entity=True)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "text = 'his frown goes down'\n",
    "lemmatize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Write a function that uses nltk.corpus.stopwords to remove stopwords from a single document of text. \n",
    "# The function should be called remove_stopwords(text) that takes as input the document. \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"the, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Build a Text Normalizer that brings all of the parsing functions together into a single function.  \n",
    "# It takes as input a corpus and a boolean argument for each of the following parsing tasks as to whether it should be performed or not:  \n",
    "#    1. html_stripping:  remove_html_tags(text)\n",
    "#    2. to_lowercase: convert to lower case\n",
    "#    3. accented_char_removal:  remove_accents(text)\n",
    "#    4. special_char_removal: remove_special_chars(text, remove_digits=False, remove_apostrophe=False)\n",
    "#    5. text_lemmatization: lemmatize_text(text)\n",
    "#    6. stopword_removal: remove_stopwords(text)\n",
    "# Add, as arguments, any additional arguments in the functions to be included (such as `remove_digits=False`)\n",
    "# The function should return the normalized corpus in a list. \n",
    "# `normalize_corpus(corpus, html_stripping=True, to_lowercase=True, accented_char_removal = True, special_char_removal = True, text_lemmatization = True, stopword_removal = True, remove_digits = True, remove_apostrophe = False)`\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, to_lowercase=True, accented_char_removal = True, \n",
    "                     special_char_removal = True, text_lemmatization = True, stopword_removal = True, \n",
    "                     remove_digits = True, remove_apostrophe = False):    \n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = remove_html_tags(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accents(doc)\n",
    "        # lemmatize text\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits, remove_apostrophe=remove_apostrophe)  \n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove extra newlines\n",
    "#       doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Run your function on the entire news_df.full_text and write your new dataframe to a csv, news.csv \n",
    "\n",
    "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
    "norm_corpus = list(news_df['clean_text'])\n",
    "\n",
    "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
