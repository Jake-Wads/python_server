{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling:  Processing Text and Developing Topic Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Topic modeling algorithms such as Latent Dirichlet allocation extract themes from a set of text documents.\n",
    "\n",
    "- Clustering algorithms such as K-means and Gaussian mixture models assume that an observation belongs to one and only one cluster.\n",
    "\n",
    "- Latent Dirichlet allocation assumes that each document belongs to one or more topics.\n",
    "\n",
    "- The number of topics is a hyperparameter.\n",
    "\n",
    "- Topic models can be used to categorize tweets.\n",
    "\n",
    "- In this demonstration we will use latent Dirichlet allocation (LDA) to look for topics in tweets.  We will also perform some basic natural language processing (NLP) to prepare the data for LDA.\n",
    "\n",
    "## Lesson\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import explode   \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"lda_text.py\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the twitter data, which includes the following attributes: \"Topic\",\"Sentiment\",\"TweetId\",\"TweetDate\",\"TweetText\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126415614616154112</td>\n",
       "      <td>Tue Oct 18 21:53:25 +0000 2011</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "      <td>Tue Oct 18 21:09:33 +0000 2011</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "      <td>Tue Oct 18 21:02:20 +0000 2011</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "      <td>Tue Oct 18 20:40:10 +0000 2011</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "      <td>Tue Oct 18 20:34:00 +0000 2011</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Sentiment             TweetId                       TweetDate  \\\n",
       "0  apple  positive  126415614616154112  Tue Oct 18 21:53:25 +0000 2011   \n",
       "1  apple  positive  126404574230740992  Tue Oct 18 21:09:33 +0000 2011   \n",
       "2  apple  positive  126402758403305474  Tue Oct 18 21:02:20 +0000 2011   \n",
       "3  apple  positive  126397179614068736  Tue Oct 18 20:40:10 +0000 2011   \n",
       "4  apple  positive  126395626979196928  Tue Oct 18 20:34:00 +0000 2011   \n",
       "\n",
       "                                           TweetText  \n",
       "0  Now all @Apple has to do is get swype on the i...  \n",
       "1  @Apple will be adding more carrier support to ...  \n",
       "2  Hilarious @youtube video - guy does a duet wit...  \n",
       "3  @RIM you made it too easy for me to switch to ...  \n",
       "4  I just realized that the reason I got into twi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"data/twitter_corpus.csv\"\n",
    "df = spark.read.csv(data, header=True).na.drop()\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and transforming features\n",
    "\n",
    "- Spark MLlib provides a number of feature extractors and feature transformers to preprocess the tweets into a form appropriate for modeling.\n",
    "\n",
    "#### Clean and tokenize the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the [RegexTokenizer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer) class to tokenize the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"TweetText\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords\n",
    "\n",
    "- Spark MLlib provides a transformer to remove stopwords.\n",
    "- Use the [StopWordsRemover](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover) class to remove common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").\\\n",
    "        setStopWords(add_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the frequency of words in each tweet\n",
    "\n",
    "- Use the [CountVectorizer](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer) class to compute the term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", \n",
    "                               vocabSize=100, minDF=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `fit` method computes the top $N$ words where $N$ is set via the `vocabSize` hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def pipe(v, *fns):\n",
    "    return reduce(lambda x, f: f(x), fns, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pipelineFit.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouble shooting:  \n",
    "\n",
    "# Not using the pipeline: \n",
    "\n",
    "tokenized = regexTokenizer.transform(df)\n",
    "filtered = stopwordsRemover.transform(tokenized)\n",
    "\n",
    "## having issues with calling fit on the count vectorizer! \n",
    "\n",
    "vectorized = countVectors.fit(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The resulting word vector is stored as a [SparseVector](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.linalg.SparseVector).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify and fit a topic model using latent Dirichlet allocation (LDA)\n",
    "\n",
    "- Use the `LDA` class to specify an LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(featuresCol=\"features\", k=2, seed=23456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `explainParams` method to examine additional hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "docConcentration: Concentration parameter (commonly named \"alpha\") for the prior placed on documents' distributions over topics (\"theta\"). (undefined)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "k: The number of topics (clusters) to infer. Must be > 1. (default: 10, current: 2)\n",
      "keepLastCheckpoint: (For EM optimizer) If using checkpointing, this indicates whether to keep the last checkpoint. If false, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. (default: True)\n",
      "learningDecay: Learning rate, set as anexponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. (default: 0.51)\n",
      "learningOffset: A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less (default: 1024.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "optimizeDocConcentration: Indicates whether the docConcentration (Dirichlet parameter for document-topic distribution) will be optimized during training. (default: True)\n",
      "optimizer: Optimizer or inference algorithm used to estimate the LDA model.  Supported: online, em (default: online)\n",
      "seed: random seed. (default: 8405381931737959799, current: 23456)\n",
      "subsamplingRate: Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. (default: 0.05)\n",
      "topicConcentration: Concentration parameter (commonly named \"beta\" or \"eta\") for the prior placed on topic' distributions over terms. (undefined)\n",
      "topicDistributionCol: Output column with estimates of the topic mixture distribution for each document (often called \"theta\" in the literature). Returns a vector of zeros for an empty document. (default: topicDistribution)\n"
     ]
    }
   ],
   "source": [
    "print(lda.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `fit` method to fit the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = lda.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The resulting model is an instance of the `LDAModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.clustering.LocalLDAModel"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the LDA topic model\n",
    "\n",
    "- Examine the estimated distribution of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.4833, 0.5488])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.estimatedDocConcentration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine the estimated distribution of words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(100, 2, [372.4489, 668.3935, 31.1719, 308.9413, 22.1776, 100.7804, 126.8033, 71.8673, ..., 37.2824, 37.0261, 24.7345, 26.9979, 22.9744, 1.2864, 1.2527, 20.4995], 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=0, termIndices=[1, 0, 8, 3, 13, 9, 25, 14, 24, 6], termWeights=[0.1248746577244419, 0.06958390260879416, 0.06040851853180488, 0.0577188961304506, 0.03181813614912774, 0.029369340526942275, 0.024398159614899058, 0.024138727222349568, 0.02377859546634016, 0.023690416903199462]),\n",
       " Row(topic=1, termIndices=[2, 0, 4, 5, 3, 6, 7, 15, 10, 11], termWeights=[0.08197539387027768, 0.07460177931634981, 0.06827597090783148, 0.045118127189146956, 0.03880336339759435, 0.03753399649358934, 0.032804124666203006, 0.024632188134694716, 0.02440204456515243, 0.023828759138879642])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.describeTopics().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function to print out the terms and weights for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, n_terms, vocabulary):\n",
    "    rows = model.describeTopics(n_terms).collect()\n",
    "    for row in rows:\n",
    "        print(\"---- Topic %s ----\" % row[\"topic\"])\n",
    "        print(zip([vocabulary[i] for i in row[\"termIndices\"]], row[\"termWeights\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TweetText='Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is', topicDistribution=DenseVector([0.0352, 0.9648])),\n",
       " Row(TweetText='@Apple will be adding more carrier support to the iPhone 4S (just announced)', topicDistribution=DenseVector([0.0602, 0.9398])),\n",
       " Row(TweetText=\"Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\", topicDistribution=DenseVector([0.0876, 0.9124])),\n",
       " Row(TweetText='@RIM you made it too easy for me to switch to @Apple iPhone. See ya!', topicDistribution=DenseVector([0.0608, 0.9392])),\n",
       " Row(TweetText='I just realized that the reason I got into twitter was ios5 thanks @apple', topicDistribution=DenseVector([0.0733, 0.9267]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lda_model.transform(dataset)\n",
    "predictions.select(\"TweetText\", \"topicDistribution\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine various model performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.149987740101946"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logLikelihood(dataset)\n",
    "lda_model.logPerplexity(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Wikipedia - Topic model](https://en.wikipedia.org/wiki/Topic_model)\n",
    "- [Wikipedia - Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "- [Spark Documentation - Latent Dirichlet allocation](http://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda)\n",
    "- [Spark Python API - pyspark.ml.clustering.LDA class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA)\n",
    "- [Spark Python API - pyspark.ml.clustering.LDAModel class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LDAModel)\n",
    "- [Spark Python API - pyspark.ml.clustering.LocalLDAModel class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.LocalLDAModel)\n",
    "- [Spark Python API - psypark.ml.clustering.DistributedLDAModel class](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.DistributedLDAModel)\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Use the `NGram` transformer to generate pairs of words (bigrams) from the tokenized tweets.  *See below if you need some hints/additional guidance*.\n",
    "\n",
    "2. Fit an LDA model with $k=3$ topics.  *See below if you need some hints/additional guidance*.\n",
    "\n",
    "### Exercise Hints\n",
    "\n",
    "#### Exercise 1:\n",
    "\n",
    "1. Import the `NGram` class from the `pyspark.ml.feature` module\n",
    "2. Create an instance of the `NGram` class\n",
    "3. Use the `transform` method to apply the `NGram` instance to the `tokenized` DataFrame\n",
    "4. Print out a few rows of the transformed DataFrame\n",
    "\n",
    "#### Exercise 2:\n",
    "\n",
    "1. Use the `setK` method to change the number of topics for the `lda` instance\n",
    "2. Use the `fit` method to fit the LDA model to the `vectorized` DataFrame\n",
    "3. Use the `print_topics` function to examine the topics\n",
    "4. Use the `transform` method to apply the LDA model to the `vectorized` DataFrame\n",
    "5. Print out a few rows of the transformed DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
