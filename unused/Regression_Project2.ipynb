{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression Project\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "### Background: \n",
    "\n",
    "Zillow wants to improve their Zestimate.  The zestimate is estimated value of a home.  Zillow theorizes that there is more information to be gained to improve its existing model.  Because of that, Zillow wants you to develop a model to predict the error between the Zestimate and the sales price of a home.  In predicting the error, you will discover features that will help them improve the Zestimate estimate itself.  Your goal of this project is to develop a linear regression model that will best predict the log error of the Zestimate.  The error is the difference of the sales price and the Zestimate.  The log error is computed by taking the log function of that error.  You don't need to worry about the fact that the error is of a logarithmic function.  It is a continuous number that represents an error rate. \n",
    "\n",
    "### Your deliverables:\n",
    "\n",
    "1. A report (in the form of a presentation, both verbal and through a slides) that summarizes your findings about the drivers of the Zestimate error.  This will be come from the analysis you do during the exploration phase of the pipeline.  In the report, you will have charts that visually tell the story of what is driving the errors.\n",
    "\n",
    "2. A Jupyter notebook, titled 'Regression_Proj_YourName', that contains a clearly labeled section and code clearly documented for each the stages below (project planning, data acquisition, data prep, exploration, and modeling).  All of the work will take place in your jupyter notebook. \n",
    "\n",
    "\n",
    "## Project Planning\n",
    "\n",
    "You will use the available data dictionary to understand the fields.  During the Project Planning stage, you will want to document the meaning of the fields you will be using (see below for list of fields).   You will want to brainstorm how you plan to go about this, especially your thoughts around your analysis, any hypotheses you might have, ideas about what might matter, what might not.  This part, in the real world, is for you and maybe your co-workers only.  Grammer, technicalities, etc. do not matter here.  It is about creating a plan so that you know when you go off course and get lost in the weeds.  You will likely not follow your plan exactly, which is why it is important to clearly state your goal.  That way when you get a bit lost, you can quickly refer to the goal and ask, \"Is what I'm doing now the best use of my time to help me achieve my goal?\"\n",
    "\n",
    "Also in this step, prepare your environment with the Python libraries you will need throughout your project.  You may need to go back and add more as you go.  \n",
    "\n",
    "\n",
    "1. Your goal clearly stated.  Why?  So that when you get a bit lost, you can quickly refer to the goal and ask, \"Is what I'm doing now the best use of my time to help me achieve my goal?\"\n",
    "\n",
    "2. Your deliverables clearly stated, so you know when you are done!\n",
    "\n",
    "3. Data dictionary of fields you will use. Why? So that you can refer back and others can refer to the meanings as you are developing your model.  This is about gaining knowledge in the domain space so that you will understand when data doesn't look right, be able to more effectively develop hypotheses, and use that domain knowledge to build a more robust model (among other reasons)\n",
    "\n",
    "4. Brainstorming ideas, hypotheses, related to how variables might impact or relate to each other, both within indepependent variables and between the independent variables and dependent variable, and also related to any ideas for new features you may have while first looking at the existing variables and challenge ahead of you.\n",
    " \n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "Using the SQL zillow schema, write a query via Python to generate a cohesive data set that includes the following fields:\n",
    "\n",
    "    - `logerror`\n",
    "    - `bathroomcnt`\n",
    "    - `bedroomcnt`\n",
    "    - `calculatedfinishedsquarefeet`\n",
    "    - `fullbathcnt`\n",
    "    - `garagecarcnt`\n",
    "    - `roomcnt`\n",
    "    - `yearbuilt`\n",
    "    - `taxvaluedollarcnt`\n",
    "    - `taxamount`\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "1. Sample the data.  Why?  So you can confirm the data look like what you would expect.\n",
    "2. Create a variable, `colnames`, that is a list of the column names.  Why?  You will likely reference this variable later. \n",
    "3. Identify the data types of each variable.  Why? You need to make sure they are what makes sense for the data and the meaning of the data that lies in that variable.  If it does not, make necessary changes.  \n",
    "4. Compute the summary statistics for the variables.  Why?  The get a glimpse into outliers, skewness, spread, central tendency.  \n",
    "5. Identify the columns that have missing values and the number of missing values in each column. Why? Missing values are going to cause issues down the line so you will need to handle those appropriately.  For each variable with missing values, if it makes sense to replace those missing with a 0, do so.  For those where that doesn't make sense, decide if you should drop the entire observations (rows) that contain the missing values, or drop the entire variable (column) that contains the missing values. \n",
    "6. Create a list of the independent variable names (aka attributes) and assign it to the variable `attributes`. Why? During exploration, you will likely use this list to refer to the attribute names. \n",
    "7. Clearly identify your dependent (target) variable.  What is the name of the variable? Is it discrete or continuous?\n",
    "8. Plot a histogram and box plot of each variable.  Why?  To see the distribution, skewness, outliers, and unit scales.  You will use this information in your decision of whether to normalize, standardize or neither. \n",
    "9. Bonus: Create a new data frame that is the min-max normalization of the independent variable in the original data frame (+ the original dependent variable).  You will normalize each of the independent variables independently, i.e. using the min and max of each variable, not the min/max of the whole dataframe. Why?  Regression is very sensitive to difference in units.  It will be almost impossible to extract a meaningful linear regression model with such extreme differences in scale.  For more context, see: https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc\n",
    "\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "1. Split data into training and test datasets\n",
    "2. Address each of the questions you posed in your planning & brainstorming through visual or statistical analysis.  \n",
    "3. Create a jointplot for each independent variable (normalized version) with the dependent variable.  Use your for loop created in the exercises to run through the plotting of each independent variable.  Be sure you have Pearson's r and p-value annotated on each plot.  \n",
    "4. Create a feature plot using seaborn's PairGrid() of the interaction between each variable (dependent + independent).  You will want to use the normalized dataframe so you can more clearly view the interactions.  \n",
    "5. Create a heatmap of the correlation between each variable pair.  \n",
    "6. Summarize your conclusions from these steps. \n",
    "7. Is the logerror significantly different for homes with 3 bedrooms vs those with 5 or more bedrooms?  Run a t-test to test this difference.  \n",
    "8. Do the same for another 2 samples you are interested in comparing (e.g. those with 1 bath vs. x baths)\n",
    "\n",
    "## Data Modeling\n",
    "\n",
    "### Feature Engineering & Selection\n",
    "\n",
    "1. Are there new features you could create based on existing features that might be helpful?  Come up with at least one possible new feature that is a calculation from 2+ existing variables.  Add that feature and update the normalized dataframe with the min-max normalization of that feature. \n",
    "2. Use statsmodels ordinary least squares to assess the importance of each feature with respect to the target (using the normalized dataframe)\n",
    "3. Summarize your conclusions and next steps from your analysis in step 2.  What will you try when developing your model?  (which features to use/not use/etc)\n",
    "\n",
    "#### Train & Test Model\n",
    "\n",
    "1. Fit, predict (in-sample) & evaluate multiple linear regression models to find the best one.\n",
    "2. Make any changes as necessary to improve your model.\n",
    "3. Identify the best model after all training and predict & evaluate on out-of-sample data.  \n",
    "4. Plot the residuals from your out-of-sample predictions.  \n",
    "5. Summarize your expectations about how you estimate this model will perform in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
