{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Acquisition\n",
    "\n",
    "## HDFS  & Amazon S3\n",
    "\n",
    "> `from pyspark.sql import SparkSession  \n",
    ">  spark = SparkSession.builder\\  \n",
    ">     .master(\"local\")\\  \n",
    ">     .appName(\"read\")\\  \n",
    ">     .enableHiveSupport()\\  \n",
    ">     .getOrCreate()`\n",
    "\n",
    "\n",
    "### HDFS\n",
    "\n",
    "#### Read\n",
    "\n",
    "The `text` method of the \n",
    "[DataFrameReader](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)\n",
    "reads each line of a text file into a row of a DataFrame with a single column named *value* so we will need to apply further transformations to parse each line.  \n",
    "\n",
    "> `df = spark.read.text(\"hdfs:///sa311/source/\")\n",
    ">  df.show(5, truncate=False)\n",
    ">  df.head(5)`\n",
    "\n",
    "To create an hdfs subdirectory for your files: `!hdfs dfs -mkdir my_311`\n",
    "\n",
    "To remove a directory-a hard delete:  `!hdfs dfs -rm -r -skipTrash my_311`\n",
    "\n",
    "\n",
    "#### Write\n",
    "\n",
    "The `text` method of the [DataFrameWriter](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter)\n",
    "writes each row of a DataFrame with a single string column into a line of a text file.  \n",
    "\n",
    "> `df.write.text(\"my_311/df_text\")`  \n",
    "\n",
    "For a compressed file\n",
    "\n",
    "> `df.write.text(\"my_311/df_text_compressed\", compression=\"bzip2\")`   \n",
    "\n",
    "Ensure the file is there, and peek into the contents.\n",
    "\n",
    "> `!hdfs dfs -ls my_311/df_text  \n",
    ">  !hdfs dfs -ls my_311/df_text_compressed  \n",
    ">  !hdfs dfs -cat my_311/df_text/* | head -n 5`\n",
    "\n",
    "### AMAZON S3\n",
    "\n",
    "AWS (Amazon Web Services) provides a product known as Amazon S3, Simple Storage Service.  S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. \n",
    "\n",
    "#### Read from S3\n",
    "\n",
    "In order for all fo this to work, you would need an S3 bucket created, configured, with data stored within it.  \n",
    "\n",
    "[Getting started with S3](https://aws.amazon.com/s3/getting-started/)\n",
    "[Store and retrieve files with S3](https://aws.amazon.com/getting-started/tutorials/backup-files-to-amazon-s3/?trk=s3-gs)\n",
    "\n",
    "- pyspark.sql.DataFrameReader: `spark.read.csv()`\n",
    "\n",
    "- The read method depends on the format in which the files in S3 are stored.   \n",
    "\n",
    "- If we were reading a tab delimited, we would indicate by the `sep = \"\\t\"` argument.\n",
    "\n",
    "> `df = spark.read.csv(\"s3a://bucketname/filename\",\n",
    "    sep=\",\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)`\n",
    "\n",
    "\n",
    "#### Write to S3\n",
    "\n",
    "- pyspark.sql.DataFrameWriter: `df.write.csv()`\n",
    "\n",
    "- Write tab delimited\n",
    "\n",
    "> `df.write.csv(\"s3a://bucketname/df_tsv\", sep=\"\\t\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
