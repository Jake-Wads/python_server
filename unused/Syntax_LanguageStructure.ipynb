{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *IN PROGRESS*\n",
    "## *need to merge the information below with others, add, edit, delete, customize, change source data etc. to complete this section and rest of nlp*\n",
    "\n",
    "\n",
    "# NLP Syntax & Language Structure\n",
    "\n",
    "Knowledge about the structure and syntax of language is helpful in many areas like text processing, annotation, and parsing for further operations such as text classification or summarization. Typical parsing techniques for understanding text syntax are mentioned below.\n",
    "\n",
    "1. Parts of Speech (POS) Tagging\n",
    "2. Shallow Parsing or Chunking\n",
    "3. Constituency Parsing\n",
    "4. Dependency Parsing\n",
    "\n",
    "A sentence typically follows a hierarchical structure consisting the following components:   \n",
    "sentence → clauses → phrases → words\n",
    "\n",
    "## POS Tagging\n",
    "\n",
    "Parts of speech (POS) are specific lexical categories to which words are assigned, based on their syntactic context and role. Usually, words can fall into one of the following major categories.\n",
    "\n",
    "- N(oun): This usually denotes words that depict some object or entity, which may be living or nonliving. Some examples would be fox , dog , book , and so on. The POS tag symbol for nouns is N.\n",
    "- V(erb): Verbs are words that are used to describe certain actions, states, or occurrences. There are a wide variety of further subcategories, such as auxiliary, reflexive, and transitive verbs (and many more). Some typical examples of verbs would be running , jumping , read , and write . The POS tag symbol for verbs is V.\n",
    "- Adj(ective): Adjectives are words used to describe or qualify other words, typically nouns and noun phrases. The phrase beautiful flower has the noun (N) flower which is described or qualified using the adjective (ADJ) beautiful . The POS tag symbol for adjectives is ADJ .\n",
    "- Adv(erb): Adverbs usually act as modifiers for other words including nouns, adjectives, verbs, or other adverbs. The phrase very beautiful flower has the adverb (ADV) very , which modifies the adjective (ADJ) beautiful , indicating the degree to which the flower is beautiful. The POS tag symbol for adverbs is ADV.\n",
    "- Other categories that occur frequently in the English language include pronouns, prepositions, interjections, conjunctions, determiners, and many others. \n",
    "- Each POS tag like the noun (N) can be further subdivided into categories like singular nouns (NN), singular proper nouns (NNP), and plural nouns (NNS).\n",
    "\n",
    "The process of classifying and labeling POS tags for words called parts of speech tagging or POS tagging . POS tags are used to annotate words and depict their POS, which is really helpful to perform specific analysis, such as narrowing down upon nouns and seeing which ones are the most prominent, word sense disambiguation, and grammar analysis. We will be leveraging both nltk and spacy which usually use the Penn Treebank notation for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a basic pre-processed corpus, don't lowercase to get POS context\n",
    "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
    "                          text_lemmatization=False, special_char_removal=False)\n",
    "\n",
    "# demo for POS tagging for sample news headline\n",
    "sentence = str(news_df.iloc[1].news_headline)\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "# POS tagging with Spacy \n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
    "\n",
    "# POS tagging with nltk\n",
    "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of these libraries treat tokens in their own way and assign specific tags for them. Based on what we see, spacy seems to be doing slightly better than nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Parsing  or Chunking\n",
    "Based on the hierarchy we depicted earlier, groups of words make up phrases. There are five major categories of phrases:\n",
    "\n",
    "- Noun phrase (NP): These are phrases where a noun acts as the head word. Noun phrases act as a subject or object to a verb.\n",
    "- Verb phrase (VP): These phrases are lexical units that have a verb acting as the head word. Usually, there are two forms of verb phrases. One form has the verb components as well as other entities such as nouns, adjectives, or adverbs as parts of the object.\n",
    "- Adjective phrase (ADJP): These are phrases with an adjective as the head word. Their main role is to describe or qualify nouns and pronouns in a sentence, and they will be either placed before or after the noun or pronoun.\n",
    "- Adverb phrase (ADVP): These phrases act like adverbs since the adverb acts as the head word in the phrase. Adverb phrases are used as modifiers for nouns, verbs, or adverbs themselves by providing further details that describe or qualify them.\n",
    "- Prepositional phrase (PP): These phrases usually contain a preposition as the head word and other lexical components like nouns, pronouns, and so on. These act like an adjective or adverb describing other words or phrases.\n",
    "\n",
    "Shallow parsing, also known as light parsing or chunking , is a popular natural language processing technique of analyzing the structure of a sentence to break it down into its smallest constituents (which are tokens such as words) and group them together into higher-level phrases. This includes POS tags as well as phrases from a sentence.\n",
    "\n",
    "An example of shallow parsing depicting higher level phrase annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "data = conll2000.chunked_sents()\n",
    "train_data = data[:10900]\n",
    "test_data = data[10900:] \n",
    "\n",
    "print(len(train_data), len(test_data))\n",
    "print(train_data[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding output, you can see that our data points are sentences that are already annotated with phrases and POS tags metadata that will be useful in training our shallow parser model. We will leverage two chunking utility functions, tree2conlltags , to get triples of word, tag, and chunk tags for each token, and conlltags2tree to generate a parse tree from these token triples. We will be using these functions to train our parser. A sample is depicted below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "wtc = tree2conlltags(train_data[1])\n",
    "wtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk tags use the IOB format. This notation represents Inside, Outside, and Beginning. The B- prefix before a tag indicates it is the beginning of a chunk, and I- prefix indicates that it is inside a chunk. The O tag indicates that the token does not belong to any chunk. The B- tag is always used when there are subsequent tags of the same type following it without the presence of O tags between them.\n",
    "\n",
    "We will now define a function conll_tag_ chunks() to extract POS and chunk tags from sentences with chunked annotations and a function called combined_taggers() to train multiple taggers with backoff taggers (e.g. unigram and bigram taggers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a class NGramTagChunker that will take in tagged sentences as training input, get their (word, POS tag, Chunk tag) WTC triples, and train a BigramTagger with a UnigramTagger as the backoff tagger. We will also define a parse() function to perform shallow parsing on new sentences\n",
    "\n",
    "The UnigramTagger, BigramTagger, and TrigramTagger are classes that inherit from the base class NGramTagger , which itself inherits from the ContextTagger class , which inherits from the SequentialBackoffTagger class .\n",
    "We will use this class to train on the conll2000 chunked train_data and evaluate the model performance on the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# define the chunker class\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "  \n",
    "# train chunker model  \n",
    "ntc = NGramTagChunker(train_data)\n",
    "\n",
    "# evaluate chunker model performance\n",
    "print(ntc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chunking model gets an accuracy of around 90% which is quite good! Let’s now leverage this model to shallow parse and chunk our sample news article headline which we used earlier, “US unveils world’s most powerful supercomputer, beats China”.\n",
    "\n",
    "Thus you can see it has identified two noun phrases (NP) and one verb phrase (VP) in the news article. Each word’s POS tags are also visible. We can also visualize this in the form of a tree as follows. You might need to install ghostscript in case nltk throws an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "## download and install ghostscript from https://www.ghostscript.com/download/gsdnld.html\n",
    "\n",
    "# often need to add to the path manually (for windows)\n",
    "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
    "\n",
    "display(chunk_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency Parsing\n",
    "\n",
    "Constituent-based grammars are used to analyze and determine the constituents of a sentence. These grammars can be used to model or represent the internal structure of sentences in terms of a hierarchically ordered structure of their constituents. Each and every word usually belongs to a specific lexical category in the case and forms the head word of different phrases. These phrases are formed based on rules called phrase structure rules.\n",
    "\n",
    "Phrase structure rules form the core of constituency grammars, because they talk about syntax and rules that govern the hierarchy and ordering of the various constituents in the sentences. These rules cater to two things primarily.\n",
    "\n",
    "They determine what words are used to construct the phrases or constituents.\n",
    "They determine how we need to order these constituents together.\n",
    "The generic representation of a phrase structure rule is S → AB , which depicts that the structure S consists of constituents A and B , and the ordering is A followed by B . While there are several rules (refer to Chapter 1, Page 19: Text Analytics with Python, if you want to dive deeper), the most important rule describes how to divide a sentence or a clause. The phrase structure rule denotes a binary division for a sentence or a clause as S → NP VP where S is the sentence or clause, and it is divided into the subject, denoted by the noun phrase (NP) and the predicate, denoted by the verb phrase (VP).\n",
    "\n",
    "A constituency parser can be built based on such grammars/rules, which are usually collectively available as context-free grammar (CFG) or phrase-structured grammar. The parser will process input sentences according to these rules, and help in building a parse tree.\n",
    "\n",
    "An example of constituency parsing showing a nested hierarchical structure\n",
    "We will be using nltk and the StanfordParser here to generate parse trees.\n",
    "\n",
    "Prerequisites: Download the official Stanford Parser from here, which seems to work quite well. You can try out a later version by going to this website and checking the Release History section. After downloading, unzip it to a known location in your filesystem. Once done, you are now ready to use the parser from nltk , which we will be exploring soon.\n",
    "The Stanford parser generally uses a PCFG (probabilistic context-free grammar) parser. A PCFG is a context-free grammar that associates a probability with each of its production rules. The probability of a parse tree generated from a PCFG is simply the production of the individual probabilities of the productions used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " set java path\n",
    "import os\n",
    "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                     path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "                   \n",
    "result = list(scp.raw_parse(sentence))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing  \n",
    "\n",
    "In dependency parsing, we try to use dependency-based grammars to analyze and infer both structure and semantic dependencies and relationships between tokens in a sentence. The basic principle behind a dependency grammar is that in any sentence in the language, all words except one, have some relationship or dependency on other words in the sentence. The word that has no dependency is called the root of the sentence. The verb is taken as the root of the sentence in most cases. All the other words are directly or indirectly linked to the root verb using links , which are the dependencies.\n",
    "\n",
    "Considering our sentence “The brown fox is quick and he is jumping over the lazy dog” , if we wanted to draw the dependency syntax tree for this, we would have the structure\n",
    "\n",
    "\n",
    "A dependency parse tree for a sentence\n",
    "These dependency relationships each have their own meaning and are a part of a list of universal dependency types. This is discussed in an original paper, Universal Stanford Dependencies: A Cross-Linguistic Typology by de Marneffe et al, 2014). You can check out the exhaustive list of dependency types and their meanings here.\n",
    "\n",
    "If we observe some of these dependencies, it is not too hard to understand them.\n",
    "\n",
    "- The dependency tag det is pretty intuitive — it denotes the determiner relationship between a nominal head and the determiner. Usually, the word with POS tag DET will also have the det dependency tag relation. Examples include fox → the and dog → the.\n",
    "- The dependency tag amod stands for adjectival modifier and stands for any adjective that modifies the meaning of a noun. Examples include fox → brown and dog → lazy.\n",
    "- The dependency tag nsubj stands for an entity that acts as a subject or agent in a clause. Examples include is → fox and jumping → he.\n",
    "- The dependencies cc and conj have more to do with linkages related to words connected by coordinating conjunctions . Examples include is → and and is → jumping.\n",
    "- The dependency tag aux indicates the auxiliary or secondary verb in the clause. Example: jumping → is.\n",
    "- The dependency tag acomp stands for adjective complement and acts as the complement or object to a verb in the sentence. Example: is → quick\n",
    "- The dependency tag prep denotes a prepositional modifier, which usually modifies the meaning of a noun, verb, adjective, or preposition. Usually, this representation is used for prepositions having a noun or noun phrase complement. Example: jumping → over.\n",
    "- The dependency tag pobj is used to denote the object of a preposition . This is usually the head of a noun phrase following a preposition in the sentence. Example: over → dog.\n",
    "\n",
    "Spacy had two types of English dependency parsers based on what language models you use, you can find more details here. Based on language models, you can use the Universal Dependencies Scheme or the CLEAR Style Dependency Scheme also available in NLP4J now. We will now leverage spacy and print out the dependencies for each token in our news headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in sentence_nlp:\n",
    "    print(dependency_pattern.format(word=token.orth_, \n",
    "                                  w_type=token.dep_,\n",
    "                                  left=[t.orth_ \n",
    "                                            for t \n",
    "                                            in token.lefts],\n",
    "                                  right=[t.orth_ \n",
    "                                             for t \n",
    "                                             in token.rights]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the verb beats is the ROOT since it doesn’t have any other dependencies as compared to the other tokens. For knowing more about each annotation you can always refer to the CLEAR dependency scheme. We can also visualize the above dependencies in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence_nlp, jupyter=True, \n",
    "                options={'distance': 110,\n",
    "                         'arrow_stroke': 2,\n",
    "                         'arrow_width': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree. We showcase the dependency tree both in its raw and annotated form as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
    "\n",
    "result = list(sdp.raw_parse(sentence))  \n",
    "\n",
    "# print the dependency tree\n",
    "dep_tree = [parse.tree() for parse in result][0]\n",
    "print(dep_tree)\n",
    "\n",
    "# visualize raw dependency tree\n",
    "from IPython.display import display\n",
    "display(dep_tree)\n",
    "\n",
    "# visualize annotated dependency tree (needs graphviz)\n",
    "from graphviz import Source\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
