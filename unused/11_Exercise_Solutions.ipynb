{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## 11.01_GettingStarted\n",
    "\n",
    "1. Create a SparkSession that connects to Spark in local mode.  Configure the SparkSession to use two cores.   \n",
    "`spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"connect_solutions\") \\\n",
    "    .getOrCreate()`\n",
    "\n",
    "2. Create a small dataframe   \n",
    "`courses = spark \\  \n",
    "    .createDataFrame([(\"Linear Algebra\", \"Mathematics\", 3302), (\"Calculus III\", \"Mathematics\", 2302), \\\n",
    "    (\"Business Statistics\", \"Business\", 2305)], \\  \n",
    "    schema = [\"CourseName\", \"Department\", \"CourseID\"])`  \n",
    "\n",
    "3. Print the schema of the dataframe  \n",
    "`courses.printSchema()`\n",
    "\n",
    "4. View the dataframe  \n",
    "`courses.show()`\n",
    "\n",
    "5. Count the number of records  \n",
    "`courses.count()`\n",
    "\n",
    "7. Stop the SparkSession     \n",
    "`spark.stop()`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.03_Inspect\n",
    "\n",
    "1. Read the 311 case data into a Spark DataFrame.\n",
    "\n",
    "> `df2 = spark.read.csv(\"/sa311/case.csv\", header=True, inferSchema=True)`\n",
    "\n",
    "2. Inspect the DataFrame.  Are the data types for each column appropriate?\n",
    "\n",
    "> `df2.dtypes\n",
    "df2.columns\n",
    "df2.schema\n",
    "df2.count()\n",
    "len(df2.columns)\n",
    "df2.describe().show()\n",
    "pd.options.display.html.table_schema = True\n",
    "df2.describe().toPandas()`\n",
    "\n",
    "3. Inspect various columns of the driver DataFrame.  Are there any issues with the data?\n",
    "\n",
    "> `df2.createOrReplaceTempView(\"df2_temp\")\n",
    "spark.sql(\"select cat, count(*) from df2_temp group by cat\").show()`\n",
    "\n",
    "- Here's a routine to inspect each column in turn:\n",
    "\n",
    "> `def inspect_dataframe(df):\n",
    "  print(\"====\")\n",
    "  print(\"====\")\n",
    "  print(\"INSPECT DATAFRAME\")\n",
    "  print(\"====\")\n",
    "  df.printSchema()\n",
    "  from pyspark.sql.functions import count, countDistinct\n",
    "  for c in df.columns:\n",
    "    print(\"====\")\n",
    "    print(\"Inspecting column: \" + c)\n",
    "    print(\"====\")\n",
    "    df.select(c).printSchema()\n",
    "    df.select(c).show(5)\n",
    "    df.select(c).describe().show()\n",
    "    df.select(count(\"*\").alias(\"N\"), count(c), countDistinct(c)).show()`\n",
    "\n",
    "- Begin column notes.  Copy and paste output into a script to begin report.\n",
    "  \n",
    "> `print(\"\")\n",
    "  print(\"# ## Observations:\")\n",
    "  for c in df.columns:\n",
    "    print(\"# * \" + c + \":\")`\n",
    "    \n",
    "- Invoke the routine:\n",
    "\n",
    "> `inspect_dataframe(df2)`\n",
    "    \n",
    "- Observations:\n",
    "\n",
    "- Another routine, just to consider categorical variables:\n",
    "\n",
    "> `def inspect_categorical_variables(df, column_list):\n",
    "  print(\"====\")\n",
    "  print(\"====\")\n",
    "  print(\"INSPECT CATEGORICAL VARIABLES\")\n",
    "  print(\"====\")\n",
    "  for c in column_list:\n",
    "    print(\"====\")\n",
    "    print(\"Inspecting column: \" + c)\n",
    "    print(\"====\")\n",
    "    n = df.select(c).distinct().count()\n",
    "    print(\"Number of distinct values: \" + str(n))\n",
    "    df.groupBy(c).count().orderBy(c).show(20)`\n",
    "    \n",
    "- Begin column notes.  Copy and paste output into a script to begin report.\n",
    "  \n",
    "> `print(\"\")\n",
    "  print(\"# ## Observations:\")\n",
    "  for c in column_list:\n",
    "    print(\"# * \" + c + \":\")`\n",
    "        \n",
    "- Invoke the routine with variables of interest:    \n",
    "\n",
    "> `df2 =\\\n",
    "  [\"case_late\", \"case_closed\", \"dept_division\", \"service_request_type\", \"case_status\"]\n",
    "inspect_categorical_variables(df2, df2_categorical_columns)    \n",
    "\n",
    "- Observations:\n",
    "\n",
    "> - Further inspection w/ `show(200).orderBy(\"count\")` shows many values with just one instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.04.01_Perpare_part1\n",
    "\n",
    "\n",
    "1. Read the raw data 2 into a Spark DataFrame.\n",
    "\n",
    "> `data = \"sa311/case.csv\"\n",
    "df2 = spark.read.csv(data, header=True, inferSchema=True)\n",
    "df2.printSchema()\n",
    "df2.dtypes\n",
    "df2.columns\n",
    "len(df2.columns)\n",
    "df2.schema\n",
    "df2.count()`\n",
    "\n",
    "> `import pandas as pd\n",
    "pd.options.display.html.table_schema = True\n",
    "df2.limit(5).toPandas()`\n",
    "\n",
    "2. How old is the latest (in terms of days past SLA) currently open issue?  How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "\n",
    "- Latest (SLA) Issue:  \n",
    "\n",
    "> `from pyspark.sql.functions import datediff, current_timestamp\n",
    "df2 \\\n",
    "  .select('case_opened_date', 'case_late', 'case_closed', 'num_days_late') \\\n",
    "  .filter(df.case_late == \"YES\").where(df.case_closed == \"NO\") \\\n",
    "  .orderBy(df2.num_days_late.desc()) \\\n",
    "  .limit(1) \\\n",
    "  .withColumn( \\\n",
    "              \"age_in_days\" \\\n",
    "              , datediff(current_timestamp() \\\n",
    "                         , col(\"case_opened_date\"))/365 \\\n",
    "              ) \\\n",
    "  .show(1)`\n",
    "  \n",
    "- Oldest Issue: \n",
    "\n",
    "> `df2 \\\n",
    "  .select('case_opened_date', 'case_closed') \\\n",
    "  .filter(df.case_closed==\"NO\") \\\n",
    "  .orderBy(df2.case_opened_date.asc()) \\\n",
    "  .limit(1) \\\n",
    "  .withColumn( \\\n",
    "              \"age_in_days\" \\\n",
    "              , datediff(current_timestamp() \\\n",
    "                         , col(\"case_opened_date\"))/365 \\\n",
    "              ) \\\n",
    "  .show(1)`\n",
    "  \n",
    "  \n",
    "3. How many Stray Animal cases are there?\n",
    "\n",
    "> `df2 \\\n",
    "  .select('service_request_type') \\\n",
    "  .filter(df2.service_request_type == 'Stray Animal') \\\n",
    "  .count()`\n",
    "\n",
    "4. How many service requests that are assigned to the Field Operations department (dept_division) are not classified as \"Officer Standby\" request type (service_request_type)? \n",
    "\n",
    "> `df2 \\\n",
    "  .select('dept_division', 'service_request_type') \\\n",
    "  .filter(df2.dept_division == 'Field Operations') \\\n",
    "  .where(df2.service_request_type != 'Officer Standby') \\\n",
    "  .count()`\n",
    "\n",
    "4. Create a new DataFrame without any information related to dates or location. \n",
    "\n",
    "> `df2.drop('case_opened_date', 'case_closed_date', 'SLA_due_date', 'request_address', 'council_district').show(25)`\n",
    "\n",
    "5. Read dept.csv into a Spark DataFrame.  Inspect the `dept_name` column. Replace the missing values with \"other\" for a standard filler. \n",
    "\n",
    "> `data = \"/sa311/dept.csv\"\n",
    "> df3 = spark.read.csv(data, header=True, inferSchema=True)\n",
    "df3_fixed = df3.fillna(\"other\", \"dept_name\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.04.02_Prepare_part2\n",
    "\n",
    "\n",
    "> `from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"transform2_answer\").getOrCreate()\n",
    "data = \"/sa311/case\"\n",
    "df2 = spark.read.csv(data, header=True, inferSchema=True)`\n",
    "\n",
    "\n",
    "1. Convert the `df.council_district` column to a string column.\n",
    "\n",
    "> `from pyspark.sql.functions import format_string\n",
    "df = df.withColumn(\"council_district\", format_string(\"%012d\", \"council_district\"))`\n",
    "\n",
    "2. Extract the year from the `df.case_closed_date` column.\n",
    "\n",
    "> `from pyspark.sql.functions import year\n",
    "df = df.withColumn(\"case_closed_year\", year(\"case_closed_date\"))`\n",
    "\n",
    "3. Convert `df.num_days_late` from days to hours in new columns `df.num_hours_late`.\n",
    "\n",
    "> `from pyspark.sql.functions import col, round\n",
    "df = df.withColumn(\"num_hours_late\", round(col(\"num_days_late\")*24, 0))`\n",
    "\n",
    "4. Convert the `df.case_late` column to a boolean column.\n",
    "\n",
    "> `from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"df.case_late\", col(\"df.case_late\")==\"YES\")`\n",
    "\n",
    "5. Convert the `df.SLA_days` columns to a double column.\n",
    "\n",
    "> `from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"SLA_days\", col(\"SLA_days\") * 1.0)`\n",
    "\n",
    "\n",
    "- All together: \n",
    "\n",
    "> `from pyspark.sql.functions import format_string, year, col, round\n",
    "df = df\\\n",
    "  .withColumn(\"council_district\", format_string(\"%012d\", \"council_district\"))\\\n",
    "  .withColumn(\"case_closed_year\", year(\"case_closed_date\"))\\\n",
    "  .withColumn(\"num_hours_late\", round(col(\"num_days_late\")*24, 0))\\\n",
    "  .withColumn(\"case_late\", col(\"case_late\")==\"YES\")\\\n",
    "  .withColumn(\"SLA_days\", col(\"SLA_days\") * 1.0)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.04.03_Prepare_part3\n",
    "\n",
    "\n",
    "1. Create a DataFrame with all combinations of council_district and service_request_type (regardless of whether the combination is observed in the data).\n",
    "\n",
    "- Generate a DataFrame with all council_district:\n",
    "\n",
    "> `council_district_df = df.select(\"council_district\").distinct()`\n",
    "  \n",
    "- Generate a DataFrame with all service_request_type:\n",
    "\n",
    "> `service_request_type_df = df.select(\"service_request_type\").distinct()`\n",
    "\n",
    "- Use the `crossJoin` method to generate all combinations:\n",
    "\n",
    "> `combinations = council_district_df.crossJoin(service_request_type_df)\n",
    "combinations.orderBy(\"council_district\", \"service_request_type\").show()`\n",
    "\n",
    "- This is not bulletproof. It would not give the desired result if a service_request_type was missing in the middle of the list.\n",
    "\n",
    "2. Join the case data with the source and department data.\n",
    "\n",
    "- Since we want all the case data, we will use a sequence of left outer joins:\n",
    "\n",
    "> `data3 = \"/sa311/source\"\n",
    "df3 = spark.read.csv(data2, header=True, inferSchema=True)\n",
    "df3.show()` \n",
    "\n",
    "> `joined = df \\\n",
    "      .join(df2, df.dept_division == df2.dept_division, \"left_outer\") \\\n",
    "      .join(df3, df.source_id == df3.source_id, \"left_outer\")\n",
    "joined.printSchema()`\n",
    "\n",
    "- We might want to rename some columns before joining the data and remove the duplicate ID columns after joining the data to make this DataFrame more usable.  \n",
    "\n",
    "3. Are there any cases who have not a request source?\n",
    "\n",
    "- A solution using joins:\n",
    "\n",
    "> `case_df_no_source = df.join(df3, df.source_id == df3.source_id, \"left_anti\")\n",
    "case_df_no_source.count()\n",
    "case_df_no_source.select(\"source_id\").orderBy(\"source_id\").show()`\n",
    "\n",
    "- A solution using set operations:\n",
    "\n",
    "> `case_df_no_source2 = df.select(\"source_id\").subtract(df3.select(\"source_id\"))\n",
    "case_df_no_source2.count()\n",
    "case_df_no_source2.orderBy(\"source_id\").show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.04.04_Prepare_part4\n",
    "\n",
    "\n",
    "1. Who are the top 10 service request types in terms of number of requests?\n",
    "\n",
    "> `from pyspark.sql.functions import count\n",
    "df.groupBy(\"service_request_type\")\\\n",
    "  .agg(count(\"case_id\").alias(\"count\"))\\\n",
    "  .orderBy(\"count\", ascending=False)\\\n",
    "  .show(10)`\n",
    "\n",
    "2. Who are the top 10 service request types in terms of average days late?\n",
    "\n",
    "> `from pyspark.sql.functions import sum\n",
    "df.groupBy(\"service_request_type\")\\\n",
    "  .agg(avg(\"num_days_late\").alias(\"average_days_late\"))\\\n",
    "  .orderBy(\"average_days_late\", ascending=False)\\\n",
    "  .show(10)`\n",
    "\n",
    "3. Does number of days late depend on department?\n",
    "\n",
    "> `from pyspark.sql.functions import avg, format_number\n",
    "df.groupBy(\"dept_division\")\\\n",
    "  .agg(avg(\"num_days_late\").alias(\"avg_days_late\"))\\\n",
    "  .withColumn(\"avg_days_late\", format_number(\"avg_days_late\", 2))\\\n",
    "  .orderBy(\"avg_days_late\")\\\n",
    "  .show(30)`\n",
    "\n",
    "4. How do number of days late depend on department division and request type?\n",
    "\n",
    "> `df1.groupBy(\"service_request_type\", \"dept_division\")\\\n",
    "  .agg(avg(\"num_days_late\"))\\\n",
    "  .orderBy(\"service_request_type\", \"dept_division\")\\\n",
    "  .show()`\n",
    "  \n",
    "- or:\n",
    "\n",
    "> `df1.groupBy(\"service_request_type\")\\\n",
    "  .pivot(\"dept_division\")\\\n",
    "  .agg(avg(\"num_days_late\").alias(\"avg_days_late\"))\\\n",
    "  .show()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.05_Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.06.01_Model_TopicModeling\n",
    "\n",
    "0. Prep:  Load the twitter data, which includes the following attributes: \"Topic\",\"Sentiment\",\"TweetId\",\"TweetDate\",\"TweetText\"\n",
    "\n",
    "> `data = \"twitter_corpus.csv\"\n",
    "df = spark.read.parquet(data)\n",
    "df.head(5)`\n",
    "\n",
    "\n",
    "\n",
    "1. Use the `NGram` transformer to generate pairs of words (bigrams) from the tokenized tweets.\n",
    "\n",
    "- Import the `NGram` class from the `pyspark.ml.feature` module:\n",
    "\n",
    "> `from pyspark.ml.feature import NGram`\n",
    "\n",
    "- Create an instance of the `NGram` class:\n",
    "\n",
    "> `ngramer = NGram(inputCol=\"words\", outputCol=\"bigrams\", n=2)`\n",
    "\n",
    "- Use the `transform` method to apply the `NGram` instance to the `tokenized` DataFrame:\n",
    "\n",
    "> `from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"TweetText\", outputCol=\"words\", gaps=False, pattern=\"[a-zA-Z-']+\")\n",
    "tokenized = tokenizer.transform(df)`\n",
    "\n",
    "> `from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_removed\")\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"words\", \"words_removed\").head(5)`\n",
    "\n",
    "> `ngramed = ngramer.transform(removed)\n",
    "ngramed2 = ngramer.transform(tokenized)`\n",
    "\n",
    "- Print out a few rows of the transformed DataFrame:\n",
    "\n",
    "> `ngramed.show(5)`\n",
    "\n",
    "2. Fit an LDA model with $k=3$ topics.\n",
    "\n",
    "- Use the `setK` method to change the number of topics for the `lda` instance:\n",
    "\n",
    "> `lda.setK(3)`\n",
    "\n",
    "- Use the `fit` method to fit the LDA model to the `vectorized` DataFrame:\n",
    "\n",
    "> `lda_model = lda.fit(vectorized)`\n",
    "\n",
    "- Use the `print_topics` function to examine the topics:\n",
    "\n",
    "> `print_topics(lda_model, 5, vectorizer_model.vocabulary)`\n",
    "\n",
    "- Use the `transform` method to apply the LDA model to the `vectorized` DataFrame:\n",
    "\n",
    "> `predictions = lda_model.transform(vectorized)`\n",
    "\n",
    "- Print out a few rows of the transformed DataFrame:\n",
    "\n",
    "> `predictions.select(\"TweetText\", \"topicDistribution\").head(5)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.06.02_Classification\n",
    "\n",
    "In the exercises we add another feature to the classification model and\n",
    "determine if it improves the model performance.\n",
    "\n",
    "1. Determine if `request_address_zip` is a promising feature.\n",
    "\n",
    "2. Reassemble the feature vector and include `request_address_zip`.\n",
    "\n",
    "3. Create new train and test datasets.\n",
    "\n",
    "> `(train, test) = assembled.randomSplit([0.7, 0.3], 23451)`\n",
    "\n",
    "4. Refit the logistic regression model on the train dataset.\n",
    "\n",
    "> `log_reg_model = log_reg.fit(train)`\n",
    "\n",
    "5. Apply the refit logistic model to the test dataset.\n",
    "\n",
    "> `predictions = log_reg_model.transform(test)`\n",
    "\n",
    "6. Compute the AUC on the test dataset.\n",
    "\n",
    " `evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
