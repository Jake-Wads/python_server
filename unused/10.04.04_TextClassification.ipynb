{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification & bottom-up NLP\n",
    "\n",
    "Look for statistical patterns in word use.\n",
    "\n",
    "##### Use cases\n",
    "1. spam: detect an email as spam or not\n",
    "2. user ratings: classifying rating based on text reviews\n",
    "3. sentiment analysis: classify negative, positive or neutral sentiment\n",
    "4. abusive or obscene comments and flag it\n",
    "5. route support tickets by classifying issue topic\n",
    "6. sort and label documents\n",
    "7. layered classificaton models: first classify sentiment then topic, e.g.\n",
    "\n",
    "##### Pros & Cons\n",
    "+ adapts to unique forms/words to communicated because looking through statistical relationships between input phrases and outputs\n",
    "+ fast\n",
    "- not as detailed as the top-down approach\n",
    "\n",
    "##### How\n",
    "fasttext python library from Facebook\n",
    "Steps:\n",
    "1. Format _label_YOURLABEL\n",
    "2. text normalization:  lowercase, spaces before punctuation, removing punctuation, stemming, acronyms, abbreviations, stopwords\n",
    "3. split into test/training set (randomize order!)\n",
    "4. train the model\n",
    "5. test the model\n",
    "6. predict using fasttext\n",
    "7. iterate:  n-grams (wordNgrams), bigrams (2-word word pairs)\n",
    "8. Deploy/Implement model\n",
    "\n",
    "https://medium.com/@ageitgey/text-classification-is-your-new-secret-weapon-7ca4fad15788\n",
    "\n",
    "#### Step 1:  To build a user review model, we need training data\n",
    "Yelp provides a research dataset of 4.7 million user reviews. You can download it here (but keep in mind that you can’t use this data to build commercial applications).\n",
    "\n",
    "When you download the data, you’ll get a 4 gigabyte json file called reviews.json. Each line in the file is a json object with data like this:\n",
    "\n",
    "{\n",
    "  \"review_id\": \"abc123\",\n",
    "  \"user_id\": \"xyy123\",\n",
    "  \"business_id\": \"1234\",\n",
    "  \"stars\": 5,\n",
    "  \"date\":\" 2015-01-01\",\n",
    "  \"text\": \"This restaurant is great!\",\n",
    "  \"useful\":0,\n",
    "  \"funny\":0,\n",
    "  \"cool\":0\n",
    "}\n",
    "\n",
    "#### Step 2: Format and Pre-process Training Data\n",
    "The first step is to convert this file into the format that fastText expects.  \n",
    "\n",
    "fastText requires a text file with each piece of text on a line by itself. The beginning of each line needs to have a special prefix of __label__YOURLABEL that assigns the label to that piece of text.  \n",
    "\n",
    "In other words, our restaurant review data needs to be reformatted like this:  \n",
    "\n",
    "__label__5 This restaurant is great!  \n",
    "__label__1 This restaurant is terrible :'(  \n",
    "\n",
    "\n",
    "read the reviews.json file and write out a text file in fastText format  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/review.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dda1b7989c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfasttext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fasttext_dataset.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mreviews_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfasttext_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mreview_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         return io.open(str(self), mode, buffering, encoding, errors, newline,\n\u001b[0;32m-> 1181\u001b[0;31m                        opener=self._opener)\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/review.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "reviews_data = Path(\"dataset\") / \"review.json\"\n",
    "fasttext_data = Path(\"fasttext_dataset.txt\")\n",
    "\n",
    "with reviews_data.open() as input, fasttext_data.open(\"w\") as output:\n",
    "    for line in input:\n",
    "        review_data = json.loads(line)\n",
    "\n",
    "        rating = review_data['stars']\n",
    "        text = review_data['text'].replace(\"\\n\", \" \")\n",
    "\n",
    "        fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "        output.write(fasttext_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is totally oblivious to any English language conventions (or the conventions of any other language). As far is it knows, the words Hello, hello and hello! are all totally different words because they aren’t exactly the same characters. To fix this, we want to do a quick pass through our text to convert everything to lowercase and to put spaces before punctuation marks. This is called text normalization and it makes it a lot easier for fastText to pick up on statistical patterns in the data.\n",
    "\n",
    "This means that the textThis restaurant is great! should becomethis restaurant is great !.\n",
    "\n",
    "Here’s a simple Python function that we can add to our code to do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split the data into a Training set and a Test set\n",
    "To get an accurate measure of how well our model performs, we need to test it’s ability to classify text using text that it didn’t see during training. If we test it against the training data, it is like giving it an open book test where it can memorize the answers.\n",
    "\n",
    "So we need to extract some of the strings from the training data set and keep them in separate test data file. Then we can test the trained model’s performance with that held-back data to get a real-world measure of how well the model performs.\n",
    "\n",
    "Here’s a final version of our data parsing code that reads the Yelp dataset, removes any string formatting and writes out separate training and test files. It randomly splits out 90% of the data as test data and 10% as test data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "\n",
    "reviews_data = Path(\"dataset\") / \"review.json\"\n",
    "training_data = Path(\"fasttext_dataset_training.txt\")\n",
    "test_data = Path(\"fasttext_dataset_test.txt\")\n",
    "\n",
    "# What percent of data to save separately as test data\n",
    "percent_test_data = 0.10\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "with reviews_data.open() as input, \\\n",
    "     training_data.open(\"w\") as train_output, \\\n",
    "     test_data.open(\"w\") as test_output:\n",
    "\n",
    "    for line in input:\n",
    "        review_data = json.loads(line)\n",
    "\n",
    "        rating = review_data['stars']\n",
    "        text = review_data['text'].replace(\"\\n\", \" \")\n",
    "        text = strip_formatting(text)\n",
    "\n",
    "        fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "        if random.random() <= percent_test_data:\n",
    "            test_output.write(fasttext_line + \"\\n\")\n",
    "        else:\n",
    "            train_output.write(fasttext_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run that and you’ll have two files,fasttext_dataset_training.txt and fasttext_dataset_test.txt. Now we are ready to train!\n",
    "\n",
    "Here’s one more tip though: To make your model robust, you will also want to randomize the order of lines in each data file so that the order of the training data doesn’t influence the training process. That’s not absolutely required in this case since the data from Yelp is already pretty random, but it’s definitely worth doing when using your own data.\n",
    "\n",
    "#### Step 4: Train the Model\n",
    "You can train a classifier using the fastText command line tool. You just call fasttext, pass in the supervised keyword to tell it train a supervised classification model, and then give it the training file and and an output name for the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext supervised -input fasttext_dataset_training.txt -output reviews_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only took 3 minutes to train this model with 580 million words on my laptop. Not bad!\n",
    "\n",
    "#### Step 5: Test the Model\n",
    "Let’s see how accurate the model is by checking it against our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext test reviews_model.bin fasttext_dataset_test.txt\n",
    "\n",
    "#N N 474292\n",
    "# P@1 0.678\n",
    "# R@1 0.678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that across 474,292 examples, it guessed the user’s exact star rating 67.8% of the time. Not a bad start.\n",
    "\n",
    "You can also ask fastText to check how often the correct star rating was in one of it’s Top 2 predictions (i.e. if the model’s top two most likely guesses were “5”, “4” and the real user said “4”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext test reviews_model.bin fasttext_dataset_test.txt 2\n",
    "# N 474292\n",
    "# P@2 0.456\n",
    "# R@2 0.912"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that 91.2% of the time, it recalled the user’s star rating if we check its two best guesses. That’s a good indication that the model is not far off in most cases.\n",
    "\n",
    "You can also try out the model interactively by running the fasttext predict command and then typing in your own reviews. When you hit enter, it will tell you its prediction for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext predict reviews_model.bin -\n",
    "this is a terrible restaurant . i hate it so much .\n",
    "# __label__1\n",
    "this is a very good restaurant .\n",
    "# __label__4\n",
    "this is the best restaurant i have ever tried .\n",
    "# __label__5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Iterate on the model to make it more accurate\n",
    "With the default training settings, fastText tracks each word independently and doesn’t care at all about word order. But when you have a large training data set, you can ask it to take the order of words into consideration by using the wordNgrams parameter. That will make it track groups of words instead of just individual words.\n",
    "\n",
    "For a data set of millions of words, tracking two word pairs (also called bigrams) instead of single words is a good starting point for improving the model.\n",
    "\n",
    "Let’s train a new model with the -wordNgrams 2 parameter and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext supervised -input fasttext_dataset_training.txt -output reviews_model_ngrams -wordNgrams 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make training take a bit longer and it will make the model file much larger (since there is now an entry for every two-word pair in the data), but it can be worth it if it gives us higher accuracy.\n",
    "\n",
    "Once the training completes, you can re-run the test command the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext test reviews_model_ngrams.bin fasttext_dataset_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me, using -wordNgrams 2 got me to 71.2% accuracy on the test set, an improvement of nearly 4%. It also seems to reduce the number of obvious errors that the model makes because now it cares a little bit about the context of each word.\n",
    "\n",
    "There are other ways to improve your model, too. One of the simplest but most effective ways is skim your training data file by hand and make sure that the preprocessing code is formatting your text in a sane way.\n",
    "\n",
    "For example, my sample text pre-processing code will turn the common restaurant nameP.F. Chang into p . f . chang. That appears as five separate words to fastText.\n",
    "\n",
    "If you have cases like that where important words that represent a single concept are getting split up, you can write custom code to fix it. In this case, you might add code to look for common restaurant names and replace them with placeholders like p_f_chang so that fastText sees each as a single word.\n",
    "\n",
    "#### Step 7: Use your model in your program!\n",
    "The best part about fastText is that it’s easy to call a trained model from any Python program.\n",
    "\n",
    "There are a few different Python wrappers for fastText that you can use, but I like the official one created by Facebook. You can install it by following these directions.\n",
    "\n",
    "With that installed, here’s the entire code to load the model and use it to automatically score user reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import re\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "# Reviews to check\n",
    "reviews = [\n",
    "    \"This restaurant literally changed my life. This is the best food I've ever eaten!\",\n",
    "    \"I hate this place so much. They were mean to me.\",\n",
    "    \"I don't know. It was ok, I guess. Not really sure what to say.\"\n",
    "]\n",
    "\n",
    "# Pre-process the text of each review so it matches the training format\n",
    "preprocessed_reviews = list(map(strip_formatting, reviews))\n",
    "\n",
    "# Load the model\n",
    "classifier = fastText.load_model('reviews_model_ngrams.bin')\n",
    "\n",
    "# Get fastText to classify each review with the model\n",
    "labels, probabilities = classifier.predict(preprocessed_reviews, 1)\n",
    "\n",
    "# Print the results\n",
    "for review, label, probability in zip(reviews, labels, probabilities):\n",
    "    stars = int(label[0][-1])\n",
    "\n",
    "    print(\"{} ({}% confidence)\".format(\"☆\" * stars, int(probability[0] * 100)))\n",
    "    print(review)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s what it looks like when it runs:\n",
    "\n",
    "☆☆☆☆☆ (100% confidence)\n",
    "This restaurant literally changed my life. This is the best food I've ever eaten!\n",
    "☆ (88% confidence)\n",
    "I hate this place so much. They were mean to me.\n",
    "☆☆☆ (64% confidence)\n",
    "I don't know. It was ok, I guess. Not really sure what to say.\n",
    "Those are really good prediction results! And let’s see what prediction it would give my Yelp review:\n",
    "\n",
    "☆☆☆☆☆ (58% confidence)\n",
    "This used to be a giant parking lot where government employees that worked in the country building would park. They moved all the parking underground and built an awesome park here instead. It's literally the reverse of the Joni Mitchell song.\n",
    "Perfect!\n",
    "\n",
    "This is why machine learning is so cool. Once we figured out a good way to pose the problem, the algorithm did all the hard work of extracting meaning from the training data. You can then call that model from your code with just a couple of lines of code. And just like that, your program seemingly gains superpowers.\n",
    "\n",
    "Now go out and build you own text classifier!\n",
    "https://medium.com/@ageitgey/text-classification-is-your-new-secret-weapon-7ca4fad15788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
