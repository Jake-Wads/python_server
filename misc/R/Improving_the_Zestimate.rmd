---
title: "Improving the Zestimate"
author: "Maggie Giust"
date: "9/10/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

### Project Preparation
**GOALS** Summary, Goals, Questions & hypotheses documented, execution plan outlined

#### Zillow's Home Value Prediction, aka Zestimate

**From Zillow:** "The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost....'Zestimates' are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning....In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition."

#### Goals
Our goal in this 1st round (beyond advancing to the 2nd round), is to best predict, given the data provided & time available, the difference between the Zestimate and the Sales Price, i.e. the error.  We want to predict how on or how off the Zestimate actually is. (Technically, we will predicting the logerror, so if the error = Zestimate - SalePrice, the logerror=log(Zestimate)-log(SalePrice)).   We will submit our predictions for each property ID for October, November & December for 2016 and 2017 (i.e. 6 different time periods).  We will evaluate performance of our predictions using Mean Absolute Error.  

#### Questions
Can the existing Zestimates be improved?  How can we know for sure?  If we can predict errors of the existing predictions, then we know there is a pattern to the errors, and, if there is a pattern to the errors, then there is something the existing model isn't capitalizing on, and, if there is something not be used, then we can use it to make the model better!
If you think about regression modeling...you know you don't have an optimal model when you can see a pattern in the plotting of the residuals.  

____________________________________________________

### A. Data Engineering
**GOAL** Refine data to be useful in modeling
1. Data Acquisition
2. Data Preparation
3. Data Represenationa & Exploration

#### 1. Data Acquisition
**GOAL** Acquire data needed to achieve goals and document the metadata, i.e. the data aobut the data. 

- Sources could be SQL, a website through HTML or API, flat file (like csv or excel), big data store (like hadoop, AWS-S3), NoSQL database, image files or audio/video files  
- Also known as: Data gathering, data import  

#### 2. Data Preparation
**GOAL** Get the data all in one place and in a workable format
- Putting the data in a structure, like a data frame, that can be used for preliminary analysis.
- Ideally, each column should be a variable, each row an observation.
- Outliers may need to be removed here, data normalized, and missing values addressed.
- Also known as: Tidying the data, data cleansing

#### 3. Data Representation & Data Exploration

**GOAL** Understand the signals in the data, their strength, the features that drive the outcome, and other features to construct, and transform variables into features
- Convert data to the most optimal data types for storing and modeling 
- Narrow in on observations of interest
- Creating new variables that are functions of existing variables
- Also known as: Exploratory Visualization, Exploratory Analysis, Data Transformation

____________________________________________________


### B. Data Modeling
**GOAL** Turn the data prepared in the engineering phase into a prediction or valuable insight.
1. Data Discovery
2. Data Modeling

#### 1. Data Discovery
**GOAL** Walk away with features that you now know drive the behavior of the data you are targeting  
- Find patterns and potential insights in the data
- Define how the features can be used to build a model
- Identify the signals in the dataset that are of greater value
- Identify interdependence or correlation among variables
- Create synthetic features
- Filter out redundant variables and extract the most valuable.
- Also known as: Feature Engineering

#### 2. Data Learning
**GOAL** Create a robust & generalizable model that is a mapping between features (inputs/independent variables) and a target variable (output/dependent variable) based on discoveries and transformations made in previous steps.
- Select meta-parameters
- Sampling the data (training/testing)
- Train the model
- Selecting algorithms
- Evaluating results
- Test the model 

____________________________________________________


### C. Information Distillation
**GOAL**  Summarize the previous stages and making it available to you customers/end users
1. Insight, delivery, visualization
2. Data product creation

#### 1. Insight, Delivery, Visualization
**GOAL** Deliver the work in a consumable way to the stakeholders of the project so that the goals can be met.  
- Summarize insights into actionable items
- Advocate for your findings and recommendations through visualizations
- Deliver the model, the report and/or other data product to the stakeholders.
- This is the most critical step in order to make use of all the work done to this point. 

#### 2. Data Product Creation
**GOAL** Enable a machine learning model to be consumed through an interface by an end user  
- This is about deploying a model to production, or productionizing the model, so that it is live and consumable in the desired manner
- This is usually done through an API
- The web application will take data input by the user, and the model will take that data, extract/compute the features, feed them to the model which will generate results.  
- These results will then be fed back to the web application for it to respond accordingly, such as display a particular product for the end user. 


____________________________________________________
____________________________________________________

### A. Data Engineering
**GOAL** Refine data to be useful in modeling
1. Data Acquisition
2. Data Preparation
3. Data Represenationa & Exploration

____________________________________________________

#### 1. Data Acquisition
**GOAL** Acquire data needed to achieve goals and document the metadata, i.e. the data aobut the data. 

- Sources could be SQL, a website through HTML or API, flat file (like csv or excel), big data store (like hadoop, AWS-S3), NoSQL database, image files or audio/video files  
- Also known as: Data gathering, data import  

##### a. Metadata - Datasets
**data about the datasets**

We are provided with a list of about 2.9 million real estate properties in three counties (Los Angeles, Orange and Ventura, California).  
There are 4 files:  
  1. properties_2016.csv contains information about the properties themselves.  This is where we will find our potential "features" or "independent variables".  
  2. train_2016_v2.csv containing information about the transcations, specifically the logerror and the transaction date.  This is where we will find our "dependent variable" or "target".  
  3. sample_submission.csv which is what we will populate with our predictions.   
Let's read the data into R and see what it looks like.
  4. zillow_data_dictionary which is a workbook with a spreadsheet containing a list of the fields, and other spreadsheets containing mapping from IDs to names/descriptioins.

##### b. Acquire Sampmle
Read a sample of data into R (say, 10k rows) to get the order of the columns and the data types assigned by default

```{r IMPORT_part1, warning=FALSE, message=FALSE}
library(tidyverse)
sample <- read_csv("properties_2016.csv", n_max=200000)
spec(sample)
```

##### c. Metadata - Fields
**data about the fields**

View documentation of field definitions, customize the names to be more useful (in zillow_data_dictionary.xlsx, I added a new column in A "mycolnames" where I adjusted the names), and create a vector of new column names in the order of the table "sample" through the following steps:  
  a. In excel, I created column D and filled it all the way down with "  
  b. I then created column E, names it "mycolnames vector", and in E2 inserted the formula: =D2&A2&D2
  b. In E3, inserted the formula: =E2&","&D3&A3&D3 which I then copied that formula and pasted it all the down to the last cell (E59).  
  c. I copied E59 and pasted the value (paste special) into E60.  I then copied that cell and pasted it below.  
  d. Through manual comparison of the output of spec(sample) and the list of new column names, I re-ordered my list to match the order of the columns in the dataset.  This required referring back to the data dictionary a couple of times to make sure I was mapping the correct fields together.  The correct order is what you now see below.  

```{r IMPORT_part2, warning=FALSE, message=FALSE}
mycolnames <- c("parcelid","typeAC","typeArchitecture","sqftBasement","numBath","numBed","typeBuilidngFrame","rankBuildingCondition","numBath1","typeDeck","sqftFinishedFirstFloor","sqftFinishedAll","sqftFinishedAll1","sqftFinishedPerim","sqftFinishedAll2","sqftFinishedFirstFloor1","sqftAll","codeFIPS","numFireplace","numBathFull","numGarage","sqftGarage","boolHottub","typeHeating","locLatitude","locLongitude","sqftLot","numPool","sqftPool","boolHottub1","boolPoolHotub","boolPoolNoHottub","typePropertyUseCounty","typePropertyUseID","typePropertyUseDesc","locCensusTractBlockRaw","locCity","locCounty","locNeighborhood","locZip","numRoom","typeStory","numBath3Q","typeConstruction","numUnits","typePatio","typeShed","yearBuilt","numStories","boolFireplace","valueAssessedStructure","valueAssessedAll","yearTax","valueAssessedLand","valueTax","boolTaxPastDue","yearTaxPastDue","locCensusTractBlock")
sample <- read_csv("properties_2016.csv", col_names=mycolnames, skip=1, n_max=100000)
```

##### d. Aquire dataset of all independent variables 

```{r IMPORT_part2c, warning=FALSE, message=FALSE}
properties <- read_csv("properties_2016.csv", col_names = mycolnames, skip=1)
```

##### e. Create reference tables from the data dictionary ID tabs  
in excel: save each tab as a separate csv so that you can then read into R

```{r IMPORT_part3, warning=FALSE, message=FALSE}
acType <- read_csv("acType.csv", col_names=c("typeAC","nameAC"), skip=1)
archType <- read_csv("archType.csv", col_names=c("typeArchitecture","nameArchitecture"), skip=1)
buildingClass <- read_csv("buildingClass.csv", col_names=c("typeBuilidngFrame","nameBuildingFrame"), skip=1)
constructionType <- read_csv("constructionType.csv",
                             col_names=c("typeConstruction","nameConstructionType"), skip=1)
heatingType <- read_csv("heatingType.csv", col_names=c("typeHeating","nameHeatingType"), skip=1)
propertyType <- read_csv("propertyType.csv", col_names=c("typeProperty","namePropertyType"), skip=1)
storyType <- read_csv("storyType.csv", col_names=c("typeStory","nameStoryType"), skip=1)
```

##### f. Acquire dataset of dependent/target variable

```{r IMPORT_part4, warning=FALSE, message=FALSE}
labeled <- read_csv("train_2016_v2.csv")
```

____________________________________________________

#### 2. Data Preparation
**GOAL** Get the data all in one place and in a workable format
- Putting the data in a structure, like a data frame, that can be used for preliminary analysis.
- Ideally, each column should be a variable, each row an observation.
- Outliers may need to be removed here, data normalized, and missing values addressed.
- Also known as: Tidying the data, data cleansing

##### a. Null Values & Cleaning

First, I will view summary() of the table.

```{r tidy_part1a, warning=FALSE, message=FALSE}
summary(properties)
```

I see that 11,437 do not have values for latitude, longitude and other fields, so I look at those and see if there are any values in other columns. 

```{r tidy_part1b}
# View(head(properties[is.na(properties$locLatitude),]))
# head(properties[is.na(properties$locLatitude),])
summary(properties[is.na(properties$locLatitude),])
```

As I suspected, the only value is the parcelId.  I will filter these out for now.  

```{r tidy_part1c}
naprops <- properties[is.na(properties$locLatitude),]
myprops <- properties[!is.na(properties$locLatitude),]
remove(properties)
```

I will also separate the properties that are land only from those with residential property, as that will affect how I handle NA's.  First, I look at propertyType data frame see property types of 290 and 291 are land only.  Next, I need to identify which field holds these id's in the myprops data frame.  I see that typePropertyUseID is related to these IDs and that typePropertyUseCounty and typePropertyUseDesc are related (1 has ID, other has description)  

```{r tidy_part1d}
# table(myprops$typePropertyUseDesc); table(myprops$typePropertyUseCounty); table(myprops$typePropertyUseID)
nrow(myprops[myprops$typePropertyUseID==290 | myprops$typePropertyUseID==291,])
nrow(myprops[is.na(myprops$typePropertyUseID),])
```

In looking at this table, I see that there are 0 properties listed as land only and there are 0 properties where typePropertyUseID is null. So we can assume all of these properties have structures on them. Knowing that, we will figure out what to do with nulls. 

```{r tidy_part1e, warning=FALSE, message=FALSE}
# colnames(myprops[myprops$typeAC,]) # column name/number
# head(sort(table(myprops[2]),decreasing = TRUE))

for (i in 1:ncol(myprops)) {
  print(colnames(myprops[i]))
  print(head(sort(table(myprops[i]),decreasing = TRUE)))
}
```

**DeckPatio:** typeDeck & typePatio are low counts, so merge these to boolDeckPatio, NA="false" and other = "true"
**Pool:** numPool, boolPoolHottub & boolPoolNoHottub only include 1 value, so merge these fields along with sqftPool to boolPool, where NA="false" and all others = "true"
**Hottub:** boolHottub, boolHottub1, boolPoolHottub only include 1 value, so I will change this where NA="false" and all others = "true"
**TaxPastDue:** boolTaxPastDue & yearTaxPastDue - merge these where NA in both is "false", others are "true"
**Fireplace:** boolFireplace & numFireplace - merge these where NA in both is "false", others are "true"
locCensusTractBlock & locCensusTractBlockRaw I will remove for now.
numGarage & sqftGarage:  I will use numGarage instead of sqftGarage, as these are highly correlated.  I will group with 0, 1, 2, 3+
**yearTax:** for all < 2015, i will change to "pre-2015", all others will be "2015"
**numBath:** use numBath and remove numBathFull, numBath1, numBath3Q
**numUnits:** if NA, fill with 1

```{r tidy_part1f, warning=FALSE, message=FALSE}

# myprops <- myprops %>% mutate() %>% select()

myprops <- myprops %>% 
  mutate(
  boolDeckPatio=parse_integer(ifelse(is.na(typeDeck) & is.na(typePatio),0,1)),
  boolHottub=parse_integer(ifelse(is.na(boolHottub) & is.na(boolHottub1) & 
                                    is.na(boolPoolHotub),0,1)),
  boolPool=parse_integer(ifelse(is.na(boolPoolHotub) & 
                                  is.na(boolPoolNoHottub) & 
                                  is.na(numPool) &
                                  is.na(sqftPool),0,1)),
  boolTaxPastDue=parse_integer(ifelse(is.na(boolTaxPastDue) &
                                        is.na(yearTaxPastDue),0,1)),
  boolFireplace=parse_integer(ifelse(is.na(boolFireplace) &
                                       is.na(numFireplace),0,1)),
  boolShed=parse_integer(ifelse(is.na(typeShed),0,1)),
  yearTax=parse_integer(ifelse(yearTax==2015 | is.na(yearTax), 2015, 2014)),
  numGarage=parse_integer(
    ifelse((is.na(numGarage)|numGarage==0) & (is.na(sqftGarage)|sqftGarage==0),0,
           ifelse(numGarage>=3,3,numGarage)))) %>%
  select(-typeDeck, -typePatio, -typeShed, -boolHottub1, -boolPoolHotub,
         -boolPoolNoHottub, -numPool, -numFireplace,-yearTaxPastDue, 
         -sqftPool, -locCensusTractBlock, -locCensusTractBlockRaw, 
         -numBathFull, -numBath1, -numBath3Q, -sqftFinishedAll1,
         -sqftFinishedAll2, -sqftBasement, -sqftFinishedFirstFloor,
         -sqftFinishedPerim, -sqftFinishedFirstFloor1, -typeStory, 
         -typeAC, -typeBuilidngFrame, -sqftGarage,
         -sqftAll, -typeHeating)

summary(myprops)
```

**Home Details**
Because numBed has fewest null values, I will sort by numBed and then fill numBath, numRoom, numStories, numUnits and sqftFinishedAll based on those.

```{r}
#summary(select(myprops[is.na(myprops$sqftFinishedAll),], starts_with("sqft")))
summary(select(myprops,starts_with("num")))
summary(select(myprops,starts_with("sqft")))

```

**Location**
Sort locZip to fill with year built
for locNeighborhood, are the non-nulls spread across too many neighborhoods?

```{r}
summary(select(myprops,starts_with("loc")))
# table(myprops$locNeighborhood)  
# Yes! 
```

##### b. Data structure
Tidy the data:  ensure each row is an observation and each column is a feature

See functions of gather(), spread(), separate() and unite()

____________________________________________________

#### 3. Data Representation & Data Exploration

**GOAL** Understand the signals in the data, their strength, the features that drive the outcome, and other features to construct, and transform variables into features
- Convert data to the most optimal data types for storing and modeling 
- Narrow in on observations of interest
- Creating new variables that are functions of existing variables
- Also known as: Exploratory Visualization, Exploratory Analysis, Data Transformation

____________________________________________________
____________________________________________________

### B. Data Modeling
**GOAL** Turn the data prepared in the engineering phase into a prediction or valuable insight.
1. Data Discovery
2. Data Modeling

#### 1. Data Discovery
**GOAL** Walk away with features that you now know drive the behavior of the data you are targeting  
- Find patterns and potential insights in the data
- Define how the features can be used to build a model
- Identify the signals in the dataset that are of greater value
- Identify interdependence or correlation among variables
- Create synthetic features
- Filter out redundant variables and extract the most valuable.
- Also known as: Feature Engineering

#### 2. Data Learning
**GOAL** Create a robust & generalizable model that is a mapping between features (inputs/independent variables) and a target variable (output/dependent variable) based on discoveries and transformations made in previous steps.
- Select meta-parameters
- Sampling the data (training/testing)
- Train the model
- Selecting algorithms
- Evaluating results
- Test the model 

____________________________________________________


### C. Information Distillation
**GOAL**  Summarize the previous stages and making it available to you customers/end users
1. Insight, delivery, visualization
2. Data product creation

#### 1. Insight, Delivery, Visualization
**GOAL** Deliver the work in a consumable way to the stakeholders of the project so that the goals can be met.  
- Summarize insights into actionable items
- Advocate for your findings and recommendations through visualizations
- Deliver the model, the report and/or other data product to the stakeholders.
- This is the most critical step in order to make use of all the work done to this point. 

#### 2. Data Product Creation
**GOAL** Enable a machine learning model to be consumed through an interface by an end user  
- This is about deploying a model to production, or productionizing the model, so that it is live and consumable in the desired manner
- This is usually done through an API
- The web application will take data input by the user, and the model will take that data, extract/compute the features, feed them to the model which will generate results.  
- These results will then be fed back to the web application for it to respond accordingly, such as display a particular product for the end user. 

