{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning\n",
    "\n",
    "- [What is Machine Learning](https://www.youtube.com/watch?v=iLu9XyZ55oI)\n",
    "\n",
    "## Questions that Data Science Methods Can Answer\n",
    "\n",
    "- **Is this new observation A or B (or C, D, or E)  (Classification)**\n",
    "- How Many or How Much of something (Regression)\n",
    "- What groupings exist in the data already (Clustering)\n",
    "- What should we expect to happen next? (Time Series Analysis)\n",
    "- Is this weird? (Anomaly Detection)\n",
    "\n",
    "## What are Classification algorithms?\n",
    "\n",
    "- Classification is a **supervised learning task**. That means we train on data w/ answers/labels\n",
    "- We train with ansers/labels to produce a `decision rule` we'll use to classify future data.\n",
    "\n",
    "![machine learning vs classical programming](classical_programming_vs_machine_learning.jpeg)\n",
    "\n",
    "## Main Ideas\n",
    "- With classification, we use labeled data to train algorithms to classify future data points.\n",
    "- The training data allows us to train an algorithm to produce a decision rule\n",
    "- Using a boundary between points or a distance between points, we classify new datapoints into A or B (or C or D or E)\n",
    "\n",
    "![](classify_apples_oranges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a technique for labeling the class of an observation. This is done through the modeling of the patterns in the related data which drive the outcome.\n",
    "\n",
    "The primary goal of developing a classification model is to generalize patterns. This is done so that the category/class of new data can be identified with a high degree of certaintly. Classification can be performed on structured or unstructured data. It can be used to predict binary classes (2 classes) or multi-classes (>2 classes).\n",
    "\n",
    "## Vocab\n",
    "\n",
    "- Classifier:  An algorithm that maps the input data to a specific category.\n",
    "\n",
    "- Classification Model: A series of steps that takes the patterns of input variables, generalizes those patterns, and applies them to new data in order to predict the class. \n",
    "\n",
    "- Feature:  A feature, aka input/independent variable, is an individual measurable property of a phenomenon being observed.\n",
    "\n",
    "- Binary Classification: Classification with two possible outcomes, e.g. pass/fail.\n",
    "\n",
    "- Multiclass Classification:  Classification with more than two classes, where each sample is assigned to one and only one target label, e.g. Grade levels of students in school (1st-12th).\n",
    "    \n",
    "\n",
    "## Common Classification Algorithms\n",
    "\n",
    "- Logistic Regression (sklearn.linear_model.LogisticRegression)\n",
    "- Decision Tree (sklearn.tree.DecisionTreeClassifier)\n",
    "- Naive Bayes (sklearn.naive_bayes.BernoulliNB)\n",
    "- K-Nearest Neighbors (sklearn.neighbors.KNeighborsClassifier)\n",
    "- Random Forest (sklearn.ensemble.RandomForestClassifier)\n",
    "- Support Vector Machine (sklearn.svm.SVC)\n",
    "- Stochastic Gradient Descent (sklearn.linear_model.SGDClassifier)\n",
    "- AdaBoost (sklearn.ensemble.AdaBoostClassifier)\n",
    "- Bagging (sklearn.ensemble.BaggingClassifier)\n",
    "- Gradient Boosting (sklearn.ensemble.GradientBoostingClassifier)\n",
    "\n",
    "See [the sklearn docs on supervised methods](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Needs\n",
    "- Features need to be turned into numbers\n",
    "- Categorical features or discrete features will be numbers that represent those categories\n",
    "- Continuous features may need to be scaled so we're not comparing different units like dollars to cm to age.\n",
    "\n",
    "![scaling-features-diagram](./scale_features_or_not.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "- Technically a regression algorithm (Goal is to find the values for the coefficients that weight each input variable).\n",
    "- Used to predict binary outcomes.\n",
    "- The output is a value between 0 and 1 that represents the probability of one class over the other. \n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Interpretable: Good for understanding the influence of _several_ independent variables on a _single_ outcome variable. | Need to remove attributes which are either unrelated to the output variable or correlated to other attributes.  \n",
    "Flexible: We can choose to ‘snap’ predictions to 0 and 1 via a rule (such as if < .5, 0 else 1) OR we can choose to use the output as is, which is a probability of being class 1.  | Not one of the top performing classification algorithms. \n",
    "Easy to implement, meaning it is good to use for creating a benchmark. |\n",
    "Very efficient and does not require many computational resources.|\n",
    "\n",
    "Linear vs Logistic Regression       |     Logistic Regression\n",
    ":----------------------------------:|:----------------------------------:\n",
    "![linvlog_reg.png](linvlog_reg.png) | ![logreg.png.png](logreg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (CART: Classification And Regression Trees)\n",
    "\n",
    "- A sequence of rules used to classify 2 or more classes.\n",
    "- Each node represents a single input variable (x) and a split point or class of that variable.\n",
    "- The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.  \n",
    "- Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Simple to understand, visualize & explain. | Risk of Overfitting: Can create complex trees that do not generalise well.  \n",
    "Requires little data preparation. | Can be unstable because small variations in the data might lead to overfitting.  \n",
    "Can handle both numerical and categorical data. | \n",
    "Performs well for a broad range of problems. |  \n",
    "\n",
    "Example below:  If an observation has a length of 45, blue eyes, and 2 legs, it's going to be classified as red.\n",
    "\n",
    "![decision_tree.png](decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forest is an implementation of bootstrap aggregation, aka bagging, which is an ensemble algorithm. \n",
    "\n",
    "Bootstrapping is a statistical method for estimating a quantity from a data sample, e.g. mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value. In bootstrap aggregation, or bagging, the same approach is used for estimating entire statistical models, such as decision trees. Multiple samples of your training data are taken and models are constructed for each sample set.\n",
    "\n",
    "When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimatation of the true output value.\n",
    "\n",
    "Random forest is a tweak on this approach of bootstrapping, where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.  The models created for each sample of the data are therefore more different than they otherwise would be, in normal bootstrapping, but still accurate in their unique and different ways. This all combines their prediction results in a better estimate of the true underlying output value.\n",
    "\n",
    "If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm, e.g. using a random forest.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Less risk of overfitting than with a decision tree. | High demand on computational resources.\n",
    "More accurate than decision trees in most cases. | Difficult to implement.\n",
    "| Somewhat of a blackbox model, difficult to explain.\n",
    "\n",
    "![random_forest.png](random_forest.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor\n",
    "\n",
    "### Description\n",
    "\n",
    "K-Nearest Neighbor (KNN) makes predictions based on how close a new data point is to known data points.\n",
    "\n",
    "It is considered \"lazy\" as it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the K nearest neighbours of each point.\n",
    "\n",
    "Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances.  For regression problems, this might be the mean output variable. For classification problems, this might be the mode (or most common) class value.\n",
    "\n",
    "It is important to define a metric to measure the similarity between data instances. Euclidean distance can be used if attributes are all on the same scale (or you convert them to the same scale).\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Simple to implement. | Need to determine the value of K.\n",
    "Robust to noise. | High Computational Cost: It has to compute the distance of each instance to all the training samples...you have to hang on to your entire training dataset.  \n",
    "Performs calculations \"just in time\", i.e. when a prediction is needed (as opposed to ahead of time) | \"Curse of dimensionality\": Distance can break down in very high dimensions, negatively affecting the performance.  \n",
    "Training instances can be updated and curated over time to keep predictions accurate. |\n",
    "\n",
    "![KNN.png](KNN.png)\n",
    "\n",
    "- [Tutorial on an implementation of KNN in python](https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "\n",
    "A technique that uses higher dimensions to best seperate data points into two classes.\n",
    "\n",
    "Support Vector Machines select hyperplane (a line that splits the input variable space) to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line. \n",
    "\n",
    "An optimization algorithm is used to find the values for the coefficients that maximizes the margin. The distance between the hyperplane and the closest data points is referred to as the **margin**. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points, called the **support vectors**, are relevant in defining the hyperplane and in the construction of the classifier.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Effective in high dimensional spaces | Does not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.  \n",
    "Memory efficient: Uses a subset of training points in the decision function | \n",
    "Highly successful classifier | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes\n",
    "\n",
    "Naive Bayes is based on Bayes’ theorem that assumes independence between every pair of features.\n",
    "\n",
    "It is comprised of two types of probabilities that can be calculated directly from your training data:\n",
    "\n",
    "- The probability of each class\n",
    "- The conditional probability for each class given each x value\n",
    "\n",
    "Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem.  When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities. (so normalize your data!)\n",
    " \n",
    "It assumes that each input variable is independent (which is often not the case), thus it is called \"naive\". This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems, including document classification and spam filtering.\n",
    "\n",
    "Pros | Cons\n",
    ":------|:---------\n",
    "Works with a smaller sample size of training data than other classifiers | Can be a bad estimator if used in less than ideal problems.  \n",
    "Extremely fast compared to more sophisticated methods. | \n",
    "Simple & Powerful | \n",
    "\n",
    "Use cases: \n",
    "\n",
    "- Based on their purchase and browsing history, what promos should I offer to my customers?\n",
    "\n",
    "- Learn from IB to develop methods for prospecting new customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Categorical Outcomes\n",
    "\n",
    "In this module, we will explore **supervised** machine learning related to **classification** using **structured** data. We will work through the data science pipeline, expanding our knowledge of both techniques, through each stage of the data science pipeline, as well as improving our knowledge of the python programming language.\n",
    "\n",
    "[![](xkcd_meteorologist.png)](https://xkcd.com/1985/ \"Hi, I'm your new meteorologist and a former software developer. Hey, when we say 12pm, does that mean the hour from 12pm to 1pm, or the hour centered on 12pm? Or is it a snapshot at 12:00 exactly? Because our 24-hour forecast has midnight at both ends, and I'm worried we have an off-by-one error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
