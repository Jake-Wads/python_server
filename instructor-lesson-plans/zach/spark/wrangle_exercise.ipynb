{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/case.csv', header=True, inferSchema=True)\n",
    "print('nrows:', df.count())\n",
    "stray_animal_cases = df.filter(df.service_request_type == 'Stray Animal').count()\n",
    "print('stray animal cases:', stray_animal_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "df = df.withColumnRenamed('SLA_due_date', 'case_due_date')\n",
    "\n",
    "# Convert to better data types\n",
    "df = (\n",
    "    df.withColumn('case_late', col('case_late') == 'YES')\n",
    "    .withColumn('case_closed', col('case_closed') == 'YES')\n",
    ")\n",
    "df = df.withColumn('council_district', format_string('%04d', col('council_district')))\n",
    "df = (\n",
    "    df.withColumn('case_opened_date', to_timestamp(col('case_opened_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_closed_date', to_timestamp(col('case_closed_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_due_date', to_timestamp(col('case_due_date'), 'M/d/yy H:mm'))\n",
    ")\n",
    "\n",
    "# Cleanup text data\n",
    "df = df.withColumn('request_address', lower(trim(col('request_address'))))\n",
    "# Extract zipcode\n",
    "df = df.withColumn('zipcode', regexp_extract(col('request_address'), r'\\d+$', 0))\n",
    "\n",
    "# Create a `case_lifetime` feature\n",
    "df = (\n",
    "    df.withColumn('case_age', datediff(current_timestamp(), 'case_opened_date'))\n",
    "    .withColumn('days_to_closed', datediff('case_closed_date', 'case_opened_date'))\n",
    "    .withColumn('case_lifetime', when(col('case_closed'), col('days_to_closed')).otherwise(col('case_age')))\n",
    "    .drop('case_age', 'days_to_closed')\n",
    ")\n",
    "\n",
    "# Join departments and sources\n",
    "depts = spark.read.csv('data/dept.csv', header=True, inferSchema=True)\n",
    "sources = spark.read.csv('data/source.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.join(depts, 'dept_division', 'left').join(sources, 'source_id', 'left')\n",
    "\n",
    "# # Train Test Split\n",
    "# train, test = df.randomSplit([.8, .2], seed=123)\n",
    "# train, validate, test = df.randomSplit([.7, .15, .15], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to query our dataframe with Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploration Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How old is the latest (in terms of days past SLA) currently open issue?\n",
    "# How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "spark.sql('''\n",
    "SELECT DATEDIFF(current_timestamp, case_due_date) AS days_past_due\n",
    "FROM df\n",
    "WHERE NOT case_closed\n",
    "ORDER BY days_past_due DESC\n",
    "LIMIT 15\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Stray Animal cases are there?\n",
    "df.filter(df.service_request_type == 'Stray Animal').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26760?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.groupBy('service_request_type')\n",
    "    .count()\n",
    "    .filter(expr('service_request_type == \"Stray Animal\"'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many service requests that are assigned to the Field Operations department (dept_division)\n",
    "# are not classified as \"Officer Standby\" request type (service_request_type)?\n",
    "(\n",
    "    df.filter(df.dept_division == 'Field Operations')\n",
    "    .filter(df.service_request_type != 'Officer Standby')\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do it\n",
    "(\n",
    "    df.filter(expr(\"dept_division == 'Field Operations'\"))\n",
    "    .filter(expr('service_request_type != \"Officer Standby\"'))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the council_district column to a string column.\n",
    "\n",
    "# Already done in the data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the case_closed_date column.\n",
    "df.select('case_closed_date', year('case_closed_date')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert num_days_late from days to hours in new columns num_hours_late.\n",
    "(\n",
    "    df.withColumn('num_hours_late', df.num_days_late * 24)\n",
    "    .select('num_days_late', 'num_hours_late')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the case data with the source and department data.\n",
    "\n",
    "# already joined in the data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any cases that do not have a request source?\n",
    "# are there any null values for source_id?\n",
    "(\n",
    "    df.select(df.source_id.isNull().cast('int').alias('is_null'))\n",
    "    .agg(sum('is_null'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col('source_id').isNull()).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the top 10 service request types in terms of number of requests?\n",
    "(\n",
    "    df.groupby('service_request_type')\n",
    "    .count()\n",
    "    .sort(col('count').desc())\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 10 service request types in terms of average days late?\n",
    "# - just the late cases\n",
    "# - for the late cases:\n",
    "#   - what is the average number of days late by request type?\n",
    "(\n",
    "    df.where('case_late') # just the rows where case_late == true\n",
    "    .groupBy('service_request_type')\n",
    "    .agg(mean('num_days_late').alias('n_days_late'), count('*').alias('n_cases'))\n",
    "    .sort(desc('n_days_late'))\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does number of days late depend on department?\n",
    "(\n",
    "    df.filter('case_late')\n",
    "    .groupby('dept_name')\n",
    "    .agg(mean('num_days_late').alias('days_late'), count('num_days_late').alias('n_cases_late'))\n",
    "    .sort('days_late')\n",
    "    .withColumn('days_late', round(col('days_late'), 1))\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('dept_name').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do number of days late depend on department and request type?\n",
    "(\n",
    "    df.filter(\"case_closed\")\n",
    "    .filter(\"case_late\")\n",
    "    .groupby(\"standardized_dept_name\", \"service_request_type\")\n",
    "    .agg(avg(\"num_days_late\").alias(\"days_late\"), count(\"*\").alias(\"n_cases\"))\n",
    "    .withColumn(\"days_late\", round(col(\"days_late\"), 1))\n",
    "    .sort(desc(\"days_late\"))\n",
    "    .show(40, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Recap: getting data from spark dataframes:\n",
    "\n",
    "- `.show(n)`: prints the first `n` rows. Doesn't produce a value that can be used later\n",
    "- `.first`: gives us the first row object\n",
    "- `.head(n)`: gives us a list of the first `n` row objects\n",
    "- `.collect()`: turns *all* the rows into a list of row objects **be careful here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(max('case_opened_date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = df.select(max('case_opened_date'), max('case_closed_date')).first()[0]\n",
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = max_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df = (\n",
    "    df.withColumn('case_age', datediff(lit(max_date), 'case_opened_date'))\n",
    "    .withColumn('days_to_closed', datediff('case_closed_date', 'case_opened_date'))\n",
    "    .withColumn('case_lifetime', when(col('case_closed'), col('days_to_closed')).otherwise(col('case_age')))\n",
    "    .drop('case_age', 'days_to_closed')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Python Code Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.withColumn('case_age', datediff(lit(max_date), 'case_opened_date')).withColumn('days_to_closed', datediff('case_closed_date', 'case_opened_date')).withColumn('case_lifetime', when(col('case_closed'), col('days_to_closed')).otherwise(col('case_age'))).drop('case_age', 'days_to_closed'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
